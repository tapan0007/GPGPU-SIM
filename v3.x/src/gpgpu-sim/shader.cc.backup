// Copyright (c) 2009-2011, Tor M. Aamodt, Wilson W.L. Fung, Ali Bakhoda,
// George L. Yuan, Andrew Turner, Inderpreet Singh 
// The University of British Columbia
// All rights reserved.
//
// Redistribution and use in source and binary forms, with or without
// modification, are permitted provided that the following conditions are met:
//
// Redistributions of source code must retain the above copyright notice, this
// list of conditions and the following disclaimer.
// Redistributions in binary form must reproduce the above copyright notice, this
// list of conditions and the following disclaimer in the documentation and/or
// other materials provided with the distribution.
// Neither the name of The University of British Columbia nor the names of its
// contributors may be used to endorse or promote products derived from this
// software without specific prior written permission.
//
// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
// ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
// WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
// DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
// FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
// DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
// SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
// CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
// OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
// OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

#include <float.h>
#include "shader.h"
#include "gpu-sim.h"
#include "addrdec.h"
#include "dram.h"
#include "stat-tool.h"
#include "gpu-misc.h"
#include "../cuda-sim/ptx_sim.h"
#include "../cuda-sim/ptx-stats.h"
#include "../cuda-sim/cuda-sim.h"
#include "gpu-sim.h"
#include "mem_fetch.h"
#include "mem_latency_stat.h"
#include "visualizer.h"
#include "../statwrapper.h"
#include "icnt_wrapper.h"
#include <string.h>
#include <limits.h>
#include "traffic_breakdown.h"
#include "shader_trace.h"

#define PRIORITIZE_MSHR_OVER_WB 1
#define MAX(a,b) (((a)>(b))?(a):(b))
#define MIN(a,b) (((a)<(b))?(a):(b))
    

/////////////////////////////////////////////////////////////////////////////

std::list<unsigned> shader_core_ctx::get_regs_written( const inst_t &fvt ) const
{
   std::list<unsigned> result;
   for( unsigned op=0; op < MAX_REG_OPERANDS; op++ ) {
      int reg_num = fvt.arch_reg.dst[op]; // this math needs to match that used in function_info::ptx_decode_inst
      if( reg_num >= 0 ) // valid register
         result.push_back(reg_num);
   }
   return result;
}

#define GAMMA 0.95

//#define ALPHA 0.05
#define ALPHA 0.1
//#define ALPHA 0.9
float rl_scheduler::gCurrAlpha = ALPHA;

#define EXPLORATION_PERCENT 5
//#define EXPLORATION_PERCENT 90
//#define EXPLORATION_PERCENT 100
unsigned int rl_scheduler::gCurrExplorationPercent = EXPLORATION_PERCENT;

#define EXPLORATION_REDUCTION_CYCLES 150
#define ALPHA_REDUCTION_CYCLES 150

#define REWARD 					2
#define MEM_OP_REWARD 			200
#define MEM_LOAD_OP_REWARD 		200
#define SH_MEM_LOAD_OP_REWARD 	20
#define MEM_STORE_OP_REWARD 	20
#define SFU_OP_REWARD 			50
#define SP_OP_REWARD 			REWARD
#define NON_UNIFORM_REWARDS 	1
#define PENALTY 				-5
#define LAST_WARP_REWARD 		25
#define CORRECT_TB_REWARD 		50
#define CORRECT_PIPE_REWARD		25

#define NON_UNIFORM_BUCKET_SIZES 0
#define REVERSED_BUCKET_RANGES 0

#define INIT_PHASE_NUM_INSTRS 0 //0xFFFFFFFF

bool rl_scheduler::gShareQvalueTable = true;
bool rl_scheduler::gCumulativeRewardPenalty = false;

float gMaxReward = 2.0;
float gMinReward = 0.0;

#define NO_INST_ISSUED 0
#define INST_ISSUED 1

#define USE_TB_ID_AS_ACTION 1
#define USE_WARP_ID_AS_ACTION 2
#define USE_CMD_PIPE_AS_ACTION 3
#define USE_TB_CMD_PIPE_AS_ACTION 4
#define USE_TB_WARP_ID_AS_ACTION 5

unsigned int rl_scheduler::gActionType = 0xdeaddead;

unsigned int rl_scheduler::gRandomSeed = 0;

float* rl_scheduler::gQvalues = 0;
unsigned int rl_scheduler::gStateVal = 0;
unsigned int rl_scheduler::gNumActionBits = 0;
Scoreboard* rl_scheduler::gScoreboard = 0;
simt_stack** rl_scheduler::gSimtStack = 0;

#define NUM_WARPS_PER_SCHEDULER 24
#define DUMMY_WARP_ID 48 //assuming 48 warps from 0 to 47

#define MAX_NUM_TB_PER_SM 8
#define MAX_NUM_WARP_PER_SM 48

bool rl_scheduler::gUsePrevQvalues = true;

unsigned int rl_scheduler::gNumWarpsExecutingMemInstrGPU = 0;
unsigned int rl_scheduler::gNumReqsInMemSchedQs = 0;
unsigned int rl_scheduler::gNumMemSchedQsLoaded = 0;
unsigned int rl_scheduler::gNumSpInstrIssued = 0;
unsigned int rl_scheduler::gNumSfuInstrIssued = 0;
unsigned int rl_scheduler::gNumGTCMemInstrIssued = 0;
unsigned int rl_scheduler::gNumSpInstrIssued1 = 0;
unsigned int rl_scheduler::gNumSfuInstrIssued1 = 0;
unsigned int rl_scheduler::gNumGTCMemInstrIssued1 = 0;
unsigned int rl_scheduler::gNumSpInstrIssued2 = 0;
unsigned int rl_scheduler::gNumSfuInstrIssued2 = 0;
unsigned int rl_scheduler::gNumGTCMemInstrIssued2 = 0;
unsigned int rl_scheduler::gNumGTCMemInstrFinished = 0;
unsigned long long rl_scheduler::gNumGTCMemLatencyCycles = 0;

#define GTC_LONG_LAT_MEM_INSTR_CACHE_SIZE 4
unsigned int* gGTCLongLatMemInstrCache = 0;
unsigned int gGTCLongLatMemInstrCacheIndex = 0;
unsigned long long gGTCTotalLatCycles = 0;
bool gGTCLongLatMemInstrReady = false;

#define SFU_LONG_LAT_INSTR_CACHE_SIZE 4
unsigned int* gSFULongLatInstrCache = 0;
unsigned int gSFULongLatInstrCacheIndex = 0;
unsigned long long gSFUTotalLatCycles = 0;
bool gSFULongLatInstrReady = false;
unsigned int gNumSFUInstrFinished = 0;

#define LONG_LATENCY 50

unsigned int* rl_scheduler::gTBWithWarpsFinished = 0;
unsigned int* rl_scheduler::gTBWithWarpsAtBarrier = 0;
unsigned int* rl_scheduler::gNumWarpsExecutingMemInstr = 0;
unsigned int* rl_scheduler::gNumReadyMemInstrsWithSameTB = 0;
unsigned int* rl_scheduler::gLastMemInstrTB = 0;
unsigned int* rl_scheduler::gNumReadyMemInstrsWithSamePC = 0;
unsigned int* rl_scheduler::gLastMemInstrPC = 0;
unsigned int* rl_scheduler::gNumWarpsWaitingAtBarrier = 0;
unsigned int* rl_scheduler::gNumWarpsFinished = 0;
unsigned int* gSchedProgressArray = 0;
unsigned int* gTBProgressArray = 0;
unsigned int* gTBPhaseArray = 0;
unsigned int* gWarpProgressArray = 0;
unsigned int* gWarpPhaseArray = 0;
unsigned int* gSelectedTB = 0;
unsigned int* gSelectedWarp = 0;
#define NUM_SCHED_PER_SM 2
unsigned int* gTBWarpsExecutingMemInstr = 0; //per tb per sm
std::vector<std::map<unsigned int, unsigned int> > gDynWarpIdToTBIdMapVec;
unsigned int* gNumTBsGoingToBarrierList;
unsigned int* gNumTBsGoingToFinishList;
unsigned int gNumCallsToSortInorderTBs = 0;
unsigned int gNumCallsToSortFinishTBs = 0;
unsigned int gNumCallsToSortWarps = 0;
unsigned int gNumCallsToSortFinishWarps = 0;
unsigned int gNumCallsToSortBarrierWarps = 0;
unsigned int gNumCallsToSortInorderWarps = 0;

unsigned int* rl_scheduler::gTBNumSpInstrsArray = 0;
unsigned int* rl_scheduler::gTBNumSfuInstrsArray = 0;
unsigned int* rl_scheduler::gTBNumMemInstrsArray = 0;
unsigned int rl_scheduler::gNumInstrsExecedByFinishedTBs = 0;
unsigned int gMaxNumInstrsExecedByTB = 0;
unsigned int gMinNumInstrsExecedByTB = 0xFFFFFFFF;
unsigned int rl_scheduler::gNumFinishedTBs = 0;

unsigned int gNumSMs = 0;
unsigned int gNumWarpsPerBlock = 0;
unsigned int rl_scheduler::gExplorationCnt = 0;

unsigned int rl_scheduler::gSelectedActionVal = 12345678;

std::set<unsigned int> gRRModeTBSet;
std::set<unsigned int> gGTOModeTBSet;
unsigned int gRRModeCnt = 0;
unsigned int gGTOModeCnt = 0;
unsigned int gRR_GTO_partition = 0xdeaddead;

bool gLRRSched = false;
bool gGTOSched = false;
bool gRLSched = false;
bool gFBISched = false;

unsigned int gMemPipeLineStall = 0;
unsigned int gSfuPipeLineStall = 0;
unsigned int gSpPipeLineStall = 0;
unsigned int gMemSfuSpPipeLineStall = 0;
unsigned int gMemSpPipeLineStall = 0;
unsigned int gMemSfuPipeLineStall = 0;
unsigned int gSfuSpPipeLineStall = 0;

void allocateTBWithWarpsFinishedArray(unsigned int numSMs)
{
	if (rl_scheduler::gTBWithWarpsFinished == 0)
	{
		rl_scheduler::gTBWithWarpsFinished = new unsigned int[numSMs];
		for (unsigned int i = 0; i < numSMs; i++)
			rl_scheduler::gTBWithWarpsFinished[i] = 0xdeaddead;
	}
}

void allocateTBWithWarpsAtBarrierArray(unsigned int numSMs)
{
	if (rl_scheduler::gTBWithWarpsAtBarrier == 0)
	{
		rl_scheduler::gTBWithWarpsAtBarrier = new unsigned int[numSMs];
		for (unsigned int i = 0; i < numSMs; i++)
			rl_scheduler::gTBWithWarpsAtBarrier[i] = 0xdeaddead;
	}
}

void allocateWarpPhaseArray(unsigned int numSMs)
{
	if (gWarpPhaseArray == 0)
	{
		gWarpPhaseArray = new unsigned int[numSMs * MAX_NUM_WARP_PER_SM];
		for (unsigned int i = 0; i < numSMs; i++)
		{
			for (unsigned int j = 0; j < MAX_NUM_WARP_PER_SM; j++)
			{
				unsigned int index = i * MAX_NUM_WARP_PER_SM + j;
				gWarpPhaseArray[index] = 0;
			}
		}
	}
}
void allocateWarpProgressArray(unsigned int numSMs)
{
	if (gWarpProgressArray == 0)
	{
		gWarpProgressArray = new unsigned int[numSMs * MAX_NUM_WARP_PER_SM];
		for (unsigned int i = 0; i < numSMs; i++)
		{
			for (unsigned int j = 0; j < MAX_NUM_WARP_PER_SM; j++)
			{
				unsigned int index = i * MAX_NUM_WARP_PER_SM + j;
				gWarpProgressArray[index] = 0;
			}
		}
	}
}

void allocateTBWarpsExecutingMemInstArray(unsigned int numSMs)
{
	if (gTBWarpsExecutingMemInstr == 0)
	{
		gTBWarpsExecutingMemInstr = new unsigned int[numSMs * MAX_NUM_TB_PER_SM];
		for (unsigned int i = 0; i < numSMs; i++)
		{
			for (unsigned int j = 0; j < MAX_NUM_TB_PER_SM; j++)
			{
				unsigned int index = i * MAX_NUM_TB_PER_SM + j;
				gTBWarpsExecutingMemInstr[index] = 0;
			}
		}
	}
}

void allocateTBProgressArray(unsigned int numSMs)
{
	if (gTBProgressArray == 0)
	{
		gTBProgressArray = new unsigned int[numSMs * MAX_NUM_TB_PER_SM];
		for (unsigned int i = 0; i < numSMs; i++)
		{
			for (unsigned int j = 0; j < MAX_NUM_TB_PER_SM; j++)
			{
				unsigned int index = i * MAX_NUM_TB_PER_SM + j;
				gTBProgressArray[index] = 0;
			}
		}
	}
}

void allocateSchedProgressArray(unsigned int numSMs)
{
	if (gSchedProgressArray == 0)
	{
		gSchedProgressArray = new unsigned int[numSMs * 2];
		for (unsigned int i = 0; i < numSMs; i++)
		{
			for (unsigned int j = 0; j < 2; j++)
			{
				unsigned int index = i * 2 + j;
				gSchedProgressArray[index] = 0;
			}
		}
	}
}

void allocateTBPhaseArray(unsigned int numSMs)
{
	if (gTBPhaseArray == 0)
	{
		gTBPhaseArray = new unsigned int[numSMs * MAX_NUM_TB_PER_SM];
		for (unsigned int i = 0; i < numSMs; i++)
		{
			for (unsigned int j = 0; j < MAX_NUM_TB_PER_SM; j++)
			{
				unsigned int index = i * MAX_NUM_TB_PER_SM + j;
				gTBPhaseArray[index] = 0;
			}
		}
	}
}

std::vector<std::map<unsigned int, unsigned int> > gNumWarpsAtBarrierMapVec;
std::vector<std::map<unsigned int, unsigned int> > gNumWarpsAtFinishMapVec;

void initNumWarpsAtFinishMapVec(unsigned int numSMs, unsigned int smId)
{
	if (gNumWarpsAtFinishMapVec.size() != numSMs)
		gNumWarpsAtFinishMapVec.resize(numSMs);

	std::map<unsigned int, unsigned int>& numWarpsAtFinishMap = gNumWarpsAtFinishMapVec.at(smId);
	for (unsigned int i = 0; i < MAX_NUM_TB_PER_SM; i++)
		numWarpsAtFinishMap[i] = 0;
}

void initNumWarpsAtBarrierMapVec(unsigned int numSMs, unsigned int smId)
{
	if (gNumWarpsAtBarrierMapVec.size() != numSMs)
		gNumWarpsAtBarrierMapVec.resize(numSMs);

	std::map<unsigned int, unsigned int>& numWarpsAtBarrierMap = gNumWarpsAtBarrierMapVec.at(smId);
	for (unsigned int i = 0; i < MAX_NUM_TB_PER_SM; i++)
		numWarpsAtBarrierMap[i] = 0;
}


shader_core_ctx::shader_core_ctx( class gpgpu_sim *gpu, 
                                  class simt_core_cluster *cluster,
                                  unsigned shader_id,
                                  unsigned tpc_id,
                                  const struct shader_core_config *config,
                                  const struct memory_config *mem_config,
                                  shader_core_stats *stats )
   : core_t( gpu, NULL, config->warp_size, config->n_thread_per_shader ),
     m_barriers( config->max_warps_per_shader, config->max_cta_per_core ),
     m_dynamic_warp_id(0)
{
    m_cluster = cluster;
    m_config = config;
    m_memory_config = mem_config;
    m_stats = stats;
    unsigned warp_size=config->warp_size;
    
    m_sid = shader_id;
    m_tpc = tpc_id;
    
    m_pipeline_reg.reserve(N_PIPELINE_STAGES);
    for (int j = 0; j<N_PIPELINE_STAGES; j++) {
        m_pipeline_reg.push_back(register_set(m_config->pipe_widths[j],pipeline_stage_name_decode[j]));
    }
    
    m_threadState = (thread_ctx_t*) calloc(sizeof(thread_ctx_t), config->n_thread_per_shader);
    
    m_not_completed = 0;
    m_active_threads.reset();
    m_n_active_cta = 0;
    for ( unsigned i = 0; i<MAX_CTA_PER_SHADER; i++ ) 
        m_cta_status[i]=0;
    for (unsigned i = 0; i<config->n_thread_per_shader; i++) {
        m_thread[i]= NULL;
        m_threadState[i].m_cta_id = -1;
        m_threadState[i].m_active = false;
    }
    
    // m_icnt = new shader_memory_interface(this,cluster);
    if ( m_config->gpgpu_perfect_mem ) {
        m_icnt = new perfect_memory_interface(this,cluster);
    } else {
        m_icnt = new shader_memory_interface(this,cluster);
    }
    m_mem_fetch_allocator = new shader_core_mem_fetch_allocator(shader_id,tpc_id,mem_config);
    
    // fetch
    m_last_warp_fetched = 0;
    
    #define STRSIZE 1024
    char name[STRSIZE];
    snprintf(name, STRSIZE, "L1I_%03d", m_sid);
    m_L1I = new read_only_cache( name,m_config->m_L1I_config,m_sid,get_shader_instruction_cache_id(),m_icnt,IN_L1I_MISS_QUEUE);
    
    m_warp.resize(m_config->max_warps_per_shader, shd_warp_t(this, warp_size));
    m_scoreboard = new Scoreboard(m_sid, m_config->max_warps_per_shader);
    
    //scedulers
    //must currently occur after all inputs have been initialized.
    std::string sched_config = m_config->gpgpu_scheduler_string;
    const concrete_scheduler scheduler = sched_config.find("lrr") != std::string::npos ?
                                         CONCRETE_SCHEDULER_LRR :
                                         sched_config.find("two_level_active") != std::string::npos ?
                                         CONCRETE_SCHEDULER_TWO_LEVEL_ACTIVE :
                                         sched_config.find("gto") != std::string::npos ?
                                         CONCRETE_SCHEDULER_GTO :
                                         sched_config.find("warp_limiting") != std::string::npos ?
                                         CONCRETE_SCHEDULER_WARP_LIMITING:
										 sched_config.find("rl") != std::string::npos ?
										 CONCRETE_SCHEDULER_REINFORCEMENT_LEARNING:
										 sched_config.find("round_robin_plus_greedy_then_old") != std::string::npos ?
										 CONCRETE_SCHEDULER_ROUND_ROBIN_PLUS_GREEDY:
										 sched_config.find("gto_sched_0_lrr_sched1") != std::string::npos ?
										 CONCRETE_SCHEDULER_0_GREEDY_SCHEDULER_1_LRR:
										 sched_config.find("random") != std::string::npos ?
										 CONCRETE_SCHEDULER_RANDOM:
										 sched_config.find("my_greedy") != std::string::npos ?
										 CONCRETE_SCHEDULER_MY_GREEDY:
										 sched_config.find("tb_prio") != std::string::npos ?
										 CONCRETE_SCHEDULER_TB_PRIO:
										 sched_config.find("fbi") != std::string::npos ?
										 CONCRETE_SCHEDULER_FBI:
                                         NUM_CONCRETE_SCHEDULERS;
    assert ( scheduler != NUM_CONCRETE_SCHEDULERS );
    

	float* QvalueTable = 0;
	unsigned int* QvalueUpdateTable = 0;
	gNumSMs = this->get_config()->n_simt_clusters * this->get_config()->n_simt_cores_per_cluster;
    for (int i = 0; i < m_config->gpgpu_num_sched_per_core; i++) {
        switch( scheduler )
        {
            case CONCRETE_SCHEDULER_LRR:
				gLRRSched = true;
                schedulers.push_back(
                    new lrr_scheduler( m_stats,
                                       this,
                                       m_scoreboard,
                                       m_simt_stack,
                                       &m_warp,
                                       &m_pipeline_reg[ID_OC_SP],
                                       &m_pipeline_reg[ID_OC_SFU],
                                       &m_pipeline_reg[ID_OC_MEM],
                                       i
                                     )
                );
                break;
            case CONCRETE_SCHEDULER_TWO_LEVEL_ACTIVE:
                schedulers.push_back(
                    new two_level_active_scheduler( m_stats,
                                                    this,
                                                    m_scoreboard,
                                                    m_simt_stack,
                                                    &m_warp,
                                                    &m_pipeline_reg[ID_OC_SP],
                                                    &m_pipeline_reg[ID_OC_SFU],
                                                    &m_pipeline_reg[ID_OC_MEM],
                                                    i,
                                                    config->gpgpu_scheduler_string
                                                  )
                );
                break;
            case CONCRETE_SCHEDULER_GTO:
				gGTOSched = true;
                schedulers.push_back(
                    new gto_scheduler( m_stats,
                                       this,
                                       m_scoreboard,
                                       m_simt_stack,
                                       &m_warp,
                                       &m_pipeline_reg[ID_OC_SP],
                                       &m_pipeline_reg[ID_OC_SFU],
                                       &m_pipeline_reg[ID_OC_MEM],
                                       i
                                     )
                );
                break;
            case CONCRETE_SCHEDULER_WARP_LIMITING:
                schedulers.push_back(
                    new swl_scheduler( m_stats,
                                       this,
                                       m_scoreboard,
                                       m_simt_stack,
                                       &m_warp,
                                       &m_pipeline_reg[ID_OC_SP],
                                       &m_pipeline_reg[ID_OC_SFU],
                                       &m_pipeline_reg[ID_OC_MEM],
                                       i,
                                       config->gpgpu_scheduler_string
                                     )
                );
                break;
			case CONCRETE_SCHEDULER_REINFORCEMENT_LEARNING:
			{
				gRLSched = true;
				rl_scheduler* rlSched = 
                    new rl_scheduler( m_stats,
                                       this,
                                       m_scoreboard,
                                       m_simt_stack,
                                       &m_warp,
                                       &m_pipeline_reg[ID_OC_SP],
                                       &m_pipeline_reg[ID_OC_SFU],
                                       &m_pipeline_reg[ID_OC_MEM],
                                       i);
                schedulers.push_back(rlSched);

				initNumWarpsAtBarrierMapVec(gNumSMs, m_sid);
				initNumWarpsAtFinishMapVec(gNumSMs, m_sid);

				if ((m_sid == 0) && (i == 0))
					printf("Running RL scheduler\n");

				//if ((EXPLORATION_PERCENT == 100) && (rl_scheduler::gRandomSeed == 0))
				//{
					//rl_scheduler::gRandomSeed = time(0);
					//printf("Using random seed %u\n", rl_scheduler::gRandomSeed);
					//srandom(rl_scheduler::gRandomSeed);
					if ((m_sid == 0) && (i == 0))
					{
						unsigned int seed = time(0);
						printf("Using random seed %u\n", seed);
						srandom(seed);
					}
				//}

				rlSched->addAttributes(m_config->rl_attrs);

				if ((rl_scheduler::gShareQvalueTable == true) && (i != 0))
				{
					assert(QvalueTable != 0);
					assert(QvalueUpdateTable != 0);
					rlSched->allocateQvalues(QvalueTable);
					rlSched->allocateQvalueUpdates(QvalueUpdateTable);
				}
				else
				{
					QvalueTable = rlSched->allocateQvalues(0);
					QvalueUpdateTable = rlSched->allocateQvalueUpdates(0);
					rlSched->initQvalues();
					rlSched->initQvalueUpdates();
				}
				rlSched->initCurrStateAndAction();

                break;
			}
            case CONCRETE_SCHEDULER_ROUND_ROBIN_PLUS_GREEDY:
			{
				rr_gto_scheduler* rr_gto_sched = new rr_gto_scheduler( m_stats,
                                       this,
                                       m_scoreboard,
                                       m_simt_stack,
                                       &m_warp,
                                       &m_pipeline_reg[ID_OC_SP],
                                       &m_pipeline_reg[ID_OC_SFU],
                                       &m_pipeline_reg[ID_OC_MEM],
                                       i);
                schedulers.push_back(rr_gto_sched);
				gRR_GTO_partition = m_config->rr_gto_partition;
				if ((m_sid == 0) && (i == 0))
					printf("rr_gto_partition = %u\n", gRR_GTO_partition);
                break;
			}
            case CONCRETE_SCHEDULER_RANDOM:
			{
				random_scheduler* random_sched = new random_scheduler( m_stats,
                                       this,
                                       m_scoreboard,
                                       m_simt_stack,
                                       &m_warp,
                                       &m_pipeline_reg[ID_OC_SP],
                                       &m_pipeline_reg[ID_OC_SFU],
                                       &m_pipeline_reg[ID_OC_MEM],
                                       i);
                schedulers.push_back(random_sched);
				if ((m_sid == 0) && (i == 0))
				{
					unsigned int seed = 0xFFFFFFFF - (unsigned int) time(0);
					printf("Using random seed %u\n", seed);
					srandom(seed);
				}
                break;
			}
			case CONCRETE_SCHEDULER_0_GREEDY_SCHEDULER_1_LRR:
			{
				if (i == 0)
				{
                	schedulers.push_back(
                    	new gto_scheduler( m_stats,
                                       	this,
                                       	m_scoreboard,
                                       	m_simt_stack,
                                       	&m_warp,
                                       	&m_pipeline_reg[ID_OC_SP],
                                       	&m_pipeline_reg[ID_OC_SFU],
                                       	&m_pipeline_reg[ID_OC_MEM],
                                       	i
                                     	)
                	);
                break;
				}
				else
				{
                	schedulers.push_back(
                    	new lrr_scheduler( m_stats,
                                       	this,
                                       	m_scoreboard,
                                       	m_simt_stack,
                                       	&m_warp,
                                       	&m_pipeline_reg[ID_OC_SP],
                                       	&m_pipeline_reg[ID_OC_SFU],
                                       	&m_pipeline_reg[ID_OC_MEM],
                                       	i
                                     	)
                	);
                	break;
				}
			}
			case CONCRETE_SCHEDULER_MY_GREEDY:
			{
				my_greedy_scheduler* my_greedy_sched = new my_greedy_scheduler( m_stats,
                                       	this,
                                       	m_scoreboard,
                                       	m_simt_stack,
                                       	&m_warp,
                                       	&m_pipeline_reg[ID_OC_SP],
                                       	&m_pipeline_reg[ID_OC_SFU],
                                       	&m_pipeline_reg[ID_OC_MEM],
                                       	i
                                     	);
                	schedulers.push_back(my_greedy_sched);
                	break;
			}
			case CONCRETE_SCHEDULER_TB_PRIO:
			{
				tb_prio_scheduler* tb_prio_sched = new tb_prio_scheduler( m_stats,
                                       	this,
                                       	m_scoreboard,
                                       	m_simt_stack,
                                       	&m_warp,
                                       	&m_pipeline_reg[ID_OC_SP],
                                       	&m_pipeline_reg[ID_OC_SFU],
                                       	&m_pipeline_reg[ID_OC_MEM],
                                       	i
                                     	);
                schedulers.push_back(tb_prio_sched);
				gNumSMs = this->get_config()->n_simt_clusters * this->get_config()->n_simt_cores_per_cluster;


				allocateTBProgressArray(gNumSMs);

				allocateWarpProgressArray(gNumSMs);
		
				allocateTBWithWarpsFinishedArray(gNumSMs);

				allocateTBWithWarpsAtBarrierArray(gNumSMs);

				if (gSelectedTB == 0)
				{
					gSelectedTB = new unsigned int[gNumSMs];
					for (unsigned int i = 0; i < gNumSMs; i++)
						gSelectedTB[i] = 0xdeaddead;
				}
				if (gSelectedWarp == 0)
				{
					gSelectedWarp = new unsigned int[gNumSMs * NUM_SCHED_PER_SM];
					for (unsigned int i = 0; i < gNumSMs * NUM_SCHED_PER_SM; i++)
						gSelectedWarp[i] = 0xdeaddead;
				}
                break;
			}
			case CONCRETE_SCHEDULER_FBI:
			{
				fbi_scheduler* fbi_sched = new fbi_scheduler( m_stats,
                                       	this,
                                       	m_scoreboard,
                                       	m_simt_stack,
                                       	&m_warp,
                                       	&m_pipeline_reg[ID_OC_SP],
                                       	&m_pipeline_reg[ID_OC_SFU],
                                       	&m_pipeline_reg[ID_OC_MEM],
                                       	i
                                     	);

				gFBISched = true;
                schedulers.push_back(fbi_sched);

				gNumSMs = this->get_config()->n_simt_clusters * this->get_config()->n_simt_cores_per_cluster;

				gNumTBsGoingToBarrierList = new unsigned int[MAX_NUM_TB_PER_SM];
				gNumTBsGoingToFinishList = new unsigned int[MAX_NUM_TB_PER_SM];
				for (unsigned int i = 0; i < MAX_NUM_TB_PER_SM; i++)
				{
					gNumTBsGoingToBarrierList[i] = 0;
					gNumTBsGoingToFinishList[i] = 0;
				}

				fbi_sched->mInitArrays(gNumSMs);

				fbi_sched->mInitLists(gNumSMs);

				gDynWarpIdToTBIdMapVec.resize(gNumSMs);
				allocateTBWarpsExecutingMemInstArray(gNumSMs);

				allocateTBProgressArray(gNumSMs);
				allocateTBPhaseArray(gNumSMs);
				allocateSchedProgressArray(gNumSMs);

				allocateWarpProgressArray(gNumSMs);
				allocateWarpPhaseArray(gNumSMs);

                break;
			}
            default:
                abort();
        };

		if ((m_sid == 0) && (i == 0))
		{
			printf("ALPHA = %f, GAMMA = %f, REWARD = %u, PENALTY = %u, EXPLORATION = %f, QVALUE_SHARE=%u, INIT_PHASE=%u, CUMULATIVE_REWARD_PENALTY=%u, DRAM_LATENCY=%u, NON_UNIFORM_REWARDS=%u, NON_UNIFORM_BUCKET_SIZES=%u, REVERSED_BUCKET_RANGES=%u, PERFECT_MEM=%u, USE_WARP_ID_AS_ACTION=%u, USE_TB_CMD_PIPE_AS_ACTION=%u, USE_CMD_PIPE_AS_ACTION=%u, USE_TB_ID_AS_ACTION=%u, USE_TB_WARP_ID_AS_ACTION=%u, RANDOM_SEED=%u\n", ALPHA, GAMMA, REWARD, PENALTY, (float)EXPLORATION_PERCENT/100, (rl_scheduler::gShareQvalueTable == true) ? 1 : 0, INIT_PHASE_NUM_INSTRS, rl_scheduler::gCumulativeRewardPenalty, mem_config->dram_latency, NON_UNIFORM_REWARDS, NON_UNIFORM_BUCKET_SIZES, REVERSED_BUCKET_RANGES, m_config->gpgpu_perfect_mem, rl_scheduler::gActionType == USE_WARP_ID_AS_ACTION, rl_scheduler::gActionType == USE_TB_CMD_PIPE_AS_ACTION, rl_scheduler::gActionType == USE_CMD_PIPE_AS_ACTION, rl_scheduler::gActionType == USE_TB_ID_AS_ACTION, rl_scheduler::gActionType == USE_TB_WARP_ID_AS_ACTION, rl_scheduler::gRandomSeed);

			printf("RESULT_DIR_EXT=_%.2f_%.2f_%u_%u_%.2f_%u_%u_%u_%u_%u_%u_%u_%u_%u_%u_%u_%u_%u_RANDOM%u\n", ALPHA, GAMMA, REWARD, PENALTY, (float)EXPLORATION_PERCENT, (rl_scheduler::gShareQvalueTable == true) ? 1 : 0, INIT_PHASE_NUM_INSTRS, rl_scheduler::gCumulativeRewardPenalty, mem_config->dram_latency, NON_UNIFORM_REWARDS, NON_UNIFORM_BUCKET_SIZES, REVERSED_BUCKET_RANGES, m_config->gpgpu_perfect_mem, rl_scheduler::gActionType == USE_WARP_ID_AS_ACTION, rl_scheduler::gActionType == USE_TB_CMD_PIPE_AS_ACTION, rl_scheduler::gActionType == USE_CMD_PIPE_AS_ACTION, rl_scheduler::gActionType == USE_TB_ID_AS_ACTION, rl_scheduler::gActionType == USE_TB_WARP_ID_AS_ACTION, rl_scheduler::gRandomSeed);
		}
    }
    
    for (unsigned i = 0; i < m_warp.size(); i++) {
        //distribute i's evenly though schedulers;
        schedulers[i%m_config->gpgpu_num_sched_per_core]->add_supervised_warp_id(i);
    }
    for ( int i = 0; i < m_config->gpgpu_num_sched_per_core; ++i ) {
        schedulers[i]->done_adding_supervised_warps();
    }
    
    //op collector configuration
    enum { SP_CUS, SFU_CUS, MEM_CUS, GEN_CUS };
    m_operand_collector.add_cu_set(SP_CUS, m_config->gpgpu_operand_collector_num_units_sp, m_config->gpgpu_operand_collector_num_out_ports_sp);
    m_operand_collector.add_cu_set(SFU_CUS, m_config->gpgpu_operand_collector_num_units_sfu, m_config->gpgpu_operand_collector_num_out_ports_sfu);
    m_operand_collector.add_cu_set(MEM_CUS, m_config->gpgpu_operand_collector_num_units_mem, m_config->gpgpu_operand_collector_num_out_ports_mem);
    m_operand_collector.add_cu_set(GEN_CUS, m_config->gpgpu_operand_collector_num_units_gen, m_config->gpgpu_operand_collector_num_out_ports_gen);
    
    opndcoll_rfu_t::port_vector_t in_ports;
    opndcoll_rfu_t::port_vector_t out_ports;
    opndcoll_rfu_t::uint_vector_t cu_sets;
    for (unsigned i = 0; i < m_config->gpgpu_operand_collector_num_in_ports_sp; i++) {
        in_ports.push_back(&m_pipeline_reg[ID_OC_SP]);
        out_ports.push_back(&m_pipeline_reg[OC_EX_SP]);
        cu_sets.push_back((unsigned)SP_CUS);
        cu_sets.push_back((unsigned)GEN_CUS);
        m_operand_collector.add_port(in_ports,out_ports,cu_sets);
        in_ports.clear(),out_ports.clear(),cu_sets.clear();
    }
    
    for (unsigned i = 0; i < m_config->gpgpu_operand_collector_num_in_ports_sfu; i++) {
        in_ports.push_back(&m_pipeline_reg[ID_OC_SFU]);
        out_ports.push_back(&m_pipeline_reg[OC_EX_SFU]);
        cu_sets.push_back((unsigned)SFU_CUS);
        cu_sets.push_back((unsigned)GEN_CUS);
        m_operand_collector.add_port(in_ports,out_ports,cu_sets);
        in_ports.clear(),out_ports.clear(),cu_sets.clear();
    }
    
    for (unsigned i = 0; i < m_config->gpgpu_operand_collector_num_in_ports_mem; i++) {
        in_ports.push_back(&m_pipeline_reg[ID_OC_MEM]);
        out_ports.push_back(&m_pipeline_reg[OC_EX_MEM]);
        cu_sets.push_back((unsigned)MEM_CUS);
        cu_sets.push_back((unsigned)GEN_CUS);                       
        m_operand_collector.add_port(in_ports,out_ports,cu_sets);
        in_ports.clear(),out_ports.clear(),cu_sets.clear();
    }   
    
    
    for (unsigned i = 0; i < m_config->gpgpu_operand_collector_num_in_ports_gen; i++) {
        in_ports.push_back(&m_pipeline_reg[ID_OC_SP]);
        in_ports.push_back(&m_pipeline_reg[ID_OC_SFU]);
        in_ports.push_back(&m_pipeline_reg[ID_OC_MEM]);
        out_ports.push_back(&m_pipeline_reg[OC_EX_SP]);
        out_ports.push_back(&m_pipeline_reg[OC_EX_SFU]);
        out_ports.push_back(&m_pipeline_reg[OC_EX_MEM]);
        cu_sets.push_back((unsigned)GEN_CUS);   
        m_operand_collector.add_port(in_ports,out_ports,cu_sets);
        in_ports.clear(),out_ports.clear(),cu_sets.clear();
    }
    
    m_operand_collector.init( m_config->gpgpu_num_reg_banks, this );
    
    // execute
    m_num_function_units = m_config->gpgpu_num_sp_units + m_config->gpgpu_num_sfu_units + 1; // sp_unit, sfu, ldst_unit
    //m_dispatch_port = new enum pipeline_stage_name_t[ m_num_function_units ];
    //m_issue_port = new enum pipeline_stage_name_t[ m_num_function_units ];
    
    //m_fu = new simd_function_unit*[m_num_function_units];
    
    for (int k = 0; k < m_config->gpgpu_num_sp_units; k++) {
        m_fu.push_back(new sp_unit( &m_pipeline_reg[EX_WB], m_config, this ));
        m_dispatch_port.push_back(ID_OC_SP);
        m_issue_port.push_back(OC_EX_SP);
    }
    
    for (int k = 0; k < m_config->gpgpu_num_sfu_units; k++) {
        m_fu.push_back(new sfu( &m_pipeline_reg[EX_WB], m_config, this ));
        m_dispatch_port.push_back(ID_OC_SFU);
        m_issue_port.push_back(OC_EX_SFU);
    }
    
    m_ldst_unit = new ldst_unit( m_icnt, m_mem_fetch_allocator, this, &m_operand_collector, m_scoreboard, config, mem_config, stats, shader_id, tpc_id );
    m_fu.push_back(m_ldst_unit);
    m_dispatch_port.push_back(ID_OC_MEM);
    m_issue_port.push_back(OC_EX_MEM);
    
    assert(m_num_function_units == m_fu.size() and m_fu.size() == m_dispatch_port.size() and m_fu.size() == m_issue_port.size());
    
    //there are as many result buses as the width of the EX_WB stage
    num_result_bus = config->pipe_widths[EX_WB];
    for(unsigned i=0; i<num_result_bus; i++){
        this->m_result_bus.push_back(new std::bitset<MAX_ALU_LATENCY>());
    }
    
    m_last_inst_gpu_sim_cycle = 0;
    m_last_inst_gpu_tot_sim_cycle = 0;

	whichSchedFirst = 0;
}

void shader_core_ctx::reinit(unsigned start_thread, unsigned end_thread, bool reset_not_completed ) 
{
   if( reset_not_completed ) {
       m_not_completed = 0;
       m_active_threads.reset();
   }
   for (unsigned i = start_thread; i<end_thread; i++) {
      m_threadState[i].n_insn = 0;
      m_threadState[i].m_cta_id = -1;
   }
   for (unsigned i = start_thread / m_config->warp_size; i < end_thread / m_config->warp_size; ++i) {
      m_warp[i].reset();
      m_simt_stack[i]->reset();
   }
}

extern std::map<std::string, std::map<unsigned int, unsigned int> > gKernelBackEdgeSrcDstMap;
std::vector<unsigned int> gPhaseEndPCVec;

void createInstrPhaseMap(std::string kernelName)
{
	std::map<unsigned int, unsigned int> backEdgeSrcDstMap = gKernelBackEdgeSrcDstMap[kernelName];
	if (backEdgeSrcDstMap.size() == 0)
	{
		printf("%s does not have any backedges\n", kernelName.c_str());
		return;
	}
	for (std::map<unsigned int, unsigned int>::iterator iter = backEdgeSrcDstMap.begin();
	     iter != backEdgeSrcDstMap.end();
		 iter++)
	{
		unsigned int backEdgeSrcPC = iter->first;
		unsigned int backEdgeDstPC = iter->second;
		unsigned int beginPC;
		unsigned int endPC;

		if (backEdgeSrcPC < backEdgeDstPC)
		{
			beginPC = backEdgeSrcPC;
			endPC = backEdgeDstPC;
		}
		else
		{
			beginPC = backEdgeDstPC;
			endPC = backEdgeSrcPC;
		}

		if ((fbi_scheduler::gLastMemInstrPC > beginPC) && (fbi_scheduler::gLastMemInstrPC < endPC))
		{
			fbi_scheduler::gLastMemInstrPC = 0xdeaddead;
			printf("resetting fbi_scheduler::gLastMemInstrPC = 0x%x\n", fbi_scheduler::gLastMemInstrPC);
		}

		bool addPhase = true;

		for (std::map<unsigned int, unsigned int>::iterator iter2 = backEdgeSrcDstMap.begin();
	     	iter2 != backEdgeSrcDstMap.end();
		 	iter2++)
		{
			unsigned int backEdgeSrcPC2 = iter2->first;
			unsigned int backEdgeDstPC2 = iter2->second;
			unsigned int beginPC2;
			unsigned int endPC2;
			if (backEdgeSrcPC2 < backEdgeDstPC2)
			{
				beginPC2 = backEdgeSrcPC2;
				endPC2 = backEdgeDstPC2;
			}
			else
			{
				beginPC2 = backEdgeDstPC2;
				endPC2 = backEdgeSrcPC2;
			}

			if ((beginPC != beginPC2) && (endPC != endPC2))
			{
				if ((beginPC > beginPC2) && (beginPC < endPC2))
				{
					addPhase = false;
					printf("%u to %u is a not phase, conflicts with %u to %u\n", beginPC, endPC, beginPC2, endPC2);
				}
			}
		}
		if (addPhase == true)
		{
			printf("%u to %u is a phase\n", beginPC, endPC);
			gPhaseEndPCVec.push_back(endPC);
		}
	}
	if (gPhaseEndPCVec.size() > 0)
		std::sort(gPhaseEndPCVec.begin(), gPhaseEndPCVec.end());
}

void shader_core_ctx::init_warps( unsigned cta_id, unsigned start_thread, unsigned end_thread )
{
    address_type start_pc = next_pc(start_thread);
    if (m_config->model == POST_DOMINATOR) {
        unsigned start_warp = start_thread / m_config->warp_size;
        unsigned end_warp = end_thread / m_config->warp_size + ((end_thread % m_config->warp_size)? 1 : 0);
        for (unsigned i = start_warp; i < end_warp; ++i) {
            unsigned n_active=0;
            simt_mask_t active_threads;
            for (unsigned t = 0; t < m_config->warp_size; t++) {
                unsigned hwtid = i * m_config->warp_size + t;
                if ( hwtid < end_thread ) {
                    n_active++;
                    assert( !m_active_threads.test(hwtid) );
                    m_active_threads.set( hwtid );
                    active_threads.set(t);
                }
            }
            m_simt_stack[i]->launch(start_pc,active_threads);
            m_warp[i].init(start_pc,cta_id,i,active_threads, m_dynamic_warp_id);
            ++m_dynamic_warp_id;
            m_not_completed += n_active;
      }
	  if (gNumWarpsPerBlock == 0)
	  {
	  	gNumWarpsPerBlock = end_warp - start_warp;
		printf("Number of warps per block = %u\n", gNumWarpsPerBlock);
		printf("Number of blocks = %u\n", kernel_max_cta_per_shader);
		printf("Number of active warps = %u\n", kernel_max_cta_per_shader * gNumWarpsPerBlock);

		if (gFBISched)
		{
	  		fbi_scheduler::mSetLastMemInstrPC(m_kernel->name().c_str());

			createInstrPhaseMap(m_kernel->name());
		}
	  }
    }
}

std::vector<std::list<unsigned int> > fbi_scheduler::finishListVec;
std::vector<std::list<unsigned int> > fbi_scheduler::barrierListVec;
std::vector<std::list<unsigned int> > fbi_scheduler::inorderListVec;

unsigned int fbi_scheduler::gLastMemInstrPC = 0xdeaddead;
unsigned int* fbi_scheduler::numActiveWarps = 0;
unsigned int* fbi_scheduler::numWarpsReachedLastMemInstr = 0;
bool* fbi_scheduler::issueLastMemInstr = 0;
unsigned int* fbi_scheduler::numReadyNonLastMemInstrs = 0;

#define SORT_THRESHOLD 1000
//#define SORT_THRESHOLD 500
//#define SORT_THRESHOLD 200

void fbi_scheduler::printWarpProgress(unsigned int tbId)
{
	unsigned int smId = m_shader->get_sid();
    for (std::vector< shd_warp_t* >::const_iterator iter = m_next_cycle_prioritized_warps.begin();
     	 iter != m_next_cycle_prioritized_warps.end();
     	 iter++) 
	{
		shd_warp_t* warpPtr = (*iter);
       	if (warpPtr == NULL || warpPtr->done_exit())
       		continue;
		if (warpPtr->get_cta_id() == tbId)
		{
			unsigned int warpProg = gWarpProgressArray[smId * MAX_NUM_WARP_PER_SM + warpPtr->get_warp_id()];
			printf("%llu: warp %u tb %u progress %u\n", gpu_sim_cycle, warpPtr->get_warp_id(), warpPtr->get_cta_id(), warpProg);
		}
	}
}

void fbi_scheduler::mSetLastMemInstrPC(const char* kernelName)
{
	if (fbi_scheduler::gLastMemInstrPC == 0xdeaddead)
	{
		fbi_scheduler::gLastMemInstrPC = 0xdeadbeef;

		if (strcmp(kernelName, "_Z13aesEncrypt128PjS_i") == 0) //AES
			fbi_scheduler::gLastMemInstrPC = 0xa70;
		else if (strcmp(kernelName, "_Z22bpnn_layerforward_CUDAPfS_S_S_ii") == 0) //backprop
			fbi_scheduler::gLastMemInstrPC = 0x308;
		else if (strcmp(kernelName, "_Z24bpnn_adjust_weights_cudaPfiS_iS_S_") == 0) //backprop
			fbi_scheduler::gLastMemInstrPC = 0x4d8;//0x5c0;
		else if (strcmp(kernelName, "findRangeK") == 0) //b+tree
			fbi_scheduler::gLastMemInstrPC = 0x378;
		else if (strcmp(kernelName, "findK") == 0) //b+tree
			fbi_scheduler::gLastMemInstrPC = 0x5d0;
		else if (strcmp(kernelName, "_Z7cenergyifPf") == 0) //CP, CP_BIG
			fbi_scheduler::gLastMemInstrPC = 0x208;
		else if (strcmp(kernelName, "_Z14calculate_tempiPfS_S_iiiiffffff") == 0) //hotspot
			fbi_scheduler::gLastMemInstrPC = 0x860;
		else if (strcmp(kernelName, "_Z29Pathcalc_Portfolio_KernelGPU2Pf") == 0) //LIB, LIB_BIG
			fbi_scheduler::gLastMemInstrPC = 0x1fd8;
		else if (strcmp(kernelName, "_Z28Pathcalc_Portfolio_KernelGPUPfS_") == 0) //LIB, LIB_BIG
			fbi_scheduler::gLastMemInstrPC = 0x1a88;
		else if (strcmp(kernelName, "_Z13GPU_laplace3diiiiPfS_") == 0) //LPS, LPS_BIG
			fbi_scheduler::gLastMemInstrPC = 0x5c8;
		else if (strcmp(kernelName, "_Z15mummergpuKernelP10MatchCoordPcPKiS3_ii") == 0) //MUM
			fbi_scheduler::gLastMemInstrPC = 0x968;
		else if (strcmp(kernelName, "_Z17executeFirstLayerPfS_S_") == 0) //NN
			fbi_scheduler::gLastMemInstrPC = 0x420;
		else if (strcmp(kernelName, "_Z18executeSecondLayerPfS_S_") == 0) //NN
			fbi_scheduler::gLastMemInstrPC = 0x928;
		else if (strcmp(kernelName, "_Z17executeThirdLayerPfS_S_") == 0) //NN
			fbi_scheduler::gLastMemInstrPC = 0xeb0;
		else if (strcmp(kernelName, "_Z18executeFourthLayerPfS_S_") == 0) //NN
			fbi_scheduler::gLastMemInstrPC = 0x1438;
		else if (strcmp(kernelName, "_Z24solve_nqueen_cuda_kerneliiPjS_S_S_i") == 0) //NQU
			fbi_scheduler::gLastMemInstrPC = 0x510;
		else if (strcmp(kernelName, "_Z14dynproc_kerneliPiS_S_iiii") == 0) //pathfinder
			fbi_scheduler::gLastMemInstrPC = 0x3b0;
		else if (strcmp(kernelName, "_Z6renderPjP4Nodejjff") == 0) //RAY
			fbi_scheduler::gLastMemInstrPC = 0x2490;
		else if (strcmp(kernelName, "_Z12sha1_overlapPhiiiiS_") == 0) //STO
			fbi_scheduler::gLastMemInstrPC = 0xd8e8;
		else if (strcmp(kernelName, "_Z25cuda_initialize_variablesiPf") == 0) //cfd
			fbi_scheduler::gLastMemInstrPC = 0x0b8;
		else if (strcmp(kernelName, "_Z24cuda_compute_step_factoriPfS_S_") == 0) //cfd
			fbi_scheduler::gLastMemInstrPC = 0x230;
		else if (strcmp(kernelName, "_Z17cuda_compute_fluxiPiPfS0_S0_") == 0) //cfd
			fbi_scheduler::gLastMemInstrPC = 0x1950;
		else if (strcmp(kernelName, "_Z14cuda_time_stepiiPfS_S_S_") == 0) //cfd
			fbi_scheduler::gLastMemInstrPC = 0x1b38;
		else if (strcmp(kernelName, "_Z15BlackScholesGPUPfS_S_S_S_ffi") == 0) //BlackScholes
			fbi_scheduler::gLastMemInstrPC = 0x2e8;
		else if (strcmp(kernelName, "_Z21convolutionRowsKernelPfS_iii") == 0) //convolutionSeparable
			fbi_scheduler::gLastMemInstrPC = 0xb18;
		else if (strcmp(kernelName, "_Z24convolutionColumnsKernelPfS_iii") == 0) //convolutionSeparable
			fbi_scheduler::gLastMemInstrPC = 0x1730;
		else if (strcmp(kernelName, "_Z17histogram64KernelPjP5uint4j") == 0) //histogram
			fbi_scheduler::gLastMemInstrPC = 0x13b8;
		else if (strcmp(kernelName, "_Z18histogram256KernelPjS_j") == 0) //histogram
			fbi_scheduler::gLastMemInstrPC = 0x448;
		else if (strcmp(kernelName, "_Z22mergeHistogram64KernelPjS_j") == 0) //histogram
			fbi_scheduler::gLastMemInstrPC = 0x1698;
		else if (strcmp(kernelName, "_Z23mergeHistogram256KernelPjS_j") == 0) //histogram
			fbi_scheduler::gLastMemInstrPC = 0x750;
		else if (strcmp(kernelName, "_Z16inverseCNDKernelPfPjj") == 0) //MonteCarlo
			fbi_scheduler::gLastMemInstrPC = 0x36a8;
		else if (strcmp(kernelName, "_Z27MonteCarloOneBlockPerOptionPfi") == 0) //MonteCarlo
			fbi_scheduler::gLastMemInstrPC = 0x2eb0;
		else if (strcmp(kernelName, "_Z13scalarProdGPUPfS_S_ii") == 0) //scalarProd
			fbi_scheduler::gLastMemInstrPC = 0x240;
		else if (strcmp(kernelName, "_Z12lud_diagonalPfii") == 0) //lud
			fbi_scheduler::gLastMemInstrPC = 0x700;
		else if (strcmp(kernelName, "_Z13lud_perimeterPfii") == 0) //lud
			fbi_scheduler::gLastMemInstrPC = 0x1618;
		
		if (fbi_scheduler::gLastMemInstrPC != 0xdeaddead)
			printf("fbi_scheduler::gLastMemInstrPC = 0x%x\n", fbi_scheduler::gLastMemInstrPC);
	}
}

bool fbi_scheduler::mRemoveBlockFromList(std::list<unsigned int>& list, unsigned int tbNum)
{
	std::list<unsigned int>::iterator removePos = list.end();
	for (std::list<unsigned int>::iterator iter = list.begin(); iter != list.end(); iter++)
	{
		unsigned int tbId = (*iter);
		if (tbId == tbNum)
		{
			removePos = iter;
			break;
		}
	}
	if (removePos != list.end())
	{
		list.erase(removePos);
		return true;
	}
	return false;
}

void fbi_scheduler::mClear()
{
	unsigned int smId = m_shader->get_sid();

	std::list<unsigned int>& finishList = finishListVec.at(smId);
	finishList.clear();

	std::list<unsigned int>& barrierList = barrierListVec.at(smId);
	barrierList.clear();

	std::list<unsigned int>& inorderList = inorderListVec.at(smId);
	inorderList.clear();

	std::map<unsigned int, unsigned int>& numWarpsAtBarrierMap = gNumWarpsAtBarrierMapVec.at(smId);
	numWarpsAtBarrierMap.clear();
	for (unsigned int i = 0; i < MAX_NUM_TB_PER_SM; i++)
		numWarpsAtBarrierMap[i] = 0;

	std::map<unsigned int, unsigned int>& numWarpsAtFinishMap = gNumWarpsAtFinishMapVec.at(smId);
	numWarpsAtFinishMap.clear();
	for (unsigned int i = 0; i < MAX_NUM_TB_PER_SM; i++)
		numWarpsAtFinishMap[i] = 0;

	fbi_scheduler::numActiveWarps[smId] = 0xdeaddead;
	fbi_scheduler::numWarpsReachedLastMemInstr[smId] = 0xdeaddead;
	fbi_scheduler::issueLastMemInstr[smId] = false;
	fbi_scheduler::numReadyNonLastMemInstrs[smId] = 0;

	fbi_scheduler::gLastMemInstrPC = 0xdeaddead;
}

void fbi_scheduler::mRemoveBlockFromFinishList(unsigned int tbNum)
{
	unsigned int smId = m_shader->get_sid();
	std::list<unsigned int>& finishList = finishListVec.at(smId);

	bool removed = mRemoveBlockFromList(finishList, tbNum);
	bool printFlag = false;
	if (removed && printFlag && (smId == 0))
		printf("%llu: FBI removing from Finish List block %u\n", gpu_sim_cycle, tbNum);
}

void fbi_scheduler::mRemoveBlockFromBarrierList(unsigned int tbNum)
{
	unsigned int smId = m_shader->get_sid();
	std::list<unsigned int>& barrierList = barrierListVec.at(smId);

	bool removed = mRemoveBlockFromList(barrierList, tbNum);
	bool printFlag = false;
	if (removed && printFlag && (smId == 0))
		printf("%llu: FBI removing from Barrier List block %u\n", gpu_sim_cycle, tbNum);
}

void fbi_scheduler::mRemoveBlockFromInorderList(unsigned int tbNum)
{
	unsigned int smId = m_shader->get_sid();
	std::list<unsigned int>& inorderList = inorderListVec.at(smId);

	bool removed = mRemoveBlockFromList(inorderList, tbNum);
	bool printFlag = false;
	if (removed && printFlag && (smId == 0))
		printf("%llu: FBI removing from Inorder List block %u\n", gpu_sim_cycle, tbNum);
}

unsigned int gSmId = 0xdeaddead;

bool tb_num_warp_at_barrier_comp(unsigned int tbId1, unsigned int tbId2)
{
	std::map<unsigned int, unsigned int>& numWarpsAtBarrierMap = gNumWarpsAtBarrierMapVec.at(gSmId);

	unsigned int numWarpsAtBarrier1 = numWarpsAtBarrierMap[tbId1];
	unsigned int numWarpsAtBarrier2 = numWarpsAtBarrierMap[tbId2];

	if (numWarpsAtBarrier1 == numWarpsAtBarrier2)
	{
		unsigned int index = gSmId * MAX_NUM_TB_PER_SM + tbId1;
		unsigned int prog1 = gTBProgressArray[index];
		unsigned int phase1 = gTBPhaseArray[index];

		index = gSmId * MAX_NUM_TB_PER_SM + tbId2;
		unsigned int prog2 = gTBProgressArray[index];
		unsigned int phase2 = gTBPhaseArray[index];

		if (phase1 == phase2)
		{
			if (prog1 == prog2)
				return tbId1 < tbId2;
			return prog1 > prog2;
		}
		else
			return phase1 > phase2;
	}
	return numWarpsAtBarrier1 > numWarpsAtBarrier2;
}

bool tb_num_warp_at_finish_comp(unsigned int tbId1, unsigned int tbId2)
{
	std::map<unsigned int, unsigned int>& numWarpsAtFinishMap = gNumWarpsAtFinishMapVec.at(gSmId);

	unsigned int numWarpsAtFinish1 = numWarpsAtFinishMap[tbId1];
	unsigned int numWarpsAtFinish2 = numWarpsAtFinishMap[tbId2];

	if (numWarpsAtFinish1 == numWarpsAtFinish2)
	{
		unsigned int index = gSmId * MAX_NUM_TB_PER_SM + tbId1;
		unsigned int prog1 = gTBProgressArray[index];
		unsigned int phase1 = gTBPhaseArray[index];

		index = gSmId * MAX_NUM_TB_PER_SM + tbId2;
		unsigned int prog2 = gTBProgressArray[index];
		unsigned int phase2 = gTBPhaseArray[index];

		if (phase1 == phase2)
		{
			if (prog1 == prog2)
				return tbId1 < tbId2;
			return prog1 > prog2;
		}
		else
			return phase1 > phase2;
	}
	return numWarpsAtFinish1 > numWarpsAtFinish2;
}

bool fbi_scheduler::mIsHighPrioWarp(shd_warp_t* warpPtr)
{
	unsigned int smId = m_shader->get_sid();
	unsigned int tbId = warpPtr->get_cta_id();

	std::list<unsigned int>& finishList = finishListVec.at(smId);
	bool highPrio = mExistsInList(finishList, tbId);
	if (highPrio == false)
	{
		std::list<unsigned int>& barrierList = barrierListVec.at(smId);
		highPrio = mExistsInList(barrierList, tbId);
		if (highPrio == false)
		{
			bool moreTBsLeft = m_shader->m_cluster->get_gpu()->get_more_cta_left() ? true : false;
			if (moreTBsLeft == true)
			{
				std::list<unsigned int>& inorderList = inorderListVec.at(smId);
				std::list<unsigned int>::iterator iter = inorderList.begin();
				assert(iter != inorderList.end());
				if (tbId == (*iter))
					highPrio = true;
			}
		}
	}
	return highPrio;
}

bool fbi_scheduler::mExistsInList(std::list<unsigned int>& list, unsigned int tbNum)
{
	bool exists = false;
	for (std::list<unsigned int>::iterator iter = list.begin(); iter != list.end(); iter++)
	{
		if (tbNum == (*iter))
		{
			exists = true;
			break;
		}
	}
	return exists;
}

void fbi_scheduler::mSortFinishTBs()
{
	//sort TBs in decreasing order of number of warps that have finished

	unsigned int smId = m_shader->get_sid();
	std::list<unsigned int>& finishList = finishListVec.at(smId);
	if (finishList.size() > 1)
	{
		//gNumCallsToSortFinishTBs++;

		std::vector<unsigned int> vec;
		for (std::list<unsigned int>::iterator iter = finishList.begin(); iter != finishList.end(); iter++)
		{
			unsigned int tbId = (*iter);
			vec.push_back(tbId);
		}

		gSmId = smId;
		std::sort(vec.begin(), vec.end(), tb_num_warp_at_finish_comp);
		gSmId = 0xdeaddead;

		finishList.clear();
		for (unsigned int i = 0; i < vec.size(); i++)
		{
			unsigned int tbId = vec[i];
			finishList.push_back(tbId);
		}
		
		mPrintFinishTBs();
	}
}

void fbi_scheduler::mPrintFinishTBs()
{
	unsigned int smId = m_shader->get_sid();
	bool printFlag = false;
	if (printFlag && (smId == 1))
	{
		std::map<unsigned int, unsigned int>& numWarpsAtFinishMap = gNumWarpsAtFinishMapVec.at(smId);
		std::list<unsigned int>& finishList = finishListVec.at(smId);
		for (std::list<unsigned int>::iterator iter = finishList.begin(); iter != finishList.end(); iter++)
		{
			unsigned int tbId = (*iter);

			unsigned int numWarpsAtFinish = numWarpsAtFinishMap[tbId];
			unsigned int index = smId * MAX_NUM_TB_PER_SM + tbId;
			unsigned int prog = gTBProgressArray[index];
			unsigned int progPhase = gTBPhaseArray[index];
			if (prog > 0)
				printf("sm %u tb %u progress %u(%u), num warps at finish %u\n", smId, tbId, prog/32, progPhase, numWarpsAtFinish);
		}
		printf("\n");
	}
}

#define INCREASING_ORDER 0
#define DECREASING_ORDER 1

void fbi_scheduler::mInorderToFinishDataCollection(unsigned int tbNum)
{
	unsigned int smId = m_shader->get_sid();
	unsigned int pos = 1;
	std::list<unsigned int>& inorderList = inorderListVec.at(smId);
	for (std::list<unsigned int>::iterator iter = inorderList.begin();
	     iter != inorderList.end();
		 iter++)
	{
		if ((*iter) == tbNum)
		{
			gNumTBsGoingToFinishList[pos]++;

			bool printFlag = true;
			if ((smId == 0) && printFlag)
			{
     			unsigned index = smId * MAX_NUM_TB_PER_SM + tbNum;
				unsigned int tbProgress = gTBProgressArray[index];
				unsigned int tbPhase = gTBPhaseArray[index];
				printf("%llu: Inserting block %u sm %u into finish list, it was at position %u in inorder list with %u(%u) progress\n", gpu_sim_cycle, tbNum, smId, pos, tbProgress/32, tbPhase);
				if (pos > 2)
				{
    				for (std::vector<shd_warp_t*>::iterator iter = m_supervised_warps.begin(); 
						iter != m_supervised_warps.end(); 
						iter++) 
					{
						shd_warp_t* warpPtr = (*iter);
       					if ((warpPtr == NULL) || warpPtr->done_exit())
							continue;
						else if (warpPtr->get_cta_id() == tbNum)
						{
							unsigned int warpId = warpPtr->get_warp_id();
							unsigned int index = smId * MAX_NUM_WARP_PER_SM + warpId;
							unsigned int prog = gWarpProgressArray[index];
							unsigned int phase = gWarpPhaseArray[index];
							printf("Warp %u with %u(%u) progress\n", warpId, prog/32, phase);
						}
					}
				}
			}
			break;
		}
		pos++;
	}
}

void fbi_scheduler::mInsertBlockInFinishList(unsigned int tbNum)
{
	unsigned int smId = m_shader->get_sid();
	std::list<unsigned int>& finishList = finishListVec.at(smId);

	bool exists = mExistsInList(finishList, tbNum);

	if (exists == false)
	{
		finishList.push_back(tbNum);

		mSortFinishWarps(tbNum, INCREASING_ORDER);

		mInorderToFinishDataCollection(tbNum);
	}

	mSortFinishTBs();
}

void fbi_scheduler::mSortBarrierTBs()
{
	//sort TBs in decreasing order of number of warps that have reached barrier

	unsigned int smId = m_shader->get_sid();
	std::list<unsigned int>& barrierList = barrierListVec.at(smId);
	if (barrierList.size() > 1)
	{
		std::vector<unsigned int> vec;
		for (std::list<unsigned int>::iterator iter = barrierList.begin(); iter != barrierList.end(); iter++)
		{
			unsigned int tbId = (*iter);
			vec.push_back(tbId);
		}
	
		gSmId = smId;
		std::sort(vec.begin(), vec.end(), tb_num_warp_at_barrier_comp);
		gSmId = 0xdeaddead;

		barrierList.clear();
		for (unsigned int i = 0; i < vec.size(); i++)
		{
			unsigned int tbId = vec[i];
			barrierList.push_back(tbId);
		}

		mPrintBarrierTBs();
	}
}

void fbi_scheduler::mPrintBarrierTBs()
{
	unsigned int smId = m_shader->get_sid();
	bool printFlag = false;
	if (printFlag && (smId == 3))
	{
		std::map<unsigned int, unsigned int>& numWarpsAtBarrierMap = gNumWarpsAtBarrierMapVec.at(smId);
		std::list<unsigned int>& barrierList = barrierListVec.at(smId);
		for (std::list<unsigned int>::iterator iter = barrierList.begin(); iter != barrierList.end(); iter++)
		{
			unsigned int tbId = (*iter);

			unsigned int numWarpsAtBarrier = numWarpsAtBarrierMap[tbId];

			unsigned int index = smId * MAX_NUM_TB_PER_SM + tbId;
			unsigned int prog = gTBProgressArray[index];
			unsigned int progPhase = gTBPhaseArray[index];
			if (prog > 0)
				printf("sm %u tb %u progress %u(%u), num warps at barrier %u\n", smId, tbId, prog/32, progPhase, numWarpsAtBarrier);
		}
		printf("\n");
	}
}

//sort in non-decreasing order of tb progress
bool tb_prog_comp_dec(unsigned int tbId1, unsigned int tbId2)
{
	unsigned int index = gSmId * MAX_NUM_TB_PER_SM + tbId1;
	unsigned int prog1 = gTBProgressArray[index];
	unsigned int phase1 = gTBPhaseArray[index];

	index = gSmId * MAX_NUM_TB_PER_SM + tbId2;
	unsigned int prog2 = gTBProgressArray[index];
	unsigned int phase2 = gTBPhaseArray[index];

	if (phase1 == phase2)
	{
		if (prog1 == prog2)
			return tbId1 < tbId2;
		return prog1 > prog2;
	}
	else
		return phase1 > phase2;
}

bool tb_prog_comp_inc(unsigned int tbId1, unsigned int tbId2)
{
	unsigned int index = gSmId * MAX_NUM_TB_PER_SM + tbId1;
	unsigned int prog1 = gTBProgressArray[index];
	unsigned int phase1 = gTBPhaseArray[index];

	index = gSmId * MAX_NUM_TB_PER_SM + tbId2;
	unsigned int prog2 = gTBProgressArray[index];
	unsigned int phase2 = gTBPhaseArray[index];

	if (phase1 == phase2)
	{
		if (prog1 == prog2)
			return tbId1 < tbId2;
		return prog1 < prog2;
	}
	else
		return phase1 < phase2;
}

void fbi_scheduler::mInorderToBarrierDataCollection(unsigned int tbNum)
{
	unsigned int smId = m_shader->get_sid();
	unsigned int pos = 1;
	std::list<unsigned int>& inorderList = inorderListVec.at(smId);
	for (std::list<unsigned int>::iterator iter = inorderList.begin();
	     iter != inorderList.end();
		 iter++)
	{
		if ((*iter) == tbNum)
		{
			gNumTBsGoingToBarrierList[pos]++;

			//printf("%llu: Inserting block %u into barrier list, it was at position %u in inorder list\n", gpu_sim_cycle, tbNum, pos);
			break;
		}
		pos++;
	}
}

void fbi_scheduler::mInsertBlockInBarrierList(unsigned int tbNum)
{
	unsigned int smId = m_shader->get_sid();

	std::map<unsigned int, unsigned int>& numWarpsAtBarrierMap = gNumWarpsAtBarrierMapVec.at(smId);

	/*
	unsigned int pos = 1;
	std::list<unsigned int>& inorderList = inorderListVec.at(smId);
	for (std::list<unsigned int>::iterator iter = inorderList.begin();
	     iter != inorderList.end();
		 iter++)
	{
		if ((*iter) == tbNum)
			break;
		pos++;
	}
	*/

	std::list<unsigned int>& barrierList = barrierListVec.at(smId);
	bool exists = mExistsInList(barrierList, tbNum);

	if (exists == false)
	{
		//if (pos <= (gNumWarpsPerBlock / 2))
		//barrierList.push_back(tbNum);

		mSortBarrierWarps(tbNum, INCREASING_ORDER);

		mInorderToBarrierDataCollection(tbNum);
	}

	if (numWarpsAtBarrierMap[tbNum] == gNumWarpsPerBlock)
	{
		mRemoveBlockFromBarrierList(tbNum);
		numWarpsAtBarrierMap[tbNum] = 0;

		mSortInorderTBs();

		/*
		//move this block down in the inorder list
		std::list<unsigned int>& inorderList = inorderListVec.at(smId);
		std::list<unsigned int>::iterator prevIter = inorderList.end();
		for (std::list<unsigned int>::iterator currIter = inorderList.begin(); currIter != inorderList.end(); currIter++)
		{
			unsigned int currId = (*currIter);
			if (prevIter == inorderList.end())
			{
				if (currId != tbNum)
					continue;
				prevIter = currIter;
			}
			else
			{
				unsigned int prevId = (*prevIter);
				unsigned int currId = (*currIter);

				bool exists = mExistsInList(barrierList, (*currIter));
				if (exists)
				{
					(*prevIter) = currId;
					(*currIter) = prevId;
				}
				else 
				{
					bool result;

					gSmId = smId;
					if (moreTBsLeft)
						result = tb_prog_comp_dec(prevId, currId);
					else
						result = tb_prog_comp_inc(prevId, currId);
					gSmId = 0xdeaddead;

					if (result == false)
					{
						(*prevIter) = currId;
						(*currIter) = prevId;
					}
					else
						break;
				}
				prevIter = currIter;
			}
		}
		*/
	}
	else
	{
		/*
		//move this block up by one position in the inorder list
		std::list<unsigned int>& inorderList = inorderListVec.at(smId);
		std::list<unsigned int>::iterator prevIter = inorderList.end();
		for (std::list<unsigned int>::iterator currIter = inorderList.begin(); currIter != inorderList.end(); currIter++)
		{
			unsigned int currId = (*currIter);
			if (currId == tbNum)
			{
				if (prevIter != inorderList.end())
				{
					unsigned int prevId = (*prevIter);

					std::list<unsigned int>& barrierList = barrierListVec.at(smId);
					bool exists = mExistsInList(barrierList, prevId);

					if (exists == false)
					{
						(*prevIter) = currId;
						(*currIter) = prevId;
					}
					else
					{
						//check if prevId has more warps at barrier than currId
						gSmId = smId;
						bool result = tb_num_warp_at_barrier_comp(prevId, currId);
						gSmId = 0xdeaddead;
						if (result == false)
						{
							(*prevIter) = currId;
							(*currIter) = prevId;
						}
					}
				}
				break;
			}
			prevIter = currIter;
		}
		*/
	}

	mSortBarrierTBs();
}

void fbi_scheduler::mInsertBlockInInorderList(unsigned int tbNum)
{
	unsigned int smId = m_shader->get_sid();
	std::list<unsigned int>& inorderList = inorderListVec.at(smId);

	bool exists = mExistsInList(inorderList, tbNum);

	assert(exists == false);

	inorderList.push_back(tbNum);

	bool printFlag = false;
	if (printFlag && (smId == 0))
	{
       	unsigned index = smId * MAX_NUM_TB_PER_SM + tbNum;
		unsigned int tbProgress = gTBProgressArray[index];
		unsigned int tbPhase = gTBPhaseArray[index];
		printf("%llu: FBI inserting in Inorder List block %u, progress = %u(%u)\n", gpu_sim_cycle, tbNum, tbProgress/32, tbPhase);
	}
}

void fbi_scheduler::mInitArrays(unsigned int numSMs)
{
	if (fbi_scheduler::numActiveWarps == 0)
	{
		fbi_scheduler::numActiveWarps = new unsigned int[numSMs];
		fbi_scheduler::numWarpsReachedLastMemInstr = new unsigned int[numSMs];
		fbi_scheduler::issueLastMemInstr = new bool[numSMs];
		fbi_scheduler::numReadyNonLastMemInstrs = new unsigned int[numSMs];
		for (unsigned int i = 0; i < numSMs; i++)
		{
			fbi_scheduler::numActiveWarps[i] = 0xdeaddead;
			fbi_scheduler::numWarpsReachedLastMemInstr[i] = 0xdeaddead;
			fbi_scheduler::issueLastMemInstr[i] = false;
			fbi_scheduler::numReadyNonLastMemInstrs[i] = 0;
		}
	}
}

void fbi_scheduler::mInitLists(unsigned int numSMs)
{
	unsigned int smId = m_shader->get_sid();

	initNumWarpsAtBarrierMapVec(numSMs, smId);

	initNumWarpsAtFinishMapVec(numSMs, smId);

	finishListVec.resize(numSMs);
	barrierListVec.resize(numSMs);
	inorderListVec.resize(numSMs);
}

//sort in non-decreasing order of warp progress
bool warp_prog_comp_dec(shd_warp_t* warpPtr1, shd_warp_t* warpPtr2)
{
	unsigned int warpId1 = warpPtr1->get_warp_id();
	unsigned int index1 = gSmId * MAX_NUM_WARP_PER_SM + warpId1;
	unsigned int prog1 = gWarpProgressArray[index1];
	unsigned int phase1 = gWarpProgressArray[index1];

	unsigned int warpId2 = warpPtr2->get_warp_id();
	unsigned int index2 = gSmId * MAX_NUM_WARP_PER_SM + warpId2;
	unsigned int prog2 = gWarpProgressArray[index2];
	unsigned int phase2 = gWarpProgressArray[index2];

	if (phase1 == phase2)
	{
		if (prog1 == prog2)
			return warpId1 < warpId2;
		return prog1 > prog2;
	}
	else
		return phase1 > phase2;

}

//sort in non-increasing order of warp progress
bool warp_prog_comp_inc(shd_warp_t* warpPtr1, shd_warp_t* warpPtr2)
{
	unsigned int warpId1 = warpPtr1->get_warp_id();
	unsigned int index1 = gSmId * MAX_NUM_WARP_PER_SM + warpId1;
	unsigned int prog1 = gWarpProgressArray[index1];
	unsigned int phase1 = gWarpPhaseArray[index1];

	unsigned int warpId2 = warpPtr2->get_warp_id();
	unsigned int index2 = gSmId * MAX_NUM_WARP_PER_SM + warpId2;
	unsigned int prog2 = gWarpProgressArray[index2];
	unsigned int phase2 = gWarpPhaseArray[index2];

	if (phase1 == phase2)
	{
		if (prog1 == prog2)
			return warpId1 < warpId2;
		return prog1 < prog2;
	}
	else
		return phase1 < phase2;
}

void fbi_scheduler::mSortFinishWarps(unsigned int tbId, unsigned int order)
{
	gNumCallsToSortFinishWarps++;
	mSortWarps(tbId, order);
}

void fbi_scheduler::mSortBarrierWarps(unsigned int tbId, unsigned int order)
{
	gNumCallsToSortBarrierWarps++;
	mSortWarps(tbId, order);
}

void fbi_scheduler::mSortInorderWarps(unsigned int tbId, unsigned int order)
{
	gNumCallsToSortInorderWarps++;
	mSortWarps(tbId, order);
}

void fbi_scheduler::mSortWarps(unsigned int tbId, unsigned int order)
{
	std::vector<shd_warp_t*> warpVec1;
	std::vector<shd_warp_t*> warpVec2;
	std::vector<shd_warp_t*> warpVec3;

	gNumCallsToSortWarps++;

	unsigned int whichVec = 1;
    for (std::vector<shd_warp_t*>::iterator iter = m_supervised_warps.begin(); 
		iter != m_supervised_warps.end(); 
		iter++) 
	{
		shd_warp_t* warpPtr = (*iter);
       	if ((warpPtr == NULL) || warpPtr->done_exit())
			whichVec = 3;
		else if (warpPtr->get_cta_id() == tbId)
			whichVec = 2;
		else if (whichVec == 2)
			whichVec = 3;
		
		if (whichVec == 1)
			warpVec1.push_back(warpPtr);
		else if (whichVec == 2)
			warpVec2.push_back(warpPtr);
		else
		{
			assert(whichVec == 3);
			warpVec3.push_back(warpPtr);
		}
	}

	unsigned int smId = m_shader->get_sid();
	bool printFlag = false;
	if (printFlag && (smId == 0))
	{
		printf("Before sorting warp %s order: ", (order == INCREASING_ORDER) ? "increasing" : "decreasing");
    	for (std::vector<shd_warp_t*>::iterator iter = warpVec2.begin(); iter != warpVec2.end(); iter++) 
		{
			shd_warp_t* warpPtr = (*iter);
			unsigned int warpId = warpPtr->get_warp_id();
			unsigned int index = smId * MAX_NUM_WARP_PER_SM + warpId;
			unsigned int prog = gWarpProgressArray[index];
			unsigned int phase = gWarpPhaseArray[index];
			printf(" %u(%u, %u)", warpId, prog, phase);
		}
		printf("\n");
	}
	gSmId = smId;
	if (order == INCREASING_ORDER)
		std::sort(warpVec2.begin(), warpVec2.end(), warp_prog_comp_inc);
	else
		std::sort(warpVec2.begin(), warpVec2.end(), warp_prog_comp_dec);
	gSmId = 0xdeaddead;

	printFlag = false;
	if (printFlag && (smId == 0))
	{
		printf("After sorting tb %u, warp %s order: ", tbId, (order == INCREASING_ORDER) ? "increasing" : "decreasing");
    	for (std::vector<shd_warp_t*>::iterator iter = warpVec2.begin(); iter != warpVec2.end(); iter++) 
		{
			shd_warp_t* warpPtr = (*iter);
			unsigned int warpId = warpPtr->get_warp_id();
			unsigned int index = smId * MAX_NUM_WARP_PER_SM + warpId;
			unsigned int prog = gWarpProgressArray[index];
			unsigned int phase = gWarpPhaseArray[index];
			printf(" %u(%u, %u)", warpId, prog, phase);
		}
		printf("\n");
	}

	m_supervised_warps.clear();

	for (unsigned int i = 0; i < warpVec1.size(); i++)
		m_supervised_warps.push_back(warpVec1[i]);

	for (unsigned int i = 0; i < warpVec2.size(); i++)
		m_supervised_warps.push_back(warpVec2[i]);

	for (unsigned int i = 0; i < warpVec3.size(); i++)
		m_supervised_warps.push_back(warpVec3[i]);
}

void fbi_scheduler::mInsertWarps(unsigned int tbNum)
{
   	for (std::vector<shd_warp_t*>::const_iterator iter = m_supervised_warps.begin(); 
		iter != m_supervised_warps.end(); 
		iter++) 
	{
		shd_warp_t* warpPtr = (*iter);
		if (warpPtr->get_cta_id() == tbNum)
			m_next_cycle_prioritized_warps.push_back(warpPtr);
	}
}

void fbi_scheduler::mSortInorderTBs()
{
	if (this->m_id != 0)
		return;

	gNumCallsToSortInorderTBs++;
	unsigned int smId = m_shader->get_sid();
	std::list<unsigned int>& inorderList = inorderListVec.at(smId);

	std::vector<unsigned int> vec;

	for (std::list<unsigned int>::iterator iter = inorderList.begin(); iter != inorderList.end(); iter++)
		vec.push_back(*iter);

	bool moreTBsLeft = m_shader->m_cluster->get_gpu()->get_more_cta_left() ? true : false;
	if (moreTBsLeft)
	{
		gSmId = smId;
		std::sort(vec.begin(), vec.end(), tb_prog_comp_dec);
		gSmId = 0xdeaddead;

		/*
		std::list<unsigned int>& barrierList = barrierListVec.at(smId);
		if (vec.size() > 1)
		{
			bool repeatFlag = true;
			while (repeatFlag)
			{
				repeatFlag = false;

				for (unsigned int i = 0; i < vec.size(); i += 2)
				{
					if ((i+1) < vec.size())
					{
						unsigned int tbId1 = vec[i];
						unsigned int tbId2 = vec[i+1];
						if ((mExistsInList(barrierList, tbId1) == false) && (mExistsInList(barrierList, tbId2) == false))
						{
							gSmId = smId;
							bool result = tb_prog_comp_dec(tbId1, tbId2);
							gSmId = 0xdeaddead;

							if (result == false)
							{
								//swap tbId1 and tbId2
								vec[i] = tbId2;
								vec[i+1] = tbId1;
								sortOrderChanged = true;
								repeatFlag = true;
							}
						}
					}
				}
				for (unsigned int i = 1; i < vec.size(); i += 2)
				{
					if ((i+1) < vec.size())
					{
						unsigned int tbId1 = vec[i];
						unsigned int tbId2 = vec[i+1];
						if ((mExistsInList(barrierList, tbId1) == false) && (mExistsInList(barrierList, tbId2) == false))
						{
							gSmId = smId;
							bool result = tb_prog_comp_dec(tbId1, tbId2);
							gSmId = 0xdeaddead;

							if (result == false)
							{
								//swap tbId1 and tbId2
								vec[i] = tbId2;
								vec[i+1] = tbId1;
								sortOrderChanged = true;
								repeatFlag = true;
							}
						}
					}
				}
			}
		}
		*/
	}
	else
	{
		//gSmId = smId;
		//std::sort(vec.begin(), vec.end(), tb_prog_comp_inc);
		//gSmId = 0xdeaddead;

		if (vec.size() > 1)
		{
			unsigned int last = vec[vec.size() - 1];

			for (int i = vec.size() - 2; i >= 0; i--)
				vec[i + 1] = vec[i];

			vec[0] = last;
		}

		/*
		bool anyTbAtBarrier = false;
		for (unsigned int i = 0; i < vec.size(); i++)
		{
			std::list<unsigned int>& barrierList = barrierListVec.at(smId);
			if (mExistsInList(barrierList, i))
			{
				anyTbAtBarrier = true;
				break;
			}
		}
		if ((anyTbAtBarrier == false) && (vec.size() > 1))
		{
			unsigned int last = vec[vec.size() - 1];

			for (int i = vec.size() - 2; i >= 0; i--)
				vec[i + 1] = vec[i];

			vec[0] = last;
			sortOrderChanged = true;
		}
		*/
	}

	inorderList.clear();

	for (unsigned int i = 0; i < vec.size(); i++)
	{
		unsigned int tbId = vec[i];
		inorderList.push_back(tbId);
	}

	mPrintInorderTBs();
}

void fbi_scheduler::mPrintInorderTBs()
{
	unsigned int smId = m_shader->get_sid();
	bool printFlag = false;
	if (printFlag && (smId == 3))
	{
		std::list<unsigned int>& inorderList = inorderListVec.at(smId);
		printf("%llu: inorder TBs on sm %u\n", gpu_sim_cycle, smId);
		for (std::list<unsigned int>::iterator iter = inorderList.begin(); iter != inorderList.end(); iter++)
		{
			unsigned int tbId = (*iter);

			unsigned int index = smId * MAX_NUM_TB_PER_SM + tbId;
			unsigned int prog = gTBProgressArray[index];
			unsigned int progPhase = gTBPhaseArray[index];
			if (prog > 0)
				printf("sm %u tb %u progress %u(%u)\n", smId, tbId, prog/32, progPhase);
		}
		printf("\n");
	}
}

void fbi_scheduler::mInsertInorderWarps(std::set<unsigned int>& barrierTBSet, std::set<unsigned int>& insertedInorderTBSet)
{
	unsigned int smId = m_shader->get_sid();
	std::list<unsigned int>& inorderList = inorderListVec.at(smId);

	for (std::list<unsigned int>::iterator iter = inorderList.begin(); iter != inorderList.end(); iter++)
	{
		unsigned int tbId = (*iter);
		if ((insertedInorderTBSet.find(tbId) == insertedInorderTBSet.end()) && 
			(barrierTBSet.find(tbId) == barrierTBSet.end()))
		{
			mInsertWarps(tbId);
		}
	}
}

void fbi_scheduler::mInsertBarrierWarps(std::set<unsigned int>& barrierTBSet, std::set<unsigned int>& insertedInorderTBSet)
{
	unsigned int smId = m_shader->get_sid();
	std::list<unsigned int>& barrierList = barrierListVec.at(smId);

	for (std::list<unsigned int>::iterator iter = barrierList.begin(); iter != barrierList.end(); iter++)
	{
		unsigned int tbId = (*iter);
		if (insertedInorderTBSet.find(tbId) == insertedInorderTBSet.end())
		{
			mInsertWarps(tbId);
			barrierTBSet.insert(tbId);
		}
	}
}

void fbi_scheduler::mInsertFinishWarps(std::set<unsigned int>& insertedInorderTBSet)
{
	unsigned int smId = m_shader->get_sid();
	std::list<unsigned int>& finishList = finishListVec.at(smId);
	for (std::list<unsigned int>::iterator iter = finishList.begin(); iter != finishList.end(); iter++)
	{
		unsigned int tbId = (*iter);
		if (insertedInorderTBSet.find(tbId) == insertedInorderTBSet.end())
			mInsertWarps(tbId);
	}
}

bool fbi_scheduler::mMoveFinishToInorder()
{
	unsigned int smId = m_shader->get_sid();
	std::list<unsigned int>& finishList = finishListVec.at(smId);

	if (finishList.empty() == false)
	{
		std::list<unsigned int>& inorderList = inorderListVec.at(smId);

		for (std::list<unsigned int>::iterator iter = finishList.begin(); iter != finishList.end(); iter++)
		{
			unsigned int tbId = (*iter);
			inorderList.push_front(tbId);
		}
		finishList.clear();
		return true;
	}
	return false;
}

void fbi_scheduler::mCountNumReadyNonLastMemInstrs()
{
	if ((fbi_scheduler::gLastMemInstrPC == 0xdeaddead) || (fbi_scheduler::gLastMemInstrPC == 0xdeadbeef))
		return;

	unsigned int smId = m_shader->get_sid();

    for (std::vector<shd_warp_t*>::const_iterator iter = m_supervised_warps.begin(); 
	     iter != m_supervised_warps.end(); 
		 iter++) 
	{
       	// Don't consider warps that are not yet valid
		shd_warp_t* warpPtr = (*iter);
       	if ((warpPtr == NULL) || warpPtr->done_exit())
			continue;

       	unsigned warp_id = warpPtr->get_warp_id();

       	if (!warp(warp_id).ibuffer_empty())
		{
			if (m_shader->warp_waiting_at_barrier(warp_id))
				continue;

           	const warp_inst_t* pI = warp(warp_id).ibuffer_next_inst();

           	if(pI) 
			{
           		unsigned pc, rpc;

           		m_simt_stack[warp_id]->get_pdom_stack_top_info(&pc, &rpc);

               	if (pc == pI->pc ) 
				{
                   	if (!m_scoreboard->checkCollision(warp_id, pI)) 
					{
                        if ((pI->op == LOAD_OP) || (pI->op == STORE_OP) || (pI->op == MEMORY_BARRIER_OP)) 
						{
							if (pI->pc != fbi_scheduler::gLastMemInstrPC)
								fbi_scheduler::numReadyNonLastMemInstrs[smId]++;
							else
								fbi_scheduler::numWarpsReachedLastMemInstr[smId]++;
						}
					}
               	}
           	} 
       	}
   	}
}

unsigned int fbi_scheduler::mCountNumActiveWarps()
{
	unsigned int activeWarps = 0;
    for (std::vector<shd_warp_t*>::const_iterator iter = m_supervised_warps.begin(); 
	     iter != m_supervised_warps.end(); 
		 iter++) 
	{
       	// Don't consider warps that are not yet valid
		shd_warp_t* warpPtr = (*iter);
       	if ((warpPtr == NULL) || warpPtr->done_exit())
			continue;

		activeWarps++;
   	}
	return activeWarps;
}

void fbi_scheduler::order_warps()
{
	unsigned int smId = m_shader->get_sid();

	bool moreTBsLeft = m_shader->m_cluster->get_gpu()->get_more_cta_left() ? true : false;

    m_next_cycle_prioritized_warps.clear();

	std::set<unsigned int> insertedInorderTBSet;
	/*
	std::list<unsigned int>& inorderList = inorderListVec.at(smId);
	for (std::list<unsigned int>::iterator iter = inorderList.begin(); iter != inorderList.end(); iter++)
	{
		unsigned int tbId = (*iter);
		unsigned index = smId * MAX_NUM_TB_PER_SM + tbId;
	
		if (gTBWarpsExecutingMemInstr)
		{
			if ((gTBWarpsExecutingMemInstr[index] > 0) && (gTBWarpsExecutingMemInstr[index] < gNumWarpsPerBlock))
			{
				//this tb has some warps which have issued global mem instruction and some have not
				mInsertWarps(tbId);
				insertedInorderTBSet.insert(tbId);
				break;
			}
		}
	}
	*/

	bool movedFinishTBs = false;
	if (moreTBsLeft == true)
		mInsertFinishWarps(insertedInorderTBSet);
	else
	{
		//no more TBs waiting, so move the TBs in finishList to inorderList
		movedFinishTBs = mMoveFinishToInorder();
	}

	std::set<unsigned int> barrierTBSet;
	mInsertBarrierWarps(barrierTBSet, insertedInorderTBSet);

	if (movedFinishTBs || ((gpu_sim_cycle % SORT_THRESHOLD) == 0))
	{
		mSortInorderTBs();

		unsigned int order;
		if (moreTBsLeft)
			order = DECREASING_ORDER;
		else
			order = INCREASING_ORDER;

		std::list<unsigned int>& inorderList = inorderListVec.at(smId);

		for (std::list<unsigned int>::iterator iter = inorderList.begin(); iter != inorderList.end(); iter++)
		{
			unsigned int tbId = (*iter);
	
			if (barrierTBSet.find(tbId) == barrierTBSet.end())
				mSortInorderWarps(tbId, order);
		}
	}

	mInsertInorderWarps(barrierTBSet, insertedInorderTBSet);

	assert(m_next_cycle_prioritized_warps.size() <= NUM_WARPS_PER_SCHEDULER);

	bool printFlag = false;
	if (printFlag && (smId == 3) && ((gpu_sim_cycle % SORT_THRESHOLD) == 0))
	{
		printf("%llu: warp order: ", gpu_sim_cycle);
   		for (std::vector<shd_warp_t*>::const_iterator iter = m_next_cycle_prioritized_warps.begin(); 
			iter != m_next_cycle_prioritized_warps.end(); 
			iter++) 
		{
			shd_warp_t* warpPtr = (*iter);
			if ((warpPtr == NULL) || (warpPtr->done_exit()))
				continue;
			printf("%u_%u ", warpPtr->get_cta_id(), warpPtr->get_warp_id());
		}
		printf("\n");
	}
}

// return the next pc of a thread 
address_type shader_core_ctx::next_pc( int tid ) const
{
    if( tid == -1 ) 
        return -1;
    ptx_thread_info *the_thread = m_thread[tid];
    if ( the_thread == NULL )
        return -1;
    return the_thread->get_pc(); // PC should already be updatd to next PC at this point (was set in shader_decode() last time thread ran)
}

void gpgpu_sim::get_pdom_stack_top_info( unsigned sid, unsigned tid, unsigned *pc, unsigned *rpc )
{
    unsigned cluster_id = m_shader_config->sid_to_cluster(sid);
    m_cluster[cluster_id]->get_pdom_stack_top_info(sid,tid,pc,rpc);
}

void shader_core_ctx::get_pdom_stack_top_info( unsigned tid, unsigned *pc, unsigned *rpc ) const
{
    unsigned warp_id = tid/m_config->warp_size;
    m_simt_stack[warp_id]->get_pdom_stack_top_info(pc,rpc);
}

unsigned int gCoalStall1 = 0;
unsigned int gCoalStall2 = 0;
unsigned int gDbgCnt1 = 0;
unsigned int gDbgCnt1_1 = 0;
unsigned int gDbgCnt1_2 = 0;
unsigned int gDbgCnt1_3 = 0;
unsigned int gDbgCnt1_4 = 0;
unsigned int gDbgCnt1_5 = 0;
unsigned int gDbgCnt1_6 = 0;
unsigned int gDbgCnt1_7 = 0;
unsigned int gDbgCnt1_8 = 0;
unsigned int gDbgCnt2 = 0;
unsigned int gDbgCnt2_1 = 0;
unsigned int gDbgCnt2_2 = 0;
unsigned int prevStallCnts_0 = 0;
unsigned int prevStallCnts_1 = 0;
unsigned int prevStallCnts_2 = 0;

void shader_core_stats::print( FILE* fout ) const
{
	unsigned long long  thread_icount_uarch=0;
	unsigned long long  warp_icount_uarch=0;

    for(unsigned i=0; i < m_config->num_shader(); i++) {
        thread_icount_uarch += m_num_sim_insn[i];
        warp_icount_uarch += m_num_sim_winsn[i];
    }
    fprintf(fout,"gpgpu_n_tot_thrd_icount = %lld\n", thread_icount_uarch);
    fprintf(fout,"gpgpu_n_tot_w_icount = %lld\n", warp_icount_uarch);

    fprintf(fout,"gpgpu_n_stall_shd_mem = %d\n", gpgpu_n_stall_shd_mem );
    fprintf(fout,"gpgpu_n_mem_read_local = %d\n", gpgpu_n_mem_read_local);
    fprintf(fout,"gpgpu_n_mem_write_local = %d\n", gpgpu_n_mem_write_local);
    fprintf(fout,"gpgpu_n_mem_read_global = %d\n", gpgpu_n_mem_read_global);
    fprintf(fout,"gpgpu_n_mem_write_global = %d\n", gpgpu_n_mem_write_global);
    fprintf(fout,"gpgpu_n_mem_texture = %d\n", gpgpu_n_mem_texture);
    fprintf(fout,"gpgpu_n_mem_const = %d\n", gpgpu_n_mem_const);

   fprintf(fout, "gpgpu_n_load_insn  = %d\n", gpgpu_n_load_insn);
   fprintf(fout, "gpgpu_n_store_insn = %d\n", gpgpu_n_store_insn);
   fprintf(fout, "gpgpu_n_shmem_insn = %d\n", gpgpu_n_shmem_insn);
   fprintf(fout, "gpgpu_n_tex_insn = %d\n", gpgpu_n_tex_insn);
   fprintf(fout, "gpgpu_n_const_mem_insn = %d\n", gpgpu_n_const_insn);
   fprintf(fout, "gpgpu_n_param_mem_insn = %d\n", gpgpu_n_param_insn);

   fprintf(fout, "gpgpu_n_shmem_bkconflict = %d\n", gpgpu_n_shmem_bkconflict);
   fprintf(fout, "gpgpu_n_cache_bkconflict = %d\n", gpgpu_n_cache_bkconflict);   

   fprintf(fout, "gpgpu_n_intrawarp_mshr_merge = %d\n", gpgpu_n_intrawarp_mshr_merge);
   fprintf(fout, "gpgpu_n_cmem_portconflict = %d\n", gpgpu_n_cmem_portconflict);

   fprintf(fout, "gpgpu_stall_shd_mem[c_mem][bk_conf] = %d\n", gpu_stall_shd_mem_breakdown[C_MEM][BK_CONF]);
   fprintf(fout, "gpgpu_stall_shd_mem[c_mem][mshr_rc] = %d\n", gpu_stall_shd_mem_breakdown[C_MEM][MSHR_RC_FAIL]);
   fprintf(fout, "gpgpu_stall_shd_mem[c_mem][icnt_rc] = %d\n", gpu_stall_shd_mem_breakdown[C_MEM][ICNT_RC_FAIL]);
   fprintf(fout, "gpgpu_stall_shd_mem[c_mem][data_port_stall] = %d\n", gpu_stall_shd_mem_breakdown[C_MEM][DATA_PORT_STALL]);
   fprintf(fout, "gpgpu_stall_shd_mem[t_mem][mshr_rc] = %d\n", gpu_stall_shd_mem_breakdown[T_MEM][MSHR_RC_FAIL]);
   fprintf(fout, "gpgpu_stall_shd_mem[t_mem][icnt_rc] = %d\n", gpu_stall_shd_mem_breakdown[T_MEM][ICNT_RC_FAIL]);
   fprintf(fout, "gpgpu_stall_shd_mem[t_mem][data_port_stall] = %d\n", gpu_stall_shd_mem_breakdown[T_MEM][DATA_PORT_STALL]);
   fprintf(fout, "gpgpu_stall_shd_mem[s_mem][bk_conf] = %d\n", gpu_stall_shd_mem_breakdown[S_MEM][BK_CONF]);
   fprintf(fout, "gpgpu_stall_shd_mem[gl_mem][bk_conf] = %d\n", 
           gpu_stall_shd_mem_breakdown[G_MEM_LD][BK_CONF] + 
           gpu_stall_shd_mem_breakdown[G_MEM_ST][BK_CONF] + 
           gpu_stall_shd_mem_breakdown[L_MEM_LD][BK_CONF] + 
           gpu_stall_shd_mem_breakdown[L_MEM_ST][BK_CONF]   
           ); // coalescing stall at data cache 
   fprintf(fout, "gpgpu_stall_shd_mem[gl_mem][coal_stall] = %d\n", 
           gpu_stall_shd_mem_breakdown[G_MEM_LD][COAL_STALL] + 
           gpu_stall_shd_mem_breakdown[G_MEM_ST][COAL_STALL] + 
           gpu_stall_shd_mem_breakdown[L_MEM_LD][COAL_STALL] + 
           gpu_stall_shd_mem_breakdown[L_MEM_ST][COAL_STALL]    
           ); // coalescing stall + bank conflict at data cache 

   printf("gCoalStall1 = %u, gCoalStall2 = %u, gDbgCnt1 = %u, gDbgCnt2 = %u\n", gCoalStall1, gCoalStall2, gDbgCnt1, gDbgCnt2);
   printf("gDbgCnt1_1 = %u, gDbgCnt1_2 = %u, gDbgCnt1_3 = %u, gDbgCnt1_7 = %u, gDbgCnt1_8 = %u, gDbgCnt2_1 = %u, gDbgCnt2_2 = %u\n", gDbgCnt1_1, gDbgCnt1_2, gDbgCnt1_3, gDbgCnt1_7, gDbgCnt1_8, gDbgCnt2_1, gDbgCnt2_2);

   fprintf(fout, "gpgpu_stall_shd_mem[gl_mem][data_port_stall] = %d\n", 
           gpu_stall_shd_mem_breakdown[G_MEM_LD][DATA_PORT_STALL] + 
           gpu_stall_shd_mem_breakdown[G_MEM_ST][DATA_PORT_STALL] + 
           gpu_stall_shd_mem_breakdown[L_MEM_LD][DATA_PORT_STALL] + 
           gpu_stall_shd_mem_breakdown[L_MEM_ST][DATA_PORT_STALL]    
           ); // data port stall at data cache 
   fprintf(fout, "gpgpu_stall_shd_mem[g_mem_ld][mshr_rc] = %d\n", gpu_stall_shd_mem_breakdown[G_MEM_LD][MSHR_RC_FAIL]);
   fprintf(fout, "gpgpu_stall_shd_mem[g_mem_ld][icnt_rc] = %d\n", gpu_stall_shd_mem_breakdown[G_MEM_LD][ICNT_RC_FAIL]);
   fprintf(fout, "gpgpu_stall_shd_mem[g_mem_ld][wb_icnt_rc] = %d\n", gpu_stall_shd_mem_breakdown[G_MEM_LD][WB_ICNT_RC_FAIL]);
   fprintf(fout, "gpgpu_stall_shd_mem[g_mem_ld][wb_rsrv_fail] = %d\n", gpu_stall_shd_mem_breakdown[G_MEM_LD][WB_CACHE_RSRV_FAIL]);
   fprintf(fout, "gpgpu_stall_shd_mem[g_mem_st][mshr_rc] = %d\n", gpu_stall_shd_mem_breakdown[G_MEM_ST][MSHR_RC_FAIL]);
   fprintf(fout, "gpgpu_stall_shd_mem[g_mem_st][icnt_rc] = %d\n", gpu_stall_shd_mem_breakdown[G_MEM_ST][ICNT_RC_FAIL]);
   fprintf(fout, "gpgpu_stall_shd_mem[g_mem_st][wb_icnt_rc] = %d\n", gpu_stall_shd_mem_breakdown[G_MEM_ST][WB_ICNT_RC_FAIL]);
   fprintf(fout, "gpgpu_stall_shd_mem[g_mem_st][wb_rsrv_fail] = %d\n", gpu_stall_shd_mem_breakdown[G_MEM_ST][WB_CACHE_RSRV_FAIL]);
   fprintf(fout, "gpgpu_stall_shd_mem[l_mem_ld][mshr_rc] = %d\n", gpu_stall_shd_mem_breakdown[L_MEM_LD][MSHR_RC_FAIL]);
   fprintf(fout, "gpgpu_stall_shd_mem[l_mem_ld][icnt_rc] = %d\n", gpu_stall_shd_mem_breakdown[L_MEM_LD][ICNT_RC_FAIL]);
   fprintf(fout, "gpgpu_stall_shd_mem[l_mem_ld][wb_icnt_rc] = %d\n", gpu_stall_shd_mem_breakdown[L_MEM_LD][WB_ICNT_RC_FAIL]);
   fprintf(fout, "gpgpu_stall_shd_mem[l_mem_ld][wb_rsrv_fail] = %d\n", gpu_stall_shd_mem_breakdown[L_MEM_LD][WB_CACHE_RSRV_FAIL]);
   fprintf(fout, "gpgpu_stall_shd_mem[l_mem_st][mshr_rc] = %d\n", gpu_stall_shd_mem_breakdown[L_MEM_ST][MSHR_RC_FAIL]);
   fprintf(fout, "gpgpu_stall_shd_mem[l_mem_st][icnt_rc] = %d\n", gpu_stall_shd_mem_breakdown[L_MEM_ST][ICNT_RC_FAIL]);
   fprintf(fout, "gpgpu_stall_shd_mem[l_mem_ld][wb_icnt_rc] = %d\n", gpu_stall_shd_mem_breakdown[L_MEM_ST][WB_ICNT_RC_FAIL]);
   fprintf(fout, "gpgpu_stall_shd_mem[l_mem_ld][wb_rsrv_fail] = %d\n", gpu_stall_shd_mem_breakdown[L_MEM_ST][WB_CACHE_RSRV_FAIL]);

   fprintf(fout, "gpu_reg_bank_conflict_stalls = %d\n", gpu_reg_bank_conflict_stalls);

   fprintf(fout, "Warp Occupancy Distribution:\n");
   fprintf(fout, "Stall:%d(%u)\t", shader_cycle_distro[2], (shader_cycle_distro[2] - prevStallCnts_2));
   fprintf(fout, "W0_Idle:%d(%u)\t", shader_cycle_distro[0], (shader_cycle_distro[0] - prevStallCnts_0));
   fprintf(fout, "W0_Scoreboard:%d(%u)", shader_cycle_distro[1], (shader_cycle_distro[1] - prevStallCnts_1));
   for (unsigned i = 3; i < m_config->warp_size + 3; i++) 
      fprintf(fout, "\tW%d:%d", i-2, shader_cycle_distro[i]);
   fprintf(fout, "\n");
   fprintf(fout, "Mem pipeline stall:%u, Sfu pipeline stall:%u, sp pipeline stall:%u, mem & sfu pipeline stall:%u, mem & sp pipeline tall:%u, mem & sfu & sp pipeline stall:%u, sfu & sp pipeline stall:%u\n", gMemPipeLineStall, gSfuPipeLineStall, gSpPipeLineStall, gMemSfuPipeLineStall, gMemSpPipeLineStall, gMemSfuSpPipeLineStall, gSfuSpPipeLineStall);

   m_outgoing_traffic_stats->print(fout); 
   m_incoming_traffic_stats->print(fout); 

	if (gNumSMs != 0)
	{
   		unsigned int actualStallCycles = ((shader_cycle_distro[2] + shader_cycle_distro[1] + shader_cycle_distro[0])  - (prevStallCnts_2 + prevStallCnts_1 + prevStallCnts_0)) / (gNumSMs * 2);
   		fprintf(fout, "RL: Exploration Cycle Count = %u\n", rl_scheduler::gExplorationCnt / (gNumSMs * 2));
   		fprintf(fout, "RL: Actual Run Cycle Count = %llu\n", gpu_sim_cycle - actualStallCycles);

		gNumWarpsPerBlock = 0;
		rl_scheduler::gExplorationCnt = 0;
		if (rl_scheduler::gCumulativeRewardPenalty)
			printf("gMaxReward = %f, gMinReward = %f\n", gMaxReward, gMinReward);
	}
   prevStallCnts_0 = shader_cycle_distro[0];
   prevStallCnts_1 = shader_cycle_distro[1];
   prevStallCnts_2 = shader_cycle_distro[2];
	rl_scheduler::gNumWarpsExecutingMemInstrGPU = 0;
	rl_scheduler::gNumReqsInMemSchedQs = 0;
	rl_scheduler::gNumMemSchedQsLoaded = 0;
	if (gGTCLongLatMemInstrCache)
	{
		for (unsigned int i = 0; i < GTC_LONG_LAT_MEM_INSTR_CACHE_SIZE; i++)
			gGTCLongLatMemInstrCache[i] = 0;
	}
	if (gSFULongLatInstrCache)
	{
		for (unsigned int i = 0; i < SFU_LONG_LAT_INSTR_CACHE_SIZE; i++)
			gSFULongLatInstrCache[i] = 0;
	}
}

void shader_core_stats::event_warp_issued( unsigned s_id, unsigned warp_id, unsigned num_issued, unsigned dynamic_warp_id ) {
    assert( warp_id <= m_config->max_warps_per_shader );
    for ( unsigned i = 0; i < num_issued; ++i ) {
        if ( m_shader_dynamic_warp_issue_distro[ s_id ].size() <= dynamic_warp_id ) {
            m_shader_dynamic_warp_issue_distro[ s_id ].resize(dynamic_warp_id + 1);
        }
        ++m_shader_dynamic_warp_issue_distro[ s_id ][ dynamic_warp_id ];
        if ( m_shader_warp_slot_issue_distro[ s_id ].size() <= warp_id ) {
            m_shader_warp_slot_issue_distro[ s_id ].resize(warp_id + 1);
        }
        ++m_shader_warp_slot_issue_distro[ s_id ][ warp_id ];
    }
}

void shader_core_stats::visualizer_print( gzFile visualizer_file )
{
    // warp divergence breakdown
    gzprintf(visualizer_file, "WarpDivergenceBreakdown:");
    unsigned int total=0;
    unsigned int cf = (m_config->gpgpu_warpdistro_shader==-1)?m_config->num_shader():1;
    gzprintf(visualizer_file, " %d", (shader_cycle_distro[0] - last_shader_cycle_distro[0]) / cf );
    gzprintf(visualizer_file, " %d", (shader_cycle_distro[1] - last_shader_cycle_distro[1]) / cf );
    gzprintf(visualizer_file, " %d", (shader_cycle_distro[2] - last_shader_cycle_distro[2]) / cf );
    for (unsigned i=0; i<m_config->warp_size+3; i++) {
       if ( i>=3 ) {
          total += (shader_cycle_distro[i] - last_shader_cycle_distro[i]);
          if ( ((i-3) % (m_config->warp_size/8)) == ((m_config->warp_size/8)-1) ) {
             gzprintf(visualizer_file, " %d", total / cf );
             total=0;
          }
       }
       last_shader_cycle_distro[i] = shader_cycle_distro[i];
    }
    gzprintf(visualizer_file,"\n");

    // warp issue breakdown
    unsigned sid = m_config->gpgpu_warp_issue_shader;
    unsigned count = 0;
    unsigned warp_id_issued_sum = 0;
    gzprintf(visualizer_file, "WarpIssueSlotBreakdown:");
    if(m_shader_warp_slot_issue_distro[sid].size() > 0){
        for ( std::vector<unsigned>::const_iterator iter = m_shader_warp_slot_issue_distro[ sid ].begin();
              iter != m_shader_warp_slot_issue_distro[ sid ].end(); iter++, count++ ) {
            unsigned diff = count < m_last_shader_warp_slot_issue_distro.size() ?
                            *iter - m_last_shader_warp_slot_issue_distro[ count ] :
                            *iter;
            gzprintf( visualizer_file, " %d", diff );
            warp_id_issued_sum += diff;
        }
        m_last_shader_warp_slot_issue_distro = m_shader_warp_slot_issue_distro[ sid ];
    }else{
        gzprintf( visualizer_file, " 0");
    }
    gzprintf(visualizer_file,"\n");

    #define DYNAMIC_WARP_PRINT_RESOLUTION 32
    unsigned total_issued_this_resolution = 0;
    unsigned dynamic_id_issued_sum = 0;
    count = 0;
    gzprintf(visualizer_file, "WarpIssueDynamicIdBreakdown:");
    if(m_shader_dynamic_warp_issue_distro[sid].size() > 0){
        for ( std::vector<unsigned>::const_iterator iter = m_shader_dynamic_warp_issue_distro[ sid ].begin();
              iter != m_shader_dynamic_warp_issue_distro[ sid ].end(); iter++, count++ ) {
            unsigned diff = count < m_last_shader_dynamic_warp_issue_distro.size() ?
                            *iter - m_last_shader_dynamic_warp_issue_distro[ count ] :
                            *iter;
            total_issued_this_resolution += diff;
            if ( ( count + 1 ) % DYNAMIC_WARP_PRINT_RESOLUTION == 0 ) {
                gzprintf( visualizer_file, " %d", total_issued_this_resolution );
                dynamic_id_issued_sum += total_issued_this_resolution;
                total_issued_this_resolution = 0;
            }
        }
        if ( count % DYNAMIC_WARP_PRINT_RESOLUTION != 0 ) {
            gzprintf( visualizer_file, " %d", total_issued_this_resolution );
            dynamic_id_issued_sum += total_issued_this_resolution;
        }
        m_last_shader_dynamic_warp_issue_distro = m_shader_dynamic_warp_issue_distro[ sid ];
        assert( warp_id_issued_sum == dynamic_id_issued_sum );
    }else{
        gzprintf( visualizer_file, " 0");
    }
    gzprintf(visualizer_file,"\n");

    // overall cache miss rates
    gzprintf(visualizer_file, "gpgpu_n_cache_bkconflict: %d\n", gpgpu_n_cache_bkconflict);
    gzprintf(visualizer_file, "gpgpu_n_shmem_bkconflict: %d\n", gpgpu_n_shmem_bkconflict);     


   // instruction count per shader core
   gzprintf(visualizer_file, "shaderinsncount:  ");
   for (unsigned i=0;i<m_config->num_shader();i++) 
      gzprintf(visualizer_file, "%u ", m_num_sim_insn[i] );
   gzprintf(visualizer_file, "\n");
   // warp instruction count per shader core
   gzprintf(visualizer_file, "shaderwarpinsncount:  ");
   for (unsigned i=0;i<m_config->num_shader();i++)
      gzprintf(visualizer_file, "%u ", m_num_sim_winsn[i] );
   gzprintf(visualizer_file, "\n");
   // warp divergence per shader core
   gzprintf(visualizer_file, "shaderwarpdiv: ");
   for (unsigned i=0;i<m_config->num_shader();i++) 
      gzprintf(visualizer_file, "%u ", m_n_diverge[i] );
   gzprintf(visualizer_file, "\n");
}

#define PROGRAM_MEM_START 0xF0000000 /* should be distinct from other memory spaces... 
                                        check ptx_ir.h to verify this does not overlap 
                                        other memory spaces */
void shader_core_ctx::decode()
{
    if( m_inst_fetch_buffer.m_valid ) {
        // decode 1 or 2 instructions and place them into ibuffer
        address_type pc = m_inst_fetch_buffer.m_pc;
        const warp_inst_t* pI1 = ptx_fetch_inst(pc);
        m_warp[m_inst_fetch_buffer.m_warp_id].ibuffer_fill(0,pI1);
        m_warp[m_inst_fetch_buffer.m_warp_id].inc_inst_in_pipeline();
        if( pI1 ) {
            m_stats->m_num_decoded_insn[m_sid]++;
            if(pI1->oprnd_type==INT_OP){
                m_stats->m_num_INTdecoded_insn[m_sid]++;
            }else if(pI1->oprnd_type==FP_OP) {
            	m_stats->m_num_FPdecoded_insn[m_sid]++;
            }
           	const warp_inst_t* pI2 = ptx_fetch_inst(pc+pI1->isize);
           	if( pI2 ) {
               	m_warp[m_inst_fetch_buffer.m_warp_id].ibuffer_fill(1,pI2);
               	m_warp[m_inst_fetch_buffer.m_warp_id].inc_inst_in_pipeline();
               	m_stats->m_num_decoded_insn[m_sid]++;
               	if(pI2->oprnd_type==INT_OP){
                   	m_stats->m_num_INTdecoded_insn[m_sid]++;
               	}else if(pI2->oprnd_type==FP_OP) {
            	   	m_stats->m_num_FPdecoded_insn[m_sid]++;
               	}
				if (INSTR_BUFFER_SIZE > 2) {
           	  		const warp_inst_t* pI3 = ptx_fetch_inst(pc+pI1->isize+pI2->isize);
           			if( pI3 ) {
               			m_warp[m_inst_fetch_buffer.m_warp_id].ibuffer_fill(2,pI3);
               			m_warp[m_inst_fetch_buffer.m_warp_id].inc_inst_in_pipeline();
               			m_stats->m_num_decoded_insn[m_sid]++;
               			if(pI3->oprnd_type==INT_OP){
                   			m_stats->m_num_INTdecoded_insn[m_sid]++;
               			} else if(pI3->oprnd_type==FP_OP) {
            	   			m_stats->m_num_FPdecoded_insn[m_sid]++;
               			}
						if (INSTR_BUFFER_SIZE > 3) {
           	  				const warp_inst_t* pI4 = ptx_fetch_inst(pc+pI1->isize+pI2->isize+pI3->isize);
           					if( pI4 ) {
               					m_warp[m_inst_fetch_buffer.m_warp_id].ibuffer_fill(3,pI4);
               					m_warp[m_inst_fetch_buffer.m_warp_id].inc_inst_in_pipeline();
               					m_stats->m_num_decoded_insn[m_sid]++;
               					if(pI4->oprnd_type==INT_OP){
                   					m_stats->m_num_INTdecoded_insn[m_sid]++;
               					} else if(pI4->oprnd_type==FP_OP) {
            	   					m_stats->m_num_FPdecoded_insn[m_sid]++;
               					}
           					}
						}
           			}
				}
           	}
        }
        m_inst_fetch_buffer.m_valid = false;
    }
}

void shader_core_ctx::fetch()
{
	//bool all_warps_functional_done = true;
	//bool all_warps_imiss_pending = true;
	//bool all_warps_ibuffer_empty = true;

    if( !m_inst_fetch_buffer.m_valid ) {
        // find an active warp with space in instruction buffer that is not already waiting on a cache miss
        // and get next 1-2 instructions from i-cache...
        for( unsigned i=0; i < m_config->max_warps_per_shader; i++ ) {
            unsigned warp_id;
			if (((gLRRSched == false) && (gGTOSched == false)) && ((smallerIssuedWarpId0 != 0xdeaddead) && (smallerIssuedWarpId1 != 0xdeaddead)))
			{
				if (smallerIssuedWarpId0 < smallerIssuedWarpId1)
            		warp_id = (smallerIssuedWarpId0 + i) % m_config->max_warps_per_shader;
				else
            		warp_id = (smallerIssuedWarpId1 + i) % m_config->max_warps_per_shader;
			}
			else
            	warp_id = (m_last_warp_fetched+1+i) % m_config->max_warps_per_shader;

            // this code checks if this warp has finished executing and can be reclaimed
            if( m_warp[warp_id].hardware_done() && !m_scoreboard->pendingWrites(warp_id) && !m_warp[warp_id].done_exit() ) {
                bool did_exit=false;
                for( unsigned t=0; t<m_config->warp_size;t++) {
                    unsigned tid=warp_id*m_config->warp_size+t;
                    if( m_threadState[tid].m_active == true ) {
                        m_threadState[tid].m_active = false; 
                        unsigned cta_id = m_warp[warp_id].get_cta_id();
                        register_cta_thread_exit(cta_id);
                        m_not_completed -= 1;
                        m_active_threads.reset(tid);
                        assert( m_thread[tid]!= NULL );
                        did_exit=true;
                    }
                }
                if( did_exit ) 
				{
                    m_warp[warp_id].set_done_exit();
					unsigned int smId = this->get_sid();

					//if (smId == 0)
					//{
						//printf("%llu: finished warp %u, tb %u\n", gpu_sim_cycle, warp_id, m_warp[warp_id].get_cta_id());
					//}

					bool moreTBsLeft = m_cluster->get_gpu()->get_more_cta_left() ? true : false;
					if (gFBISched && (moreTBsLeft == false))
					{
						if (fbi_scheduler::numActiveWarps[smId] != 0xdeaddead)
							fbi_scheduler::numActiveWarps[smId]--;
					}

					assert(warp_id < MAX_NUM_WARP_PER_SM);
					unsigned int index = smId * MAX_NUM_WARP_PER_SM + warp_id;
					if (gWarpProgressArray)
						gWarpProgressArray[index] = 0;

					if (gWarpPhaseArray)
						gWarpPhaseArray[index] = 0;

					if (gSelectedWarp)
					{
    					for (unsigned i = 0; i < schedulers.size(); i++) 
						{
							if (gSelectedWarp[smId * NUM_SCHED_PER_SM + i] == warp_id)
							{
								gSelectedWarp[smId * NUM_SCHED_PER_SM + i] = 0xdeaddead;
								if (smId == 0)
									printf("%llu: high prio warp %u finished\n", gpu_sim_cycle, warp_id);
							}
						}
					}

					if (schedulers[0]->isMyGreedyScheduler())
					{
    					for (unsigned i = 0; i < schedulers.size(); i++) 
						{
        					if (schedulers[i]->getHighPrioWarpIdx() == warp_id)
							{
								schedulers[i]->setSelectNewHighPrioWarp();
								break;
							}
    					}
					}

					//if (rl_scheduler::gTBWithWarpsFinished && (rl_scheduler::gTBWithWarpsFinished[smId] == 0xdeaddead))
						//rl_scheduler::gTBWithWarpsFinished[smId] = m_warp[warp_id].get_cta_id();
				}
            }


            // this code fetches instructions from the i-cache or generates memory requests
            if( !m_warp[warp_id].functional_done() && !m_warp[warp_id].imiss_pending() && m_warp[warp_id].ibuffer_empty() ) {
                address_type pc  = m_warp[warp_id].get_pc();
                address_type ppc = pc + PROGRAM_MEM_START;
                unsigned nbytes= (8 * INSTR_BUFFER_SIZE);

                unsigned offset_in_block = pc & (m_config->m_L1I_config.get_line_sz()-1);
                if( (offset_in_block+nbytes) > m_config->m_L1I_config.get_line_sz() )
				{
                    nbytes = (m_config->m_L1I_config.get_line_sz()-offset_in_block);
				}

                // TODO: replace with use of allocator
                // mem_fetch *mf = m_mem_fetch_allocator->alloc()
                mem_access_t acc(INST_ACC_R,ppc,nbytes,false);
                mem_fetch *mf = new mem_fetch(acc,
                                              NULL/*we don't have an instruction yet*/,
                                              READ_PACKET_SIZE,
                                              warp_id,
                                              m_sid,
                                              m_tpc,
                                              m_memory_config );
                std::list<cache_event> events;
                enum cache_request_status status = m_L1I->access( (new_addr_type)ppc, mf, gpu_sim_cycle+gpu_tot_sim_cycle,events);
                if( status == MISS ) {
                    m_last_warp_fetched=warp_id;
                    m_warp[warp_id].set_imiss_pending();
                    m_warp[warp_id].set_last_fetch(gpu_sim_cycle);
                } else if( status == HIT ) {
                    m_last_warp_fetched=warp_id;
                    m_inst_fetch_buffer = ifetch_buffer_t(pc,nbytes,warp_id);
                    m_warp[warp_id].set_last_fetch(gpu_sim_cycle);
                    delete mf;
                } else {
                    m_last_warp_fetched=warp_id;
                    assert( status == RESERVATION_FAIL );
                    delete mf;
                }
				if (m_last_warp_fetched == smallerIssuedWarpId0)
					smallerIssuedWarpId0 = 0xdeaddead;
				else if (m_last_warp_fetched == smallerIssuedWarpId1)
					smallerIssuedWarpId1 = 0xdeaddead;

				//if ((this->m_sid == 0) && (schedulers[0]->isRLSched() == 0))
                    //printf("FETCH_INSTR: warp_dynamic_id %u cta_id %u at cycle %llu, (%s)\n", m_warp[warp_id].get_dynamic_warp_id(), m_warp[warp_id].get_cta_id(), gpu_sim_cycle, (status == MISS) ? "MISS" : (status == HIT) ? "HIT" : "RESERVATION_FAIL");

                break;
            }
        }
    }
	/*
	if ((m_sid == 0) && (schedulers[0]->isRLSched() == 0))
	{
		if (all_warps_functional_done == true)
			printf("%llu: all warps functional done\n", gpu_sim_cycle);
		if (all_warps_imiss_pending == true)
			printf("%llu: all warps imiss pending\n", gpu_sim_cycle);
		if (all_warps_ibuffer_empty == true)
			printf("%llu: all warps ibuffer empty\n", gpu_sim_cycle);
	}
	*/

    m_L1I->cycle();

    if( m_L1I->access_ready() ) {
        mem_fetch *mf = m_L1I->next_access();
        m_warp[mf->get_wid()].clear_imiss_pending();
        delete mf;
    }
}

void shader_core_ctx::func_exec_inst( warp_inst_t &inst )
{
    execute_warp_inst_t(inst);
    if( inst.is_load() || inst.is_store() )
        inst.generate_mem_accesses();
}

void shader_core_ctx::issue_warp( register_set& pipe_reg_set, const warp_inst_t* next_inst, const active_mask_t &active_mask, unsigned warp_id )
{
    warp_inst_t** pipe_reg = pipe_reg_set.get_free();
    assert(pipe_reg);
    
    m_warp[warp_id].ibuffer_free();
    assert(next_inst->valid());
    **pipe_reg = *next_inst; // static instruction information
    (*pipe_reg)->issue( active_mask, warp_id, gpu_tot_sim_cycle + gpu_sim_cycle, m_warp[warp_id].get_dynamic_warp_id() ); // dynamic instruction information
    m_stats->shader_cycle_distro[2+(*pipe_reg)->active_count()]++;
    func_exec_inst( **pipe_reg );
    if( next_inst->op == BARRIER_OP ) 
        m_barriers.warp_reaches_barrier(m_warp[warp_id].get_cta_id(),warp_id);
    else if( next_inst->op == MEMORY_BARRIER_OP ) 
        m_warp[warp_id].set_membar();

    updateSIMTStack(warp_id,*pipe_reg);
    m_scoreboard->reserveRegisters(*pipe_reg);
    m_warp[warp_id].set_next_pc(next_inst->pc + next_inst->isize);
}

bool gAlternateSchedOrder = true;
std::set<unsigned int> gTBsWithValidWarps;

void shader_core_ctx::mInitNumActiveWarps()
{
	uint smId = get_sid();
	unsigned int activeWarps = 0;
   	for (unsigned i = 0; i < schedulers.size(); i++) 
	{
		fbi_scheduler* fbiSched = (fbi_scheduler*)schedulers[i];
		activeWarps += fbiSched->mCountNumActiveWarps();
	}
	fbi_scheduler::numActiveWarps[smId] = activeWarps;
	printf("%llu: activeWarps on sm %u = %u\n", gpu_sim_cycle, smId, fbi_scheduler::numActiveWarps[smId]);
}

#define SCHED_PROG_DIFF_THRESHOLD 20

bool gPrintNoMoreCTAsMsg = true;
void shader_core_ctx::issue(){
    //really is issue;

	bool moreTBsLeft = m_cluster->get_gpu()->get_more_cta_left() ? true : false;
	if ((moreTBsLeft == false) && gPrintNoMoreCTAsMsg)
	{
		printf("%llu: All TBs assigned to SMs\n", gpu_sim_cycle);
		gPrintNoMoreCTAsMsg = false;
	}

	uint smId = get_sid();
	gTBsWithValidWarps.clear();

	//check if any warp is waiting at a barrier
	unsigned int tbWithWarpsAtBarrier = 0xdeaddead;
	bool prevTBWithWarpsAtBarrier = false;
	if (rl_scheduler::gTBWithWarpsAtBarrier)
	{
		tbWithWarpsAtBarrier = rl_scheduler::gTBWithWarpsAtBarrier[smId];
		prevTBWithWarpsAtBarrier = false;
		rl_scheduler::gTBWithWarpsAtBarrier[smId] = 0xdeaddead;

		for (unsigned int i = 0; i < schedulers.size(); i++)
		{
    		for (std::vector<shd_warp_t*>::const_iterator iter = schedulers[i]->m_supervised_warps.begin(); 
	     		iter != schedulers[i]->m_supervised_warps.end(); 
		 		iter++) 
			{
       			// Don't consider warps that are not yet valid
       			if (((*iter) == NULL) || ((*iter)->done_exit()))
					continue;
		
       			unsigned warp_id = (*iter)->get_warp_id();
		
				gTBsWithValidWarps.insert((*iter)->get_cta_id());
       			if (!schedulers[i]->warp(warp_id).ibuffer_empty())
				{
					if (this->warp_waiting_at_barrier(warp_id))
					{
						if (gSelectedWarp && (gSelectedWarp[smId * NUM_SCHED_PER_SM + schedulers[i]->m_id] == warp_id))
							gSelectedWarp[smId * NUM_SCHED_PER_SM + schedulers[i]->m_id] = 0xdeaddead;
		
						if (tbWithWarpsAtBarrier == (*iter)->get_cta_id())
							prevTBWithWarpsAtBarrier = true;
						else if (rl_scheduler::gTBWithWarpsAtBarrier && (rl_scheduler::gTBWithWarpsAtBarrier[smId] == 0xdeaddead))
							rl_scheduler::gTBWithWarpsAtBarrier[smId] = (*iter)->get_cta_id();
					}
       			}
   			}
		}

		if (rl_scheduler::gTBWithWarpsAtBarrier && (prevTBWithWarpsAtBarrier == true))
			rl_scheduler::gTBWithWarpsAtBarrier[smId] = tbWithWarpsAtBarrier;
		else if (rl_scheduler::gTBWithWarpsAtBarrier[smId] != 0xdeaddead)
		{
			//if (smId == 0)
				//printf("%llu: tb %u has warps at barrier\n", gpu_sim_cycle, rl_scheduler::gTBWithWarpsAtBarrier[smId]);
		}
	}

    if (gFBISched && (moreTBsLeft == false))
	{
		fbi_scheduler::numReadyNonLastMemInstrs[smId] = 0;
		fbi_scheduler::numWarpsReachedLastMemInstr[smId] = 0;
    	for (unsigned i = 0; i < schedulers.size(); i++) 
		{
			fbi_scheduler* fbiSched = (fbi_scheduler*)schedulers[i];
			fbiSched->mCountNumReadyNonLastMemInstrs();
		}

		if (fbi_scheduler::numActiveWarps[smId] == 0xdeaddead)
			mInitNumActiveWarps();
	}

	smallerIssuedWarpId0 = 0xdeaddead;
	smallerIssuedWarpId1 = 0xdeaddead;
	if ((gGTOSched == false) && (gLRRSched == false))
	{
		if (gAlternateSchedOrder)
		{
			if ((gpu_sim_cycle % 2) == 0)
			{
        		schedulers[0]->cycle();
        		schedulers[1]->cycle();
			}
			else
			{
        		schedulers[1]->cycle();
        		schedulers[0]->cycle();
			}
		}
		else
		{
			unsigned int idx0 = smId * 2 + 0;
			unsigned int idx1 = smId * 2 + 1;

			if (gSchedProgressArray[idx0] >= (gSchedProgressArray[idx1] + SCHED_PROG_DIFF_THRESHOLD))
				whichSchedFirst = 1;
			else if (gSchedProgressArray[idx1] >= (gSchedProgressArray[idx0] + SCHED_PROG_DIFF_THRESHOLD))
				whichSchedFirst = 0;

			if (whichSchedFirst == 1)
			{
        		schedulers[1]->cycle();
        		schedulers[0]->cycle();
			}
			else
			{
        		schedulers[0]->cycle();
        		schedulers[1]->cycle();
			}
		}
	}
	else
	{
    	for (unsigned i = 0; i < schedulers.size(); i++) {
        	schedulers[i]->cycle();
    	}
	}
}

shd_warp_t& scheduler_unit::warp(int i){
    return (*m_warp)[i];
}


/**
 * A general function to order things in a Loose Round Robin way. The simplist use of this
 * function would be to implement a loose RR scheduler between all the warps assigned to this core.
 * A more sophisticated usage would be to order a set of "fetch groups" in a RR fashion.
 * In the first case, the templated class variable would be a simple unsigned int representing the
 * warp_id.  In the 2lvl case, T could be a struct or a list representing a set of warp_ids.
 * @param result_list: The resultant list the caller wants returned.  This list is cleared and then populated
 *                     in a loose round robin way
 * @param input_list: The list of things that should be put into the result_list. For a simple scheduler
 *                    this can simply be the m_supervised_warps list.
 * @param last_issued_from_input:  An iterator pointing the last member in the input_list that issued.
 *                                 Since this function orders in a RR fashion, the object pointed
 *                                 to by this iterator will be last in the prioritization list
 * @param num_warps_to_add: The number of warps you want the scheudler to pick between this cycle.
 *                          Normally, this will be all the warps availible on the core, i.e.
 *                          m_supervised_warps.size(). However, a more sophisticated scheduler may wish to
 *                          limit this number. If the number if < m_supervised_warps.size(), then only
 *                          the warps with highest RR priority will be placed in the result_list.
 */
template < class T >
void scheduler_unit::order_lrr( std::vector< T >& result_list,
                                const typename std::vector< T >& input_list,
                                const typename std::vector< T >::const_iterator& last_issued_from_input,
                                unsigned num_warps_to_add )
{
    assert( num_warps_to_add <= input_list.size() );
    result_list.clear();
    typename std::vector< T >::const_iterator iter
        = ( last_issued_from_input ==  input_list.end() ) ? input_list.begin()
                                                          : last_issued_from_input + 1;

    for ( unsigned count = 0;
          count < num_warps_to_add;
          ++iter, ++count) {
        if ( iter ==  input_list.end() ) {
            iter = input_list.begin();
        }
        result_list.push_back( *iter );
    }
}


/**
 * A general function to order things in an priority-based way.
 * The core usage of the function is similar to order_lrr.
 * The explanation of the additional parameters (beyond order_lrr) explains the further extensions.
 * @param ordering: An enum that determines how the age function will be treated in prioritization
 *                  see the definition of OrderingType.
 * @param priority_function: This function is used to sort the input_list.  It is passed to stl::sort as
 *                           the sorting fucntion. So, if you wanted to sort a list of integer warp_ids
 *                           with the oldest warps having the most priority, then the priority_function
 *                           would compare the age of the two warps.
 */
template < class T >
void scheduler_unit::order_by_priority( std::vector< T >& result_list,
                                        const typename std::vector< T >& input_list,
                                        const typename std::vector< T >::const_iterator& last_issued_from_input,
                                        unsigned num_warps_to_add,
                                        OrderingType ordering,
                                        bool (*priority_func)(T lhs, T rhs) )
{
    assert( num_warps_to_add <= input_list.size() );
    result_list.clear();
    typename std::vector< T > temp = input_list;

    if ( ORDERING_GREEDY_THEN_PRIORITY_FUNC == ordering ) {
        T greedy_value = *last_issued_from_input;
        result_list.push_back( greedy_value );

        std::sort( temp.begin(), temp.end(), priority_func );
        typename std::vector< T >::iterator iter = temp.begin();
        for ( unsigned count = 0; count < num_warps_to_add; ++count, ++iter ) {
            if ( *iter != greedy_value ) {
                result_list.push_back( *iter );
            }
        }
    } else if ( ORDERED_PRIORITY_FUNC_ONLY == ordering ) {
        std::sort( temp.begin(), temp.end(), priority_func );
        typename std::vector< T >::iterator iter = temp.begin();
        for ( unsigned count = 0; count < num_warps_to_add; ++count, ++iter ) {
            result_list.push_back( *iter );
        }
	} else if (ORDERED_ROUND_ROBIN_PLUS_GREEDY_FUNC == ordering) {
        shd_warp_t* lastIssuedWarp = *last_issued_from_input;
		const unsigned int RR_MODE = 1;
		const unsigned int GTO_MODE = 2;
		unsigned int lastIssuedWarpMode = 0;
		if (last_issued_from_input != input_list.end())
		{
			unsigned int lastIssuedTBId = lastIssuedWarp->get_cta_id();
			if (gRRModeTBSet.find(lastIssuedTBId) != gRRModeTBSet.end())
			{
				setLastRRIssuedWarpIter(last_issued_from_input);
				lastIssuedWarpMode = RR_MODE;
			}
			else
			{
				assert (gGTOModeTBSet.find(lastIssuedTBId) != gGTOModeTBSet.end());
				setLastGTOIssuedWarpIter(last_issued_from_input);
				lastIssuedWarpMode = GTO_MODE;
			}
		}

		//count num of ready instrs
		unsigned int numReadyInstrs = 0;
    	for (std::vector<shd_warp_t*>::const_iterator iter = m_supervised_warps.begin(); 
	     	iter != m_supervised_warps.end(); 
		 	iter++) 
		{
			// Don't consider warps that are not yet valid
			if ((*iter) == NULL)
				continue;
			if ((*iter)->done_exit())
				continue;

			unsigned warp_id = (*iter)->get_warp_id();

			if (!warp(warp_id).waiting() && !warp(warp_id).ibuffer_empty())
			{
				const warp_inst_t* pI = warp(warp_id).ibuffer_next_inst();

				if(pI) 
				{
					unsigned pc, rpc;

					m_simt_stack[warp_id]->get_pdom_stack_top_info(&pc, &rpc);

					if( pc == pI->pc ) 
					{
						if (!m_scoreboard->checkCollision(warp_id, pI)) 
							numReadyInstrs++;
					} 
				}
			}
		}


    	typename std::vector<shd_warp_t*>::const_iterator iter = (getLastRRIssuedWarpIter() == input_list.end()) ? input_list.begin() : getLastRRIssuedWarpIter() + 1;
		std::vector<shd_warp_t*> gtoModeWarps;
		std::vector<shd_warp_t*> rrModeWarps;

    	for (unsigned count = 0; count < num_warps_to_add; ++iter, ++count) 
		{
        	if (iter == input_list.end()) 
            	iter = input_list.begin();

			shd_warp_t* warp = (*iter);
       		if ((warp == NULL) || warp->done_exit())
				continue;

			unsigned int tbId = warp->get_cta_id();
			if (gRRModeTBSet.find(tbId) != gRRModeTBSet.end())
        		rrModeWarps.push_back(warp);
			else
			{
				assert (gGTOModeTBSet.find(tbId) != gGTOModeTBSet.end());
				if (getLastGTOIssuedWarpIter() != iter)
					gtoModeWarps.push_back(warp);
			}
    	}

		std::sort(gtoModeWarps.begin(), gtoModeWarps.end(), priority_func);

		const unsigned int RR_GTO_PRIO = 1; //first RR then GTO warps
		const unsigned int GTO_RR_PRIO = 2; //first GTO then RR warps
		const unsigned int ALTERNATE_RR_GTO = 3; //first GTO then RR warps
		const unsigned int GTO_RR_BASED_ON_NUM_READY_INSTR = 4;
		unsigned int prio = GTO_RR_BASED_ON_NUM_READY_INSTR;
		if ((prio == RR_GTO_PRIO) || ((prio == ALTERNATE_RR_GTO) && (lastIssuedWarpMode = GTO_MODE)))
		{
			for (unsigned int i = 0; i < rrModeWarps.size(); i++)
			{
				shd_warp_t* warp = rrModeWarps[i];
           		result_list.push_back(warp);
			}

			if (getLastGTOIssuedWarpIter() != input_list.end())
       			result_list.push_back(*(getLastGTOIssuedWarpIter()));

			for (unsigned int i = 0; i < gtoModeWarps.size(); i++)
			{
				shd_warp_t* warp = gtoModeWarps[i];
           		result_list.push_back(warp);
			}
		}
		else if ((prio == GTO_RR_PRIO) || ((prio == ALTERNATE_RR_GTO) && (lastIssuedWarpMode = RR_MODE)))
		{
			if (getLastGTOIssuedWarpIter() != input_list.end())
       			result_list.push_back(*(getLastGTOIssuedWarpIter()));

			for (unsigned int i = 0; i < gtoModeWarps.size(); i++)
			{
				shd_warp_t* warp = gtoModeWarps[i];
           		result_list.push_back(warp);
			}

			for (unsigned int i = 0; i < rrModeWarps.size(); i++)
			{
				shd_warp_t* warp = rrModeWarps[i];
           		result_list.push_back(warp);
			}
		}
		else if (prio == GTO_RR_BASED_ON_NUM_READY_INSTR)
		{
			if (numReadyInstrs > 8)
			{
        		T greedy_value = *last_issued_from_input;
				if (last_issued_from_input != input_list.end())
        			result_list.push_back(greedy_value);

        		std::sort( temp.begin(), temp.end(), priority_func );
        		typename std::vector< T >::iterator iter = temp.begin();
        		for ( unsigned count = 0; count < num_warps_to_add; ++count, ++iter ) {
            		if ( *iter != greedy_value ) {
                		result_list.push_back( *iter );
            		}
        		}
			}
			else
			{
    			typename std::vector< T >::const_iterator iter = (last_issued_from_input == input_list.end()) ? input_list.begin() : last_issued_from_input + 1;

    			for (unsigned count = 0; count < num_warps_to_add; ++iter, ++count) 
				{
        			if (iter == input_list.end())
            			iter = input_list.begin();
        			result_list.push_back(*iter);
    			}
			}
		}
	} else {
        fprintf( stderr, "Unknown ordering - %d\n", ordering );
        abort();
    }
}

std::map<unsigned int, std::string> gLastMemInstrMap;
extern unsigned int* gNumReqsInMemSchedArray;
bool gPrintDRAMInfo = false;

bool gLastMemInstrIssuedByAnyWarp = false;
unsigned int gNumNonLastMemInstrIssuedAfterLastMemInstrIssuedByAnyWarp = 0;

void scheduler_unit::cycle()
{
	uint smId = m_shader->get_sid();
	uint schedId = this->m_id;
	char instrTypeStr[10];
	strcpy(instrTypeStr, "NOP");

	operation_pipeline_t pipeUsed = UNKNOWN_OP;
	warp_inst_t* instrSched = 0;
	unsigned int activeMaskCount = 0;

    SCHED_DPRINTF( "scheduler_unit::cycle()\n" );
    bool valid_inst = false;  // there was one warp with a valid instruction to issue (didn't require flush due to control hazard)
    bool ready_inst = false;  // of the valid instructions, there was one not waiting for pending register writes
    bool issued_inst = false; // of these we issued one
	shd_warp_t* issued_warp = 0;
	bool isGTCMemInstr = false;
    bool sp_op = false;
    bool sfu_op = false;
    bool mem_op = false;

	bool moreTBsLeft = m_shader->m_cluster->get_gpu()->get_more_cta_left() ? true : false;

    order_warps();

    for ( std::vector< shd_warp_t* >::const_iterator iter = m_next_cycle_prioritized_warps.begin();
          iter != m_next_cycle_prioritized_warps.end();
          iter++ ) {
        // Don't consider warps that are not yet valid
        if ( (*iter) == NULL || (*iter)->done_exit() ) {
            continue;
        }
        SCHED_DPRINTF( "Testing (warp_id %u, dynamic_warp_id %u)\n",
                       (*iter)->get_warp_id(), (*iter)->get_dynamic_warp_id() );
        unsigned warp_id = (*iter)->get_warp_id();
        unsigned checked=0;
        unsigned issued=0;
        unsigned max_issue = m_shader->m_config->gpgpu_max_insn_issue_per_warp;
        while( !warp(warp_id).waiting() && !warp(warp_id).ibuffer_empty() && (checked < max_issue) && (checked <= issued) && (issued < max_issue) ) {
            const warp_inst_t *pI = warp(warp_id).ibuffer_next_inst();
            bool valid = warp(warp_id).ibuffer_next_valid();
            bool warp_inst_issued = false;
            unsigned pc,rpc;
            m_simt_stack[warp_id]->get_pdom_stack_top_info(&pc,&rpc);
            SCHED_DPRINTF( "Warp (warp_id %u, dynamic_warp_id %u) has valid instruction (%s)\n",
                           (*iter)->get_warp_id(), (*iter)->get_dynamic_warp_id(),
                           ptx_get_insn_str( pc).c_str() );
            if( pI ) {
                assert(valid);
                if( pc != pI->pc ) {
                    SCHED_DPRINTF( "Warp (warp_id %u, dynamic_warp_id %u) control hazard instruction flush\n",
                                   (*iter)->get_warp_id(), (*iter)->get_dynamic_warp_id() );
                    // control hazard
                    warp(warp_id).set_next_pc(pc);
                    warp(warp_id).ibuffer_flush();
                } else {
                    valid_inst = true;
                    if ( !m_scoreboard->checkCollision(warp_id, pI) ) {
                        SCHED_DPRINTF( "Warp (warp_id %u, dynamic_warp_id %u) passes scoreboard\n",
                                       (*iter)->get_warp_id(), (*iter)->get_dynamic_warp_id() );
                        ready_inst = true;
                        const active_mask_t &active_mask = m_simt_stack[warp_id]->get_active_mask();
                        assert( warp(warp_id).inst_in_pipeline() );
                        if (((pI->op == LOAD_OP) || (pI->op == STORE_OP) || (pI->op == MEMORY_BARRIER_OP)) && 
						     (rl_scheduler::gSelectedActionVal != UNKNOWN_OP)  ) 
						{
							bool skipInstr = false;
							if (isFBIScheduler() && (pI->pc == fbi_scheduler::gLastMemInstrPC) && (moreTBsLeft == false))
							{
								/*
								if (m_shader->m_kernel->num_running() > (gNumSMs/2))
								{
									if (fbi_scheduler::numReadyNonLastMemInstrs[smId] > 0)
									{
										fbi_scheduler* fbiSched = (fbi_scheduler*)this;
										printf("%llu: smId = %u, schedId = %u, dyn_warp_id = %u, tbId = %u, skipping last mem instr %u\n", gpu_sim_cycle, smId, schedId, (*iter)->get_warp_id(), (*iter)->get_cta_id(), fbiSched->gLastMemInstrPC);
										skipInstr = true;
									}
									else
									{
										printf("%llu: smId = %u, schedId = %u, dyn_warp_id = %u, tbId = %u, NOT skipping last mem instr\n", gpu_sim_cycle, smId, schedId, (*iter)->get_warp_id(), (*iter)->get_cta_id());
									}
								}
								*/

								unsigned int numActWarps = fbi_scheduler::numActiveWarps[smId];
								unsigned int numActiveSMs = m_shader->m_kernel->num_running();
								unsigned int numActiveWarpsThreshold;

								if (numActiveSMs >= (gNumSMs * 3 /4))
									numActiveWarpsThreshold = numActWarps * 3 / 4;
								else if (numActiveSMs >= (gNumSMs / 2))
									numActiveWarpsThreshold = numActWarps / 2;
								else if (numActiveSMs >= (gNumSMs / 4))
									numActiveWarpsThreshold = numActWarps / 4;
								else
									numActiveWarpsThreshold = 0;

								numActiveWarpsThreshold = (numActWarps * 1) / 4;

								if ((fbi_scheduler::issueLastMemInstr[smId] == false) && 
								    (fbi_scheduler::numWarpsReachedLastMemInstr[smId]  < numActiveWarpsThreshold))
								{
									skipInstr = true;
								}
								else
								{
									if (fbi_scheduler::numReadyNonLastMemInstrs[smId] > 0)
										skipInstr = true;

									fbi_scheduler::issueLastMemInstr[smId] = true;
								}
							}

							mem_op = true;
                            if( (skipInstr == false) && m_mem_out->has_free() ) {
                                m_shader->issue_warp(*m_mem_out,pI,active_mask,warp_id);
                                issued++;
                                issued_inst=true;
								issued_warp = (*iter);
								instrSched = (warp_inst_t*)pI;

								instrSched->mResetHighPrioInst();
								if ((gFBISched) && (pI->space.get_type() == global_space))
								{
									bool highPrioInst = false;
									if ((pI->pc != fbi_scheduler::gLastMemInstrPC) || (moreTBsLeft == true))
										highPrioInst = ((fbi_scheduler*)this)->mIsHighPrioWarp(issued_warp);

									//if (highPrioInst && (pI->pc == fbi_scheduler::gLastMemInstrPC))
										//highPrioInst = false;

									if (highPrioInst)
										instrSched->mSetHighPrioInst();
								}

                                warp_inst_issued = true;
								strcpy(instrTypeStr, "MEM");
								pipeUsed = MEM__OP;
								activeMaskCount = active_mask.count();

								rl_scheduler::gNumWarpsExecutingMemInstrGPU++;


								if ((pI->space.get_type() == global_space) || 
								    (pI->space.get_type() == const_space) ||
									(pI->space.get_type() == tex_space))
								{
									isGTCMemInstr = true;
									rl_scheduler::gNumGTCMemInstrIssued++;
									if (gpu_sim_cycle > 50)
										rl_scheduler::gNumGTCMemInstrIssued1++;
								}
								if (rl_scheduler::gNumWarpsExecutingMemInstr)
									rl_scheduler::gNumWarpsExecutingMemInstr[smId]++;
								if (rl_scheduler::gLastMemInstrTB)
									rl_scheduler::gLastMemInstrTB[smId] = (*iter)->get_cta_id();
								if (rl_scheduler::gLastMemInstrPC)
									rl_scheduler::gLastMemInstrPC[smId] = pI->pc;
                            }
                        } else {
         					if ( (pI->op == SFU_OP) || (pI->op == ALU_SFU_OP) )
                            	sfu_op = true;
                            else
                                sp_op = true;

                            bool sp_pipe_avail = m_sp_out->has_free();
                            bool sfu_pipe_avail = m_sfu_out->has_free();
                            if( sp_pipe_avail && (pI->op != SFU_OP) && (rl_scheduler::gSelectedActionVal != UNKNOWN_OP)) {
                                // always prefer SP pipe for operations that can use both SP and SFU pipelines
                                m_shader->issue_warp(*m_sp_out,pI,active_mask,warp_id);
                                issued++;
                                issued_inst=true;
								strcpy(instrTypeStr, "SP");
								pipeUsed = SP__OP;
								instrSched = (warp_inst_t*)pI;
								activeMaskCount = active_mask.count();
								issued_warp = (*iter);
                                warp_inst_issued = true;
								rl_scheduler::gNumSpInstrIssued++;
								if (gpu_sim_cycle > 50)
									rl_scheduler::gNumSpInstrIssued1++;
                            } else if (( (pI->op == SFU_OP) || (pI->op == ALU_SFU_OP)) && (rl_scheduler::gSelectedActionVal != UNKNOWN_OP) ) {
                                if( sfu_pipe_avail ) {
                                    m_shader->issue_warp(*m_sfu_out,pI,active_mask,warp_id);
                                    issued++;
                                    issued_inst=true;
									strcpy(instrTypeStr, "SFU");
									pipeUsed = SFU__OP;
									instrSched = (warp_inst_t*)pI;
									activeMaskCount = active_mask.count();
									issued_warp = (*iter);
                                    warp_inst_issued = true;
									rl_scheduler::gNumSfuInstrIssued++;
									if (gpu_sim_cycle > 50)
										rl_scheduler::gNumSfuInstrIssued1++;
                                }
                            } 
                        }
                    } else {
                        SCHED_DPRINTF( "Warp (warp_id %u, dynamic_warp_id %u) fails scoreboard\n",
                                       (*iter)->get_warp_id(), (*iter)->get_dynamic_warp_id() );
                    }
                }
            } else if( valid ) {
               // this case can happen after a return instruction in diverged warp
               SCHED_DPRINTF( "Warp (warp_id %u, dynamic_warp_id %u) return from diverged warp flush\n",
                              (*iter)->get_warp_id(), (*iter)->get_dynamic_warp_id() );
               warp(warp_id).set_next_pc(pc);
               warp(warp_id).ibuffer_flush();
            }
            if(warp_inst_issued) {
                SCHED_DPRINTF( "Warp (warp_id %u, dynamic_warp_id %u) issued %u instructions\n",
                               (*iter)->get_warp_id(),
                               (*iter)->get_dynamic_warp_id(),
                               issued );
                do_on_warp_issued( warp_id, issued, iter );
            }
            checked++;
        }
        if ( issued ) {
            // This might be a bit inefficient, but we need to maintain
            // two ordered list for proper scheduler execution.
            // We could remove the need for this loop by associating a
            // supervised_is index with each entry in the m_next_cycle_prioritized_warps
            // vector. For now, just run through until you find the right warp_id
            for ( std::vector< shd_warp_t* >::const_iterator supervised_iter = m_supervised_warps.begin();
                  supervised_iter != m_supervised_warps.end();
                  ++supervised_iter ) {
                if ( *iter == *supervised_iter ) {
                    m_last_supervised_issued = supervised_iter;
                }
            }
            break;
        } 
    }

    // issue stall statistics:
    if( !valid_inst ) 
	{
        m_stats->shader_cycle_distro[0]++; // idle or control hazard
	}
    else if( !ready_inst ) 
	{
        m_stats->shader_cycle_distro[1]++; // waiting for RAW hazards (possibly due to memory) 
	}
    else if( !issued_inst ) 
	{
        m_stats->shader_cycle_distro[2]++; // pipeline stalled
		if (mem_op)
			gMemPipeLineStall++;
		if (sfu_op)
			gSfuPipeLineStall++;
		if (sp_op)
			gSpPipeLineStall++;
		if (mem_op && sfu_op)
			gMemSfuPipeLineStall++;
		if (mem_op && sp_op)
			gMemSpPipeLineStall++;
		if (sfu_op && sp_op)
			gSfuSpPipeLineStall++;
		if (mem_op && sfu_op && sp_op)
			gMemSfuSpPipeLineStall++;
	}

	if (issued_warp)
	{
		if ((gLRRSched == false) && (gGTOSched == false))
		{
			if (schedId == 0)
			{
				if (issued_warp->get_warp_id() < m_shader->smallerIssuedWarpId0)
					m_shader->smallerIssuedWarpId0 = issued_warp->get_warp_id();
			}
			else
			{
				if (issued_warp->get_warp_id() < m_shader->smallerIssuedWarpId1)
					m_shader->smallerIssuedWarpId1 = issued_warp->get_warp_id();
			}
		}
	}
	
	if (isMyGreedyScheduler())
	{
		if (issued_warp && (getHighPrioWarpIdx() == 0xdeaddead))
		{
			for (unsigned int i = 0; i < m_supervised_warps.size(); i++)
			{
				if (m_supervised_warps[i] == issued_warp)
				{
					setHighPrioWarpIdx(issued_warp->get_warp_id());
					setHighPrioWarpPosInVec(i);
				}
			}
		}

		if (issued_warp && (issued_warp == m_supervised_warps[getHighPrioWarpPosInVec()]) && isGTCMemInstr)
			setSelectNewHighPrioWarp();
	}

	if (gPrintDRAMInfo && (pipeUsed == MEM__OP) &&(smId == 0))
	{
		if (gNumReqsInMemSchedArray)
		{
			printf("NMIE=%u, cycle=%llu", rl_scheduler::gNumWarpsExecutingMemInstrGPU, gpu_sim_cycle);
			for (unsigned int i = 0; i < 6; i++)
				printf(",%u", gNumReqsInMemSchedArray[i]);
			printf("\n");
		}
	}

	if (issued_warp)
	{
		//this is for the first warp that executes on each sched
		if (gSelectedTB && (gSelectedTB[smId] == 0xdeaddead))
		{
			gSelectedTB[smId] = issued_warp->get_cta_id();
			if (smId == 0)
				printf("%llu: Selected %u as high prio tb (first)\n", gpu_sim_cycle, issued_warp->get_cta_id());
		}
		if (gSelectedWarp && (gSelectedWarp[smId * NUM_SCHED_PER_SM + schedId] == 0xdeaddead))
		{
			if (issued_warp->get_cta_id() == gSelectedTB[smId])
			{
				gSelectedWarp[smId * NUM_SCHED_PER_SM + schedId] = issued_warp->get_warp_id();
				if (smId == 0)
					printf("%llu: Selected %u as high prio warp (first)\n", gpu_sim_cycle, issued_warp->get_warp_id());
			}
		}
	}

	if (issued_warp)
	{
		unsigned int tbId = issued_warp->get_cta_id();
		unsigned index = smId * MAX_NUM_TB_PER_SM + tbId;

		if ((pipeUsed == MEM__OP) && (instrSched->space.get_type() == global_space))
		{
			if (gTBWarpsExecutingMemInstr)
			{
				gTBWarpsExecutingMemInstr[index]++;

				std::map<unsigned int, unsigned int>& dynWarpIdToTBIdMap = gDynWarpIdToTBIdMapVec[smId];
				dynWarpIdToTBIdMap[issued_warp->get_dynamic_warp_id()] = tbId;
			}
		}
		else
			instrSched->mSetCtaId(0xbeefbeef);

		if (gTBProgressArray)
		{
			assert(activeMaskCount != 0);
			gTBProgressArray[index] += activeMaskCount;
		}
		if (gSchedProgressArray)
		{
			unsigned int idx = smId * 2 + schedId;
			gSchedProgressArray[idx] += 1;
		}

		unsigned int newWarpPhase = gPhaseEndPCVec.size();
		for (unsigned int i = 0; i < gPhaseEndPCVec.size(); i++)
		{
			if (instrSched->pc <= gPhaseEndPCVec[i])
			{
				newWarpPhase = i;
				break;
			}
		}

		if (gTBPhaseArray)
		{
			unsigned int warpId = issued_warp->get_warp_id();
			unsigned int index = smId * MAX_NUM_WARP_PER_SM + warpId;
			unsigned int currWarpPhase = gWarpPhaseArray[index];

			unsigned int tbId = issued_warp->get_cta_id();
			index = smId * MAX_NUM_TB_PER_SM + tbId;
			unsigned int tbPhase = gTBPhaseArray[index];

			if (newWarpPhase > currWarpPhase)
			{
				tbPhase = (tbPhase + (newWarpPhase - currWarpPhase));
				gTBPhaseArray[index] = tbPhase;
			}
		}

		if (gWarpPhaseArray)
		{
			unsigned int warpId = issued_warp->get_warp_id();
			unsigned int index = smId * MAX_NUM_WARP_PER_SM + warpId;
			gWarpPhaseArray[index] = newWarpPhase;
		}

		if (gWarpProgressArray)
		{
			unsigned int warpId = issued_warp->get_warp_id();
			unsigned int index = smId * MAX_NUM_WARP_PER_SM + warpId;
			gWarpProgressArray[index] += activeMaskCount;
		}

		if ((pipeUsed == SP__OP) && rl_scheduler::gTBNumSpInstrsArray)
			rl_scheduler::gTBNumSpInstrsArray[index]++;
		else if ((pipeUsed == SFU__OP) && rl_scheduler::gTBNumSfuInstrsArray)
			rl_scheduler::gTBNumSfuInstrsArray[index]++;
		else if ((pipeUsed == MEM__OP) && rl_scheduler::gTBNumMemInstrsArray)
			rl_scheduler::gTBNumMemInstrsArray[index]++;


		std::string instrStr = ptx_get_insn_str(instrSched->pc);
		if ((pipeUsed == MEM__OP) && (instrSched->space.get_type() == global_space))
		{
			unsigned int index = (issued_warp->get_dynamic_warp_id() << 4) + smId;
			gLastMemInstrMap[index] = instrStr;
		}

		if ((gLastMemInstrIssuedByAnyWarp == true) && (pipeUsed == MEM__OP) && (instrSched->pc != fbi_scheduler::gLastMemInstrPC))
		{
			gNumNonLastMemInstrIssuedAfterLastMemInstrIssuedByAnyWarp++;
		}
		if ((gLastMemInstrIssuedByAnyWarp == false) && (moreTBsLeft == false) && (instrSched->pc == fbi_scheduler::gLastMemInstrPC))
		{
			gLastMemInstrIssuedByAnyWarp = true;
			printf("%llu: sm %u dyn_warp %u tb %u issued last mem instr %s\n", gpu_sim_cycle, smId, issued_warp->get_dynamic_warp_id(), issued_warp->get_cta_id(), instrStr.c_str());
		}
		unsigned int issuedWarpTBId = issued_warp->get_cta_id();
		if (strstr(instrStr.c_str(), "ret;"))
		{
			if (isFBIScheduler())
			{
				if (moreTBsLeft)
				{
					((fbi_scheduler*)this)->mInsertBlockInFinishList(issuedWarpTBId);
					((fbi_scheduler*)this)->mRemoveBlockFromInorderList(issuedWarpTBId);
				}
			}
			if (gNumWarpsAtFinishMapVec.size() != 0)
			{
				std::map<unsigned int, unsigned int>& numWarpsAtFinishMap = gNumWarpsAtFinishMapVec.at(smId);
				numWarpsAtFinishMap[issuedWarpTBId]++;
			}

			if (rl_scheduler::gTBWithWarpsFinished && (rl_scheduler::gTBWithWarpsFinished[smId] == 0xdeaddead))
				rl_scheduler::gTBWithWarpsFinished[smId] = issuedWarpTBId;
		}
		if (strstr(instrStr.c_str(), "bar.sync"))
		{
			if (gNumWarpsAtBarrierMapVec.size() != 0)
			{
				std::map<unsigned int, unsigned int>& numWarpsAtBarrierMap = gNumWarpsAtBarrierMapVec.at(smId);
				numWarpsAtBarrierMap[issuedWarpTBId]++;
			}

			if (isFBIScheduler())
				((fbi_scheduler*)this)->mInsertBlockInBarrierList(issuedWarpTBId);

			if (rl_scheduler::gTBWithWarpsAtBarrier && (rl_scheduler::gTBWithWarpsAtBarrier[smId] == 0xdeaddead))
				rl_scheduler::gTBWithWarpsAtBarrier[smId] = issuedWarpTBId;
		}
	}

	//For RL scheduler
	if (isRLSched())
	{

		if (m_shader->isactive())
		{
			computeNextQvalueAndUpdateOldQvalue(issued_warp, pipeUsed);
			collectRewardAndSetAttributes(issued_inst, issued_warp, pipeUsed, instrTypeStr, instrSched);
		}
		if ((gpu_sim_cycle >= 100) && ((gpu_sim_cycle % 50) == 0) && (smId == 0) && (m_id == 0))
		{
			rl_scheduler::gNumSpInstrIssued = rl_scheduler::gNumSpInstrIssued1;
			rl_scheduler::gNumSpInstrIssued1 = 0;

			rl_scheduler::gNumSfuInstrIssued = rl_scheduler::gNumSfuInstrIssued1;
			rl_scheduler::gNumSfuInstrIssued1 = 0;

			rl_scheduler::gNumGTCMemInstrIssued = rl_scheduler::gNumGTCMemInstrIssued1;
			rl_scheduler::gNumGTCMemInstrIssued1 = 0;
		}
	}
	
	//for rr_gto sched
	if (isRR_GTOSched())
	{
		if (issued_warp)
		{
			unsigned int tbId = issued_warp->get_cta_id();
			bool rrMode = false;
			if (gRRModeTBSet.find(tbId) != gRRModeTBSet.end())
			{
				rrMode = true;
				gRRModeCnt++;
			}
			else
			{
				assert(gGTOModeTBSet.find(tbId) != gGTOModeTBSet.end());
				gGTOModeCnt++;
			}
		}
	}
}

void scheduler_unit::do_on_warp_issued( unsigned warp_id,
                                        unsigned num_issued,
                                        const std::vector< shd_warp_t* >::const_iterator& prioritized_iter )
{
    m_stats->event_warp_issued( m_shader->get_sid(),
                                warp_id,
                                num_issued,
                                warp(warp_id).get_dynamic_warp_id() );
    warp(warp_id).ibuffer_step();
}

bool scheduler_unit::sort_warps_by_oldest_dynamic_id(shd_warp_t* lhs, shd_warp_t* rhs)
{
    if (rhs && lhs) {
        if ( lhs->done_exit() || lhs->waiting() ) {
            return false;
        } else if ( rhs->done_exit() || rhs->waiting() ) {
            return true;
        } else {
            return lhs->get_dynamic_warp_id() < rhs->get_dynamic_warp_id();
        }
    } else {
        return lhs < rhs;
    }
}

unsigned int getCmdPipeType(const warp_inst_t* pI)
{
	unsigned int actionVal;
	if (pI == 0)
		actionVal = UNKNOWN_OP;
	else if ((pI->op == LOAD_OP) || (pI->op == STORE_OP) || (pI->op == MEMORY_BARRIER_OP))
		actionVal = MEM__OP;
	else if ((pI->op == SFU_OP) || (pI->op == ALU_SFU_OP))
		actionVal = SFU__OP;
	else
		actionVal = SP__OP;
	return actionVal;
}

bool scheduler_unit::sort_warps_by_highest_q_value(shd_warp_t* lhs, shd_warp_t* rhs)
{
    if (rhs && lhs) 
	{
        if ((lhs->done_exit() || lhs->waiting()) && (rhs->done_exit() || rhs->waiting()))
			return lhs < rhs;

        if (lhs->done_exit() || lhs->waiting())
            return false;

        if (rhs->done_exit() || rhs->waiting()) 
            return true;

		unsigned int lhsActionVal = 0xdeaddead;
		unsigned int rhsActionVal = 0xdeaddead;

		const warp_inst_t* pI;
		if (lhs->ibuffer_empty() == false)
		{
			pI = lhs->ibuffer_next_inst();
           	if(pI) 
			{
           		unsigned pc, rpc;

				unsigned int warp_id = lhs->get_warp_id();
          		rl_scheduler::gSimtStack[warp_id]->get_pdom_stack_top_info(&pc, &rpc);

              	if( pc == pI->pc ) 
				{
                  	if (!rl_scheduler::gScoreboard->checkCollision(warp_id, pI)) 
					{
						if (rl_scheduler::gActionType == USE_TB_CMD_PIPE_AS_ACTION)
						{
							unsigned int tbId = lhs->get_cta_id();
							unsigned int pipeUsed = getCmdPipeType(pI);
							lhsActionVal = (tbId << 2) | pipeUsed;
						}
						else if (rl_scheduler::gActionType == USE_TB_ID_AS_ACTION)
							lhsActionVal = lhs->get_cta_id();
						else if (rl_scheduler::gActionType == USE_WARP_ID_AS_ACTION)
						{
							lhsActionVal = lhs->get_warp_id();
        					if ((lhsActionVal & 0x1) == 0)
            					lhsActionVal = (lhsActionVal >> 1);
        					else
            					lhsActionVal = ((lhsActionVal - 1) >> 1);
						}
						else if (rl_scheduler::gActionType == USE_TB_WARP_ID_AS_ACTION)
						{
							unsigned int tbId = lhs->get_cta_id();
							unsigned int warpId = lhs->get_warp_id();
        					if ((warpId & 0x1) == 0)
            					warpId = (warpId >> 1);
        					else
            					warpId = ((warpId - 1) >> 1);
							lhsActionVal = (tbId << 2) | warpId;
						}
						else
						{
							assert(rl_scheduler::gActionType == USE_CMD_PIPE_AS_ACTION);
							lhsActionVal = getCmdPipeType(pI);
						}
					}
				}
			}
		}

		if (rhs->ibuffer_empty() == false)
		{
			pI = rhs->ibuffer_next_inst();
           	if(pI) 
			{
           		unsigned pc, rpc;

				unsigned int warp_id = rhs->get_warp_id();
          		rl_scheduler::gSimtStack[warp_id]->get_pdom_stack_top_info(&pc, &rpc);

              	if( pc == pI->pc ) 
				{
                  	if (!rl_scheduler::gScoreboard->checkCollision(warp_id, pI)) 
					{
						if (rl_scheduler::gActionType == USE_TB_CMD_PIPE_AS_ACTION)
						{
							unsigned int tbId = rhs->get_cta_id();
							unsigned int pipeUsed = getCmdPipeType(pI);
							rhsActionVal = (tbId << 2) | pipeUsed;
						}
						else if (rl_scheduler::gActionType == USE_TB_ID_AS_ACTION)
							rhsActionVal = rhs->get_cta_id();
						else if (rl_scheduler::gActionType == USE_WARP_ID_AS_ACTION)
						{
							rhsActionVal = rhs->get_warp_id();
        					if ((rhsActionVal & 0x1) == 0)
            					rhsActionVal = (rhsActionVal >> 1);
        					else
            					rhsActionVal = ((rhsActionVal - 1) >> 1);
						}
						else if (rl_scheduler::gActionType == USE_TB_WARP_ID_AS_ACTION)
						{
							unsigned int tbId = rhs->get_cta_id();
							unsigned int warpId = rhs->get_warp_id();
        					if ((warpId & 0x1) == 0)
            					warpId = (warpId >> 1);
        					else
            					warpId = ((warpId - 1) >> 1);
							rhsActionVal = (tbId << 2) | warpId;
						}
						else
						{
							assert(rl_scheduler::gActionType == USE_CMD_PIPE_AS_ACTION);
							rhsActionVal = getCmdPipeType(pI);
						}
					}
				}
			}
		}
		if ((lhsActionVal == 0xdeaddead) && (rhsActionVal == 0xdeaddead))
			return lhs < rhs;

		if (lhsActionVal == 0xdeaddead)
			return false;

		if (rhsActionVal == 0xdeaddead)
			return true;

		unsigned int index = (rl_scheduler::gStateVal << rl_scheduler::gNumActionBits) + lhsActionVal;
		float lhsQ =  rl_scheduler::gQvalues[index];

		index = (rl_scheduler::gStateVal << rl_scheduler::gNumActionBits) + rhsActionVal;
		float rhsQ =  rl_scheduler::gQvalues[index];

		return (lhsQ > rhsQ);
    } 
	else 
	{
        return lhs < rhs;
    }
}

void tb_prio_scheduler::order_warps()
{
	uint smId = m_shader->get_sid();

	//if the selected tb was a tb with warps at barrier and now there is no tb at barrier then reset selected tb
	if (rl_scheduler::gTBWithWarpsAtBarrier && (rl_scheduler::gTBWithWarpsAtBarrier[smId] == 0xdeaddead))
	{
		if (slowWarpModeBarrier == true)
		{
			gSelectedTB[smId] = 0xdeaddead;
			//if (smId == 0)
				//printf("%llu: No more tbs at barrier\n", gpu_sim_cycle);
		}
	}

	//if the selected tb was a tb with warps finished and now there is no tb with warps finished then reset selected tb
	if (rl_scheduler::gTBWithWarpsFinished && (rl_scheduler::gTBWithWarpsFinished[smId] == 0xdeaddead))
	{
		if (slowWarpModeFinish == true)
		{
			gSelectedTB[smId] = 0xdeaddead;
			//if (smId == 0)
				//printf("%llu: No more tbs at barrier\n", gpu_sim_cycle);
		}
	}

	bool selectSlowestWarp = false;
	bool selectFastestWarp = false;
	bool selectSlowestTB = false;
	bool selectFastestTB = false;

	if (gSelectedWarp[smId * NUM_SCHED_PER_SM + m_id] == 0xdeaddead)
	{
		slowWarpModeFinish = false;
		slowWarpModeBarrier = false;
	}

	bool moreTBsLeft = m_shader->m_cluster->get_gpu()->get_more_cta_left() ? true : false;
	if (moreTBsLeft && rl_scheduler::gTBWithWarpsFinished && (rl_scheduler::gTBWithWarpsFinished[smId] != 0xdeaddead))
	{
		slowWarpModeBarrier = false;
		if (gSelectedTB[smId] != rl_scheduler::gTBWithWarpsFinished[smId])
		{
			gSelectedTB[smId] = rl_scheduler::gTBWithWarpsFinished[smId];
			if (smId == 0)
				printf("%llu: Selected %u as high prio tb (Finish)\n", gpu_sim_cycle, gSelectedTB[smId]);
			slowWarpModeFinish = false;
		}
		if (slowWarpModeFinish == false)
		{
			selectSlowestWarp = true;
			slowWarpModeFinish = true;
			//if (smId == 0)
				//printf("%llu: Selected tb %u as high prio tb (Finish)\n", gpu_sim_cycle, gSelectedTB[smId]);
		}
	}
	else 
	{
		slowWarpModeFinish = false;

		if (rl_scheduler::gTBWithWarpsAtBarrier && (rl_scheduler::gTBWithWarpsAtBarrier[smId] != 0xdeaddead))
		{
			if (gSelectedTB[smId] != rl_scheduler::gTBWithWarpsAtBarrier[smId])
			{
				gSelectedTB[smId] = rl_scheduler::gTBWithWarpsAtBarrier[smId];
				if (smId == 0)
					printf("%llu: Selected %u as high prio tb (Barrier)\n", gpu_sim_cycle, gSelectedTB[smId]);
				slowWarpModeBarrier = false;
			}
			if (slowWarpModeBarrier == false)
			{
				selectSlowestWarp = true;
				slowWarpModeBarrier = true;
				//if (smId == 0)
					//printf("%llu: Selected tb %u as high prio tb (Barrier)\n", gpu_sim_cycle, gSelectedTB[smId]);
			}
		}
		else
		{
			slowWarpModeBarrier = false;
			if (moreTBsLeft)
			{
				if (gSelectedTB[smId] == 0xdeaddead)
				{
					selectFastestTB = true;
					gSelectedWarp[smId * NUM_SCHED_PER_SM + 0] = 0xdeaddead;
					gSelectedWarp[smId * NUM_SCHED_PER_SM + 1] = 0xdeaddead;
				}
				if (gSelectedWarp[smId * NUM_SCHED_PER_SM + m_id] == 0xdeaddead)
					selectFastestWarp = true;
			}
			else
			{
				if (gSelectedTB[smId] == 0xdeaddead)
				{
					selectSlowestTB = true;
					gSelectedWarp[smId * NUM_SCHED_PER_SM + 0] = 0xdeaddead;
					gSelectedWarp[smId * NUM_SCHED_PER_SM + 1] = 0xdeaddead;
				}
				if (gSelectedWarp[smId * NUM_SCHED_PER_SM + m_id] == 0xdeaddead)
					selectSlowestWarp = true;
			}
		}
	}

	if (selectFastestTB == true)
	{
		unsigned int maxProgress = 0;
		unsigned int maxProgressTB = 0xdeaddead;
		
		for (unsigned int tbId = 0; tbId < MAX_NUM_TB_PER_SM; tbId++)
		{
			if (gTBsWithValidWarps.find(tbId) == gTBsWithValidWarps.end())
				continue;

			unsigned index = smId * MAX_NUM_TB_PER_SM + tbId;
			unsigned int tbProgress = gTBProgressArray[index];
	
			//if (smId == 0)
				//printf("%llu: tb %u progress %u\n", gpu_sim_cycle, tbId, tbProgress);
			if (tbProgress > maxProgress)
			{
				maxProgress = tbProgress;
				maxProgressTB = tbId;
			}
		}
		if (maxProgressTB != 0xdeaddead)
		{
			gSelectedTB[smId] = maxProgressTB;
			if (smId == 0)
				printf("%llu: Selected %u as high prio tb (fastest)\n", gpu_sim_cycle, maxProgressTB);
		}
	}
	else if (selectSlowestTB == true)
	{
		unsigned int minProgress = 0xFFFFFFFF;
		unsigned int minProgressTB = 0xdeaddead;
		
		for (unsigned int tbId = 0; tbId < MAX_NUM_TB_PER_SM; tbId++)
		{
			if (gTBsWithValidWarps.find(tbId) == gTBsWithValidWarps.end())
				continue;

			unsigned index = smId * MAX_NUM_TB_PER_SM + tbId;
			unsigned int tbProgress = gTBProgressArray[index];
	
			//if (smId == 0)
				//printf("%llu: tb %u progress %u\n", gpu_sim_cycle, tbId, tbProgress);
			if (tbProgress < minProgress)
			{
				minProgress = tbProgress;
				minProgressTB = tbId;
			}
		}
		if (minProgressTB != 0xdeaddead)
		{
			gSelectedTB[smId] = minProgressTB;
			if (smId == 0)
				printf("%llu: Selected %u as high prio tb (slowest)\n", gpu_sim_cycle, minProgressTB);
		}
	}

	if (selectSlowestWarp == true)
	{
		minWarpPtr = 0;

		unsigned int minWarpProg = 0xFFFFFFFF;

    	for (std::vector<shd_warp_t*>::const_iterator iter = m_supervised_warps.begin(); 
	    	 iter != m_supervised_warps.end(); 
		 	 iter++) 
		{
			shd_warp_t* warpPtr = (*iter);
       		if ((warpPtr == NULL) || (warpPtr->done_exit()))
				continue;
	
			if (m_shader->warp_waiting_at_barrier(warpPtr->get_warp_id()))
				continue;

			if (warpPtr->get_cta_id() == gSelectedTB[smId])
			{
				unsigned int warpId = warpPtr->get_warp_id();
				unsigned int warpProg = gWarpProgressArray[smId * MAX_NUM_WARP_PER_SM + warpId];
				//if (smId == 0)
					//printf("%llu: warp %u progress %u\n", gpu_sim_cycle, warpId, warpProg);
				if (warpProg < minWarpProg)
				{
					minWarpPtr = warpPtr;
					minWarpProg = gWarpProgressArray[smId * MAX_NUM_WARP_PER_SM + warpId];
				}
			}
		}

		if (minWarpPtr != 0)
		{
			gSelectedWarp[smId * NUM_SCHED_PER_SM + m_id] = minWarpPtr->get_warp_id();
			//if (smId == 0)
				//printf("%llu: Selected %u(%u) as high prio warp(tb)%s\n", gpu_sim_cycle, minWarpPtr->get_warp_id(), minWarpPtr->get_cta_id(), slowWarpModeFinish ? "(Finish)" : slowWarpModeBarrier ? "(Barrier)" : "");
		}
	}

	if (selectFastestWarp == true)
	{
		shd_warp_t* maxWarpPtr = 0;

		unsigned int maxWarpProg = 0;

    	for (std::vector<shd_warp_t*>::const_iterator iter = m_supervised_warps.begin(); 
	    	 iter != m_supervised_warps.end(); 
		 	 iter++) 
		{
			shd_warp_t* warpPtr = (*iter);
       		if ((warpPtr == NULL) || (warpPtr->done_exit()))
				continue;
	
			if (warpPtr->get_cta_id() == gSelectedTB[smId])
			{
				//unsigned int warpId = warpPtr->get_warp_id();
				unsigned int warpProg = gWarpProgressArray[smId * MAX_NUM_WARP_PER_SM + warpPtr->get_warp_id()];
				//if (smId == 0)
					//printf("%llu: warp %u progress %u\n", gpu_sim_cycle, warpId, warpProg);

				if (warpProg > maxWarpProg)
				{
					maxWarpPtr = warpPtr;
					maxWarpProg = gWarpProgressArray[smId * MAX_NUM_WARP_PER_SM + warpPtr->get_warp_id()];
				}
			}
		}

		if (maxWarpPtr != 0)
		{
			gSelectedWarp[smId * NUM_SCHED_PER_SM + m_id] = maxWarpPtr->get_warp_id();
			//if (smId == 0)
				//printf("%llu: Selected %u(%u) as high prio warp(tb) as fastest warp\n", gpu_sim_cycle, maxWarpPtr->get_warp_id(), maxWarpPtr->get_cta_id());
		}
	}

    m_next_cycle_prioritized_warps.clear();

	for (unsigned int i = 0; i < m_supervised_warps.size(); i++)
	{
		shd_warp_t* warpPtr = m_supervised_warps[i];
		if (warpPtr->get_warp_id() == gSelectedWarp[smId * NUM_SCHED_PER_SM + m_id])
		{
			m_next_cycle_prioritized_warps.push_back(warpPtr);
			break;
		}
	}

	//for (unsigned int pipeType = 3; pipeType > 0; pipeType = pipeType - 1)
	{
		for (unsigned int cnt = 0; cnt < m_supervised_warps.size(); cnt++)
		{
			unsigned int i;
			if (selectSlowestWarp)
				i = (m_supervised_warps.size() - 1) - cnt;
			else
				i = cnt;
			shd_warp_t* warpPtr = m_supervised_warps[i];
       		if ((warpPtr == NULL) || (warpPtr->done_exit()))
				continue;
	
			unsigned int warpId = warpPtr->get_warp_id();
			if (m_shader->warp_waiting_at_barrier(warpId))
				continue;

       		if (warp(warpId).ibuffer_empty())
				continue;

			if (warpPtr->get_warp_id() == gSelectedWarp[smId * NUM_SCHED_PER_SM + m_id])
				continue;

			/*
			if (warpPtr->get_cta_id() == gSelectedTB[smId])
			{
           		const warp_inst_t* pI = warp(warpId).ibuffer_next_inst();
				if ((pI->op == LOAD_OP) || (pI->op == STORE_OP) || (pI->op == MEMORY_BARRIER_OP))
				{
            		if (pipeType == 3) 
						m_next_cycle_prioritized_warps.push_back(warpPtr);
				}
				else if ((pI->op == SFU_OP) || (pI->op == ALU_SFU_OP))
				{
					if (pipeType == 2) 
						m_next_cycle_prioritized_warps.push_back(warpPtr);
				}
				else 
				{
					if (pipeType == 1)
						m_next_cycle_prioritized_warps.push_back(warpPtr);
				}
			}
			*/
			if (warpPtr->get_cta_id() == gSelectedTB[smId])
				m_next_cycle_prioritized_warps.push_back(warpPtr);
		}
	}

	//for (unsigned int pipeType = 3; pipeType > 0; pipeType = pipeType - 1)
	{
		for (unsigned int i = 0; i < m_supervised_warps.size(); i++)
		{
			shd_warp_t* warpPtr = m_supervised_warps[i];
       		if ((warpPtr == NULL) || (warpPtr->done_exit()))
				continue;
	
			unsigned int warpId = warpPtr->get_warp_id();
			if (m_shader->warp_waiting_at_barrier(warpId))
				continue;

       		if (warp(warpId).ibuffer_empty())
				continue;

			/*
			if (warpPtr->get_cta_id() != gSelectedTB[smId])
			{
           		const warp_inst_t* pI = warp(warpId).ibuffer_next_inst();
				if ((pI->op == LOAD_OP) || (pI->op == STORE_OP) || (pI->op == MEMORY_BARRIER_OP))
				{
            		if (pipeType == 3) 
						m_next_cycle_prioritized_warps.push_back(warpPtr);
				}
				else if ((pI->op == SFU_OP) || (pI->op == ALU_SFU_OP))
				{
					if (pipeType == 2) 
						m_next_cycle_prioritized_warps.push_back(warpPtr);
				}
				else 
				{
					if (pipeType == 1)
						m_next_cycle_prioritized_warps.push_back(warpPtr);
				}
			}
			*/
			if (warpPtr->get_cta_id() != gSelectedTB[smId])
				m_next_cycle_prioritized_warps.push_back(warpPtr);
		}
	}
}

void my_greedy_scheduler::order_warps()
{
    m_next_cycle_prioritized_warps.clear();
	if (selectNewHighPrioWarp)
	{
		//find a new high prio warp from the same TB as the curr high prio warp
		shd_warp_t* currHighPrioWarpPtr = m_supervised_warps[highPrioWarpPosInVec];
		for (unsigned int loopCnt = 1; loopCnt <= m_supervised_warps.size(); loopCnt++)
		{
			unsigned int i = (highPrioWarpPosInVec + loopCnt) % m_supervised_warps.size();

			shd_warp_t* warpPtr = m_supervised_warps[i];

       		// Don't consider warps that are not yet valid
       		if ((warpPtr == NULL) || (warpPtr->done_exit()))
				continue;

			if (warpPtr->get_cta_id() != currHighPrioWarpPtr->get_cta_id())
				continue;

       		unsigned warp_id = warpPtr->get_warp_id();

       		if (!warp(warp_id).waiting() && !warp(warp_id).ibuffer_empty())
			{
           		const warp_inst_t* pI = warp(warp_id).ibuffer_next_inst();
	
           		if(pI) 
				{
           			unsigned pc, rpc;
	
           			m_simt_stack[warp_id]->get_pdom_stack_top_info(&pc, &rpc);
	
               		if (pc == pI->pc) 
					{
                   		if (!m_scoreboard->checkCollision(warp_id, pI)) 
						{
							highPrioWarpIdx = warp_id;
							highPrioWarpPosInVec = i;
							//if ((m_shader->get_sid() == 0) && (m_id == 0))
								//printf("%llu:setting high prio warp idx to %u(%u)\n", gpu_sim_cycle, warp_id, i);
							selectNewHighPrioWarp = false;
							break;
						}
               		}
           		} 
       		}
   		}
	}

	if (selectNewHighPrioWarp)
	{
		//find a new high prio warp from a diff TB from the curr high prio warp
		shd_warp_t* currHighPrioWarpPtr = m_supervised_warps[highPrioWarpPosInVec];
		for (unsigned int loopCnt = 1; loopCnt <= m_supervised_warps.size(); loopCnt++)
		{
			unsigned int i = (highPrioWarpPosInVec + loopCnt) % m_supervised_warps.size();

			shd_warp_t* warpPtr = m_supervised_warps[i];

       		// Don't consider warps that are not yet valid
       		if ((warpPtr == NULL) || (warpPtr->done_exit()))
				continue;

			if (warpPtr->get_cta_id() == currHighPrioWarpPtr->get_cta_id())
				continue;

       		unsigned warp_id = warpPtr->get_warp_id();

       		if (!warp(warp_id).waiting() && !warp(warp_id).ibuffer_empty())
			{
           		const warp_inst_t* pI = warp(warp_id).ibuffer_next_inst();
	
           		if(pI) 
				{
           			unsigned pc, rpc;
	
           			m_simt_stack[warp_id]->get_pdom_stack_top_info(&pc, &rpc);
	
               		if (pc == pI->pc) 
					{
                   		if (!m_scoreboard->checkCollision(warp_id, pI)) 
						{
							highPrioWarpIdx = warp_id;
							highPrioWarpPosInVec = i;
							//if ((m_shader->get_sid() == 0) && (m_id == 0))
								//printf("%llu:setting high prio warp idx to %u(%u)\n", gpu_sim_cycle, warp_id, i);
							selectNewHighPrioWarp = false;
							break;
						}
               		}
           		} 
       		}
   		}
	}

	for (unsigned int loopCnt = 0; loopCnt < m_supervised_warps.size(); loopCnt++)
	{
		unsigned int i = (highPrioWarpPosInVec + loopCnt) % m_supervised_warps.size();

		shd_warp_t* warpPtr = m_supervised_warps[i];
		m_next_cycle_prioritized_warps.push_back(warpPtr);
	}
}

void random_scheduler::order_warps()
{
    m_next_cycle_prioritized_warps.clear();

    std::vector<shd_warp_t*> warpVec = m_supervised_warps;

	if (warpVec.size() > 0)
	{
		long int randVal = random();
		unsigned int idx = randVal % warpVec.size();
		shd_warp_t* warp = warpVec[idx];
		m_next_cycle_prioritized_warps.push_back(warp);
	
    	for (unsigned i = (idx + 1); i < warpVec.size(); ++i)
        	m_next_cycle_prioritized_warps.push_back(warpVec[i]);
	
    	for (unsigned i = 0; i < idx; ++i)
        	m_next_cycle_prioritized_warps.push_back(warpVec[i]);
	}
}

void rr_gto_scheduler::order_warps()
{
    order_by_priority( m_next_cycle_prioritized_warps,
                       m_supervised_warps,
                       m_last_supervised_issued,
                       m_supervised_warps.size(),
                       ORDERED_ROUND_ROBIN_PLUS_GREEDY_FUNC,
                       scheduler_unit::sort_warps_by_oldest_dynamic_id );
}

void lrr_scheduler::order_warps()
{
    order_lrr( m_next_cycle_prioritized_warps,
               m_supervised_warps,
               m_last_supervised_issued,
               m_supervised_warps.size() );
}

void gto_scheduler::order_warps()
{
    order_by_priority( m_next_cycle_prioritized_warps,
                       m_supervised_warps,
                       m_last_supervised_issued,
                       m_supervised_warps.size(),
                       ORDERING_GREEDY_THEN_PRIORITY_FUNC,
                       scheduler_unit::sort_warps_by_oldest_dynamic_id );
}

void
two_level_active_scheduler::do_on_warp_issued( unsigned warp_id,
                                               unsigned num_issued,
                                               const std::vector< shd_warp_t* >::const_iterator& prioritized_iter )
{
    scheduler_unit::do_on_warp_issued( warp_id, num_issued, prioritized_iter );
    if ( SCHEDULER_PRIORITIZATION_LRR == m_inner_level_prioritization ) {
        std::vector< shd_warp_t* > new_active; 
        order_lrr( new_active,
                   m_next_cycle_prioritized_warps,
                   prioritized_iter,
                   m_next_cycle_prioritized_warps.size() );
        m_next_cycle_prioritized_warps = new_active;
    } else {
        fprintf( stderr,
                 "Unimplemented m_inner_level_prioritization: %d\n",
                 m_inner_level_prioritization );
        abort();
    }
}

void two_level_active_scheduler::order_warps()
{
    //Move waiting warps to m_pending_warps
    unsigned num_demoted = 0;
    for (   std::vector< shd_warp_t* >::iterator iter = m_next_cycle_prioritized_warps.begin();
            iter != m_next_cycle_prioritized_warps.end(); ) {
        bool waiting = (*iter)->waiting();
        for (int i=0; i<4; i++){
            const warp_inst_t* inst = (*iter)->ibuffer_next_inst();
            //Is the instruction waiting on a long operation?
            if ( inst && inst->in[i] > 0 && this->m_scoreboard->islongop((*iter)->get_warp_id(), inst->in[i])){
                waiting = true;
            }
        }

        if( waiting ) {
            m_pending_warps.push_back(*iter);
            iter = m_next_cycle_prioritized_warps.erase(iter);
            SCHED_DPRINTF( "DEMOTED warp_id=%d, dynamic_warp_id=%d\n",
                           (*iter)->get_warp_id(),
                           (*iter)->get_dynamic_warp_id() );
            ++num_demoted;
        } else {
            ++iter;
        }
    }

    //If there is space in m_next_cycle_prioritized_warps, promote the next m_pending_warps
    unsigned num_promoted = 0;
    if ( SCHEDULER_PRIORITIZATION_SRR == m_outer_level_prioritization ) {
        while ( m_next_cycle_prioritized_warps.size() < m_max_active_warps ) {
            m_next_cycle_prioritized_warps.push_back(m_pending_warps.front());
            m_pending_warps.pop_front();
            SCHED_DPRINTF( "PROMOTED warp_id=%d, dynamic_warp_id=%d\n",
                           (m_next_cycle_prioritized_warps.back())->get_warp_id(),
                           (m_next_cycle_prioritized_warps.back())->get_dynamic_warp_id() );
            ++num_promoted;
        }
    } else {
        fprintf( stderr,
                 "Unimplemented m_outer_level_prioritization: %d\n",
                 m_outer_level_prioritization );
        abort();
    }
    assert( num_promoted == num_demoted );
}

swl_scheduler::swl_scheduler ( shader_core_stats* stats, shader_core_ctx* shader,
                               Scoreboard* scoreboard, simt_stack** simt,
                               std::vector<shd_warp_t>* warp,
                               register_set* sp_out,
                               register_set* sfu_out,
                               register_set* mem_out,
                               int id,
                               char* config_string )
    : scheduler_unit ( stats, shader, scoreboard, simt, warp, sp_out, sfu_out, mem_out, id )
{
    unsigned m_prioritization_readin;
    int ret = sscanf( config_string,
                      "warp_limiting:%d:%d",
                      &m_prioritization_readin,
                      &m_num_warps_to_limit
                     );
    assert( 2 == ret );
    m_prioritization = (scheduler_prioritization_type)m_prioritization_readin;
    // Currently only GTO is implemented
    assert( m_prioritization == SCHEDULER_PRIORITIZATION_GTO );
    assert( m_num_warps_to_limit <= shader->get_config()->max_warps_per_shader );
}

void swl_scheduler::order_warps()
{
    if ( SCHEDULER_PRIORITIZATION_GTO == m_prioritization ) {
        order_by_priority( m_next_cycle_prioritized_warps,
                           m_supervised_warps,
                           m_last_supervised_issued,
                           MIN( m_num_warps_to_limit, m_supervised_warps.size() ),
                           ORDERING_GREEDY_THEN_PRIORITY_FUNC,
                           scheduler_unit::sort_warps_by_oldest_dynamic_id );
    } else {
        fprintf(stderr, "swl_scheduler m_prioritization = %d\n", m_prioritization);
        abort();
    }
}

void rl_scheduler::computeReward(bool instIssued, operation_pipeline_t pipeUsed, warp_inst_t* instrSched, shd_warp_t* warp)
{
	//uint smId = m_shader->get_sid();
	//uint schedId = this->m_id;

	if (rl_scheduler::gCumulativeRewardPenalty)
	{
		if (currPhase == NO_INST_ISSUED)
		{
			if (instIssued)
			{
				currPhaseCycles = 0;
				currPhase = INST_ISSUED;
			}
			else
				currPhaseCycles++;
		}
		else
		{
			if (instIssued == false)
			{
				currPhaseCycles = 0;
				currPhase = NO_INST_ISSUED;
			}
			else
				currPhaseCycles++;
		}
		if (instIssued)
		{
			if (NON_UNIFORM_REWARDS)
			{
				if (pipeUsed == MEM__OP)
					reward = MEM_OP_REWARD + ((float)currPhaseCycles / 100);
				else if (pipeUsed == SFU__OP)
					reward = SFU_OP_REWARD + ((float)currPhaseCycles / 100);
				else //if (pipeUsed == SP__OP)
					reward = SP_OP_REWARD + ((float)currPhaseCycles / 100);
			}
			else
				reward = REWARD + ((float)currPhaseCycles / 100);
		}
		else
			reward = PENALTY - ((float)currPhaseCycles / 100);

		if (reward > gMaxReward)
			gMaxReward = reward;
		else if (reward < gMinReward)
			gMinReward = reward;
	}
	else
	{
		if (NON_UNIFORM_REWARDS && instIssued)
		{
			reward = REWARD;
			/*
			bool memLoadSched = false;
			bool sfuReward = false;
			bool memLoadReward = false;
			bool memStoreReward = false;
			bool shMemReward = false;
			*/
			for (size_t i = 0; i < attributeVector.size(); i++)
			{
				rl_attribute& attr = attributeVector[i];

				/*
				if ((attr.attrType == READY_SFU_INSTRS) && (attr.currValue == 1))
				{
					if (pipeUsed == SFU__OP)
						sfuReward = true;
				}
				else if ((attr.attrType == READY_GLOBAL_READ_MEM_INSTRS) && (attr.currValue == 1))
				{
					if (pipeUsed == MEM__OP)
					{
						if ((instrSched->op == LOAD_OP) || (instrSched->op == MEMORY_BARRIER_OP))
						{
							if (instrSched->space.get_type() == global_space)
							{
								memLoadSched = true;
								memLoadReward = true;
							}
							else
								shMemReward = true;
						}
						else
							memStoreReward = true;
					}
				}
				else if ((attr.attrType == READY_GLOBAL_CONST_TEXTURE_READ_MEM_INSTRS) && (attr.currValue == 1))
				{
					if (pipeUsed == MEM__OP)
					{
						if ((instrSched->op == LOAD_OP) || (instrSched->op == MEMORY_BARRIER_OP))
						{
							if ((instrSched->space.get_type() == global_space) || 
								(instrSched->space.get_type() == const_space) ||
								(instrSched->space.get_type() == tex_space))
							{
								memLoadSched = true;
								memLoadReward = true;
							}
							else
								shMemReward = true;
						}
						else
							memStoreReward = true;
					}
				}
				else if ((attr.attrType == NUM_OF_MEM_INSTRS_EXECUTING) && (attr.currValue == 3))
				{
					    if (memLoadSched == true))
							memLoadReward--;
						else if (pipeUse == SFU__OP)
				} 
				*/
				if (attr.attrType == WHICH_PIPELINE)
				{
					if (attr.currValue == (unsigned int) pipeUsed)
						reward += CORRECT_PIPE_REWARD;
					//if (smId == 0)
						//printf("%llu: scheduled PIPE %u (%u)\n", gpu_sim_cycle, pipeUsed, attr.currValue);
				}
				else if (attr.attrType == LAST_WARP_ISSUED)
				{
					if (warp && (warp->get_warp_id() == attr.currValue))
						reward += LAST_WARP_REWARD;
				}
				else if (attr.attrType == WHICH_THREAD_BLOCK)
				{
					if (warp && (warp->get_cta_id() == attr.currValue))
						reward += CORRECT_TB_REWARD;
					//if (smId == 0)
						//printf("%llu: scheduled TB %u (%u)\n", gpu_sim_cycle, warp->get_cta_id(), attr.currValue);
				}
			}
			/*
			reward += (sfuReward * SFU_OP_REWARD);
			reward += (memLoadReward * MEM_LOAD_OP_REWARD);
			reward += (shMemReward * SH_MEM_LOAD_OP_REWARD);
			reward += (memStoreReward * MEM_STORE_OP_REWARD);
			*/
		}
		else
			reward = instIssued ? REWARD : PENALTY;
	}

}

unsigned int rl_scheduler::computeAction(shd_warp_t* warp, operation_pipeline_t pipeUsed)
{
	unsigned int action;

	if (rl_scheduler::gActionType == USE_TB_CMD_PIPE_AS_ACTION)
	{
		if (warp)
		{
			unsigned int tb = warp->get_cta_id();
			action = (tb << 2) | pipeUsed;
		}
		else
			action = 0;
	}
	else if (rl_scheduler::gActionType == USE_TB_ID_AS_ACTION)
	{
		if (warp)
			action = warp->get_cta_id();
		else
			action = 0;
	}
	else if (rl_scheduler::gActionType == USE_WARP_ID_AS_ACTION)
	{
		action = DUMMY_WARP_ID;
		if (warp)
			action = warp->get_warp_id();

		//warpId is the action and for action 5 bits are being used as each scheduler has only 24 warps
		//so do the following magic to make the warpId between 0-23
		if ((action & 0x1) == 0)
			action = (action >> 1);
		else
			action = ((action - 1) >> 1);
	}
	else if (rl_scheduler::gActionType == USE_TB_WARP_ID_AS_ACTION)
	{
		unsigned int tb = 0;
		unsigned int warpId = DUMMY_WARP_ID;
		if (warp)
		{
			tb = warp->get_cta_id();
			warpId = warp->get_warp_id();
		}
		if ((warpId & 0x1) == 0)
			warpId = (warpId >> 1);
		else
			warpId = ((warpId - 1) >> 1);
		action = (tb << 2) | warpId;
	}
	else
	{
		assert(rl_scheduler::gActionType == USE_CMD_PIPE_AS_ACTION);
		action = (unsigned int)pipeUsed;
	}
	return action;
}

void rl_scheduler::collectRewardAndSetAttributes(bool instIssued, shd_warp_t* warp, 
                                               operation_pipeline_t pipeUsed, char* instrType,
											   warp_inst_t* instrSched)
{
	computeReward(instIssued, pipeUsed, instrSched, warp);

	if (instIssued) 
		numInstrIssued++;

	unsigned int action = computeAction(warp, pipeUsed);

	prevState = currState;
	prevAction = action;

	setAttributeValues1(warp);

	/*
	if ((rl_scheduler::gNumWarpsExecutingMemInstr) && instIssued && (m_shader->get_sid() == 0))
	{
		unsigned int numReadyInstrs = numReadyMemInstrs + numReadySfuInstrs + numReadySpInstrs;
		unsigned int NMIEcnt = rl_scheduler::gNumWarpsExecutingMemInstr[this->m_shader->get_sid()];
		printf("RL: cycle %llu, NMIEcnt = %u, numRdyMemInstrs = %u, numRdySpInstrs = %u, numRdySfuInstrs = %u, scheduled instr type = %s(%s)\n", gpu_sim_cycle, NMIEcnt, numReadyMemInstrs, numReadySpInstrs, numReadySfuInstrs, instrType, (numReadyInstrs == numReadyMemInstrs) ? "ALL_MEM_INSTRS" : "");
	}
	*/
}

uint rl_scheduler::numBits(unsigned int numValues)
{
	if (numValues == 0)
		return 0;

	numValues--;

	unsigned int nBits = 1;
	numValues = numValues >> 1;
	while (numValues)
	{
		nBits++;
		numValues = numValues >> 1;
	}
	return nBits;
}

void rl_scheduler::initCurrStateAndAction()
{
	currState = 0;
	for (size_t i = 0; i < attributeVector.size(); i++)
	{
		rl_attribute& attr = attributeVector[i];

		if (i > 0)
		{
			unsigned int nBits = numBits(attr.numAttrValues);
			currState = currState << nBits;
		}

		unsigned int defVal = attr.defaultValue;
		currState += defVal;
	}
	//printf("Init: sched id = %u, shader id = %u, currState = %u\n", m_id, m_shader->get_sid(), currState);
}

unsigned int rl_scheduler::getAttrStateVal_4(unsigned int attrVal, unsigned int bucketSize)
{
	unsigned int maxAttrVal = bucketSize * 4;
	unsigned int stateVal = 0;
	if (attrVal < (4 * maxAttrVal / 8))
		stateVal = 0;
	else if (attrVal < (6 * maxAttrVal / 8))
		stateVal = 1;
	else if (attrVal < (7 * maxAttrVal / 8))
		stateVal = 2;
	else
		stateVal = 3;
	return stateVal;
}

unsigned int rl_scheduler::getAttrStateVal_4_r(unsigned int attrVal, unsigned int bucketSize)
{
	unsigned int maxAttrVal = bucketSize * 4;
	unsigned int stateVal = 0;
	if (attrVal > (4 * maxAttrVal / 8))
		stateVal = 3;
	else if (attrVal > (2 * maxAttrVal / 8))
		stateVal = 2;
	else if (attrVal > (1 * maxAttrVal / 8))
		stateVal = 1;
	else
		stateVal = 0;
	return stateVal;
}

std::set<unsigned int> gReadyTBIdSet;
#define HIGH_NUM_WARPS_EXECUTING_MEM_INSTR 450
//#define HIGH_NUM_WARPS_EXECUTING_MEM_INSTR 540

void rl_scheduler::setAttributeValues2()
{
	unsigned int smId = m_shader->get_sid();
	//unsigned int schedId = this->m_id;
	assert(smId < gNumSMs);
	for (size_t i = 0; i < attributeVector.size(); i++)
	{
		rl_attribute& attr = attributeVector[i];
		if (attr.attrType == LAST_WARP_ISSUED)
		{
			//this will be set in setAttributeValues1
		}
		else if (attr.attrType == WHICH_PIPELINE)
		{
			attr.currValue = UNKNOWN_OP;

			/*
			if ((rl_scheduler::gNumWarpsExecutingMemInstrGPU > HIGH_NUM_WARPS_EXECUTING_MEM_INSTR) 
				&& (gGTCLongLatMemInstrReady == false))
			{
				if (numReadySfuInstrs)
					attr.currValue = SFU__OP;
				else if (numReadySpInstrs)
					attr.currValue = SP__OP;
				else if (numReadyMemInstrs)
					attr.currValue = MEM__OP;
			}
			else if (numReadyMemInstrs)
				attr.currValue = MEM__OP;
			else if (numReadySfuInstrs)
				attr.currValue = SFU__OP;
			else if (numReadySpInstrs)
				attr.currValue = SP__OP;
			*/


			if (gGTCLongLatMemInstrReady == true)
			{
				if (rl_scheduler::gNumWarpsExecutingMemInstrGPU > HIGH_NUM_WARPS_EXECUTING_MEM_INSTR)
				{
					if (numReadySfuInstrs)
						attr.currValue = SFU__OP;
					else 
					{
						assert (numReadyMemInstrs);
						attr.currValue = MEM__OP;
					}
				}
				else
				{
					assert (numReadyMemInstrs);
					attr.currValue = MEM__OP;
				}
			}
			else if (gSFULongLatInstrReady == true)
			{
				assert (numReadySfuInstrs);
				attr.currValue = SFU__OP;
			}
			else
			{
				if (rl_scheduler::gNumWarpsExecutingMemInstrGPU > HIGH_NUM_WARPS_EXECUTING_MEM_INSTR)
				{
					if (numReadySfuInstrs)
						attr.currValue = SFU__OP;
					else if (numReadyMemInstrs)
						attr.currValue = MEM__OP;
					else if (numReadySpInstrs)
						attr.currValue = SP__OP;
				}
				else
				{
					if (numReadyMemInstrs)
						attr.currValue = MEM__OP;
					else if (numReadySfuInstrs)
						attr.currValue = SFU__OP;
					else if (numReadySpInstrs)
						attr.currValue = SP__OP;
				}
			}

		}
		else if (attr.attrType == WHICH_THREAD_BLOCK)
		{
			setWhichThreadBlock(attr);
		}
		else if (attr.attrType == WHICH_WARP)
		{
		}
		else if (attr.attrType == NUM_OF_MEM_QS_LOADED)
		{
			attr.currValue = rl_scheduler::gNumMemSchedQsLoaded / attr.bucketSize;
		}
		else if (attr.attrType == NUM_OF_MEM_REQS_IN_SCHED_Q)
		{
			attr.currValue = rl_scheduler::gNumReqsInMemSchedQs / attr.bucketSize;
		}
		else if (attr.attrType == NUM_OF_SP_SFU_INSTR_PER_MEM_INSTR_ISSUED)
		{
			//if ((smId == 0) && (m_id == 0))
				//printf("%llu: sp %u, sfu %u glMem %u\n", gpu_sim_cycle, rl_scheduler::gNumSpInstrIssued, rl_scheduler::gNumSfuInstrIssued, rl_scheduler::gNumGTCMemInstrIssued);

			if (rl_scheduler::gNumGTCMemInstrIssued > 0)
			{
				unsigned int v1 = (rl_scheduler::gNumSpInstrIssued + rl_scheduler::gNumSfuInstrIssued) / rl_scheduler::gNumGTCMemInstrIssued;
				//if ((smId == 0) && (m_id == 0))
					//printf("%llu: num sp+sfu instr per mem instr = %u\n", gpu_sim_cycle, v1);
				attr.currValue = v1 / attr.bucketSize;
			}
			else
				attr.currValue = 5; //avg num of sp+sfu instr per gtc mem instr
		}
		else if (attr.attrType == AVG_GL_MEM_LAT)
		{
			if (rl_scheduler::gNumGTCMemInstrFinished > 0)
			{
				unsigned int avgLat = rl_scheduler::gNumGTCMemLatencyCycles / rl_scheduler::gNumGTCMemInstrFinished;
				attr.currValue = avgLat / attr.bucketSize;
			}
			else
				attr.currValue = 300; //avg mem access latency cycles
		}
		else if (attr.attrType == NUM_OF_MEM_INSTRS_EXECUTING)
		{
			//attr.currValue = rl_scheduler::gNumWarpsExecutingMemInstr[smId] / attr.bucketSize;
			attr.currValue = rl_scheduler::gNumWarpsExecutingMemInstrGPU / attr.bucketSize;
		}
		else if (attr.attrType == NUM_OF_READY_MEM_INSTRS)
		{
			attr.currValue = numReadyMemInstrs / attr.bucketSize;
		}
		else if (attr.attrType == NUM_OF_READY_SFU_INSTRS)
		{
			attr.currValue = numReadySfuInstrs / attr.bucketSize;
		}
		else if (attr.attrType == NUM_OF_READY_SP_INSTRS)
		{
			attr.currValue = numReadySpInstrs / attr.bucketSize;
		}
		else if (attr.attrType == NUM_OF_READY_INSTRS)
		{
			attr.currValue = numReadyInstrs / attr.bucketSize;
		}
		else if (attr.attrType == NUM_OF_READY_MEM_INSTRS_WITH_SAME_TB_AS_LAST_MEM_INSTR)
		{
			attr.currValue = rl_scheduler::gNumReadyMemInstrsWithSameTB[smId] / attr.bucketSize;
		}
		else if (attr.attrType == NUM_OF_READY_MEM_INSTRS_WITH_SAME_PC_AS_LAST_MEM_INSTR)
		{
			attr.currValue = rl_scheduler::gNumReadyMemInstrsWithSamePC[smId] / attr.bucketSize;
		}
		else if (attr.attrType == NUM_OF_TBS_WAITING)
		{
			attr.currValue = m_shader->m_cluster->get_gpu()->get_more_cta_left() / attr.bucketSize;
		}
		else if (attr.attrType == NUM_OF_WARPS_ISSUED_BARRIER)
		{
			if (attr.bucketSize == 0)
			{
				if (gNumWarpsPerBlock < 4)
					attr.bucketSize = gNumWarpsPerBlock;
				else
					attr.bucketSize = gNumWarpsPerBlock / 4;
			}
			if (attr.bucketSize == 0)
				attr.currValue = attr.defaultValue;
			else
				attr.currValue = rl_scheduler::gNumWarpsWaitingAtBarrier[smId] / attr.bucketSize;
		}
		else if (attr.attrType == NUM_OF_WARPS_FINISHED)
		{
			if (attr.bucketSize == 0)
			{
				if (gNumWarpsPerBlock < 4)
					attr.bucketSize = gNumWarpsPerBlock;
				else
					attr.bucketSize = gNumWarpsPerBlock / 4;
			}
			if (attr.bucketSize == 0)
				attr.currValue = attr.defaultValue;
			else
				attr.currValue = rl_scheduler::gNumWarpsFinished[smId] / attr.bucketSize;
		}
		else if (attr.attrType == NUM_OF_READY_READ_MEM_INSTRS)
		{
			attr.currValue = numReadyReadMemInstrs / attr.bucketSize;
		}
		else if (attr.attrType == NUM_OF_READY_WRITE_MEM_INSTRS)
		{
			attr.currValue = numReadyWriteMemInstrs / attr.bucketSize;
		}
		else if (attr.attrType == NUM_OF_READY_GLOBAL_MEM_INSTRS)
		{
			attr.currValue = numReadyGlobalMemInstrs / attr.bucketSize;
		}
		else if (attr.attrType == NUM_OF_READY_SHARED_MEM_INSTRS)
		{
			attr.currValue = numReadySharedMemInstrs / attr.bucketSize;
		}
		else if (attr.attrType == NUM_OF_READY_CONSTANT_MEM_INSTRS)
		{
			attr.currValue = numReadyConstantMemInstrs / attr.bucketSize;
		}
		else if (attr.attrType == NUM_OF_READY_TEXTURE_MEM_INSTRS)
		{
			attr.currValue = numReadyTextureMemInstrs / attr.bucketSize;
		}
		else if (attr.attrType == L1_MISS_PERCENT)
		{
    		struct cache_sub_stats css;

			m_shader->get_L1D_sub_stats(css);

			if (css.accesses != 0)
			{
				unsigned int missPercent = (css.misses * 100) / css.accesses;

				attr.currValue = missPercent / attr.bucketSize;
			}
		}
		else if (attr.attrType == L2_MISS_PERCENT)
		{
			unsigned int numAccesses = 0;
			unsigned int numMisses = 0;

			m_shader->m_cluster->get_gpu()->getL2Stats(numAccesses, numMisses);
			if (numAccesses != 0)
			{
				unsigned int missPercent = (numMisses * 100) / numAccesses;

				attr.currValue = missPercent / attr.bucketSize;
			}
		}
		else if (attr.attrType == READY_SFU_INSTRS)
		{
			attr.currValue = readySfuInstrs;
		}
		else if (attr.attrType == READY_GLOBAL_CONST_TEXTURE_READ_MEM_INSTRS)
		{
			attr.currValue = readyGlobalConstTexReadMemInstr;
		}
		else if (attr.attrType == READY_GLOBAL_READ_MEM_INSTRS)
		{
			attr.currValue = readyGlobalReadMemInstr;
		}
		else 
			assert(0);

		if (attr.currValue >= attr.numAttrValues)
			attr.currValue = attr.numAttrValues - 1;
	}
}

bool rl_scheduler::setWhichThreadBlock(rl_attribute& attr)
{
	unsigned int smId = m_shader->get_sid();
	bool moreTBsLeft = m_shader->m_cluster->get_gpu()->get_more_cta_left() ? true : false;
	bool attrValSet = false;

	/*
	if (rl_scheduler::gTBWithWarpsFinished && (rl_scheduler::gTBWithWarpsFinished[smId] != 0xdeaddead) && moreTBsLeft)
	{
		tbId = rl_scheduler::gTBWithWarpsFinished[smId];
		if (gReadyTBIdSet.find(tbId) != gReadyTBIdSet.end())
		{
			attr.currValue = tbId;
			attrValSet = true;
		}
	}
	*/
	if (moreTBsLeft)
	{
		unsigned int maxWarpsAtFinish = 0;
		std::map<unsigned int, unsigned int>& numWarpsAtFinishMap = gNumWarpsAtFinishMapVec.at(smId);
		for (unsigned int tbId = 0; tbId < MAX_NUM_TB_PER_SM; tbId++)
		{
			unsigned int numWarpsAtFinish = numWarpsAtFinishMap[tbId];
			if (numWarpsAtFinish > maxWarpsAtFinish)
			{
				if (gReadyTBIdSet.find(tbId) != gReadyTBIdSet.end())
				{
					attr.currValue = tbId;
					attrValSet = true;
					maxWarpsAtFinish = numWarpsAtFinish;
				}
			}
		}
	}
	/*
	if ((attrValSet == false) && rl_scheduler::gTBWithWarpsAtBarrier && (rl_scheduler::gTBWithWarpsAtBarrier[smId] != 0xdeaddead))
	{
		tbId = rl_scheduler::gTBWithWarpsAtBarrier[smId];
		if (gReadyTBIdSet.find(tbId) != gReadyTBIdSet.end())
		{
			attr.currValue = tbId;
			attrValSet = true;
		}
	}
	*/
	unsigned int maxWarpsAtBarrier = 0;
	std::map<unsigned int, unsigned int>& numWarpsAtBarrierMap = gNumWarpsAtBarrierMapVec.at(smId);
	for (unsigned int tbId = 0; tbId < MAX_NUM_TB_PER_SM; tbId++)
	{
		unsigned int numWarpsAtBarrier = numWarpsAtBarrierMap[tbId];
		if (numWarpsAtBarrier > maxWarpsAtBarrier)
		{
			if (gReadyTBIdSet.find(tbId) != gReadyTBIdSet.end())
			{
				attr.currValue = tbId;
				attrValSet = true;
				maxWarpsAtBarrier = numWarpsAtBarrier;
			}
		}
	}

	if ((attrValSet == false) && gTBProgressArray)
	{
		if (moreTBsLeft)
		{
			unsigned int maxProgress = 0;
			unsigned int maxProgressTB = 0xdeaddead;

			for (unsigned int tbId = 0; tbId < MAX_NUM_TB_PER_SM; tbId++)
			{
				unsigned index = smId * MAX_NUM_TB_PER_SM + tbId;
				unsigned int tbProgress = gTBProgressArray[index];
	
				if ((tbProgress > maxProgress) && (gReadyTBIdSet.find(tbId) != gReadyTBIdSet.end()))
				{
					maxProgress = tbProgress;
					maxProgressTB = tbId;
				}
			}
			if (maxProgressTB != 0xdeaddead)
			{
				attr.currValue = maxProgressTB / attr.bucketSize;
				attrValSet = true;
			}
		}
		else
		{
			unsigned int minProgress = 0xFFFFFFFF;
			unsigned int minProgressTB = 0xdeaddead;

			for (unsigned int tbId = 0; tbId < MAX_NUM_TB_PER_SM; tbId++)
			{
				unsigned index = smId * MAX_NUM_TB_PER_SM + tbId;
				unsigned int tbProgress = gTBProgressArray[index];

				if ((tbProgress < (minProgress - 32)) && (gReadyTBIdSet.find(tbId) != gReadyTBIdSet.end()))
				{
					minProgress = tbProgress;
					minProgressTB = tbId;
				}
			}
			if (minProgressTB != 0xdeaddead)
			{
				attr.currValue = minProgressTB / attr.bucketSize;
				attrValSet = true;
			}
		}
	}
	return attrValSet;
}

void rl_scheduler::setAttributeValues1(shd_warp_t* warp)
{
	unsigned int smId = m_shader->get_sid();
	assert(smId < gNumSMs);
	for (size_t i = 0; i < attributeVector.size(); i++)
	{
		rl_attribute& attr = attributeVector[i];
		if (attr.attrType == LAST_WARP_ISSUED)
		{
			if (warp)
			{
				uint warpId = warp->get_warp_id();
				if ((warpId & 0x1) == 0)
					attr.currValue = (warpId >> 1) / attr.bucketSize;
				else
					attr.currValue = ((warpId - 1) >> 1) / attr.bucketSize;
			}
		}
		else if (attr.attrType == WHICH_THREAD_BLOCK)
		{
			bool attrValSet = setWhichThreadBlock(attr);
			if ((attrValSet == false) && warp)
			{
				attr.currValue = warp->get_cta_id();
				attrValSet = true;
			}
		}
		else if (attr.attrType == WHICH_WARP)
		{
		}
	}
}

void rl_scheduler::computeCurrState()
{
	currState = 0;
	for (size_t i = 0; i < attributeVector.size(); i++)
	{
		rl_attribute& attr = attributeVector[i];
		if (i > 0)
		{
			unsigned int nBits = numBits(attr.numAttrValues);
			currState = currState << nBits;
		}

		unsigned int curVal = attr.currValue;
		currState += curVal;
	}
	//printf("computeCurrState: sched id = %u, shader id = %u, currState = %u\n", m_id, m_shader->get_sid(), currState);
}

unsigned int rl_scheduler::getQvalueMatrixSize()
{
	numAttrBits = 0;

	for (size_t i = 0; i < attributeVector.size(); i++)
		numAttrBits += numBits(attributeVector[i].numAttrValues);

	assert(numAttrBits < 32);

	numActionBits = 0;
	if (rl_scheduler::gActionType == USE_TB_CMD_PIPE_AS_ACTION)
		numActionBits = 3 + 2; //3->TB id, 2->pipe used
	else if (rl_scheduler::gActionType == USE_TB_ID_AS_ACTION)
		numActionBits = 3; //3->TB id
	else if (rl_scheduler::gActionType == USE_WARP_ID_AS_ACTION)
		numActionBits = numBits(NUM_WARPS_PER_SCHEDULER);
	else if (rl_scheduler::gActionType == USE_TB_WARP_ID_AS_ACTION)
		numActionBits = 3 + numBits(NUM_WARPS_PER_SCHEDULER);
	else
	{
		assert(rl_scheduler::gActionType == USE_CMD_PIPE_AS_ACTION);
		numActionBits = 2;
	}
	assert(numActionBits < 32);

	unsigned int totalNumBits = numAttrBits + numActionBits;
	assert(totalNumBits <= 32);

	unsigned int matrixSize = 1 << totalNumBits;
	return matrixSize;
}

float rl_scheduler::getQvalue(uint actionVal)
{
	assert(numActionBits != 0);
	assert(numAttrBits != 0);

	unsigned int index = (currState << numActionBits) + actionVal;

	// printf("index = %u, currState = %u, actionVal = %u\n", index, currState, actionVal);

	return Qvalues[index];
}

void rl_scheduler::updateQvalue(float selectedQ)
{
	unsigned int index = (prevState << numActionBits) + prevAction;
	float Qprev =  Qvalues[index];
	//float QprevSaved = Qprev;

	//Qprev = (1 - ALPHA) * Qprev + ALPHA * (reward + GAMMA * selectedQ);
	Qprev = (1 - rl_scheduler::gCurrAlpha) * Qprev + rl_scheduler::gCurrAlpha * (reward + GAMMA * selectedQ);

	assert(index < QvalueArraySize);
	Qvalues[index] = Qprev;
	QvalueUpdates[index]++;
	//if ((this->m_id == 0) && (this->m_shader->get_sid() == 0))
		//printf("Qvalues[%u] = %f -> %f, selectedQ=%f\n", index, QprevSaved, Qvalues[index], selectedQ);
}

void rl_scheduler::addAttribute(char* name, rl_attr_type type, unsigned int numValues, unsigned int defVal, unsigned int bucketSize)
{
	if ((this->m_id == 0) && (this->m_shader->get_sid() == 0))
		printf("Adding attribute %s\n", name);
	rl_attribute attr(name, type, numValues, defVal, bucketSize);
	attributeVector.push_back(attr);
}

void rl_scheduler::addAttributes(std::string rl_attrs)
{
	bool warpIdAttr = false;
	bool cmdPipeAttr = false;
	bool tbIdAttr = false;

	gNumSMs = m_shader->get_config()->n_simt_clusters * m_shader->get_config()->n_simt_cores_per_cluster;
	if ((this->m_id == 0) && (this->m_shader->get_sid() == 0))
		printf("Attributes:%s\n", rl_attrs.c_str());

	if (rl_attrs.find("LW") != std::string::npos)
	{
		addAttribute((char*)"LastWarpIssued", LAST_WARP_ISSUED, 24, 0, 1); //1-47 OR 2-48 in steps of 2
		warpIdAttr = true;
	}

	if (rl_attrs.find("NMQL") != std::string::npos)
	{
		addAttribute((char*)"NumMemQsLoaded", NUM_OF_MEM_QS_LOADED, 7, 0, 1); //0-6
		rl_scheduler::gNumMemSchedQsLoaded = 0;
		cmdPipeAttr = true;
	}

	if (rl_attrs.find("NMRQ") != std::string::npos)
	{
		addAttribute((char*)"NumMemReqsInSchedQ", NUM_OF_MEM_REQS_IN_SCHED_Q, 4, 0, 24); //0-23, 24-47, 48-71, 72+
		rl_scheduler::gNumReqsInMemSchedQs = 0;
		cmdPipeAttr = true;
	}

	if (rl_attrs.find("NSSIPMI") != std::string::npos)
	{
		addAttribute((char*)"NumOfSpSfuInstrPerMemInstrIssued", NUM_OF_SP_SFU_INSTR_PER_MEM_INSTR_ISSUED, 4, 0, 5); //1-5, 6-10, 11-15, 16+
		rl_scheduler::gNumSpInstrIssued = 0;
		rl_scheduler::gNumSfuInstrIssued = 0;
		rl_scheduler::gNumGTCMemInstrIssued = 0;
		rl_scheduler::gNumSpInstrIssued1 = 0;
		rl_scheduler::gNumSfuInstrIssued1 = 0;
		rl_scheduler::gNumGTCMemInstrIssued1 = 0;
		rl_scheduler::gNumSpInstrIssued2 = 0;
		rl_scheduler::gNumSfuInstrIssued2 = 0;
		rl_scheduler::gNumGTCMemInstrIssued2 = 0;
		cmdPipeAttr = true;
	}

	if (rl_attrs.find("AGML") != std::string::npos)
	{
		addAttribute((char*)"AvgGlobalMemLatency", AVG_GL_MEM_LAT, 4, 0, 100); //0-99, 100-199, 200-299, 300+
		rl_scheduler::gNumGTCMemInstrFinished = 0;
		rl_scheduler::gNumGTCMemLatencyCycles = 0;
		cmdPipeAttr = true;
	}

	if (rl_attrs.find("NMIE") != std::string::npos)
	{
		addAttribute((char*)"NumOfMemInstrsExecuting", NUM_OF_MEM_INSTRS_EXECUTING, 4, 0, 10*15); //0-9, 10-19, 20-29, 30-39
		rl_scheduler::gNumWarpsExecutingMemInstrGPU = 0;
		if (rl_scheduler::gNumWarpsExecutingMemInstr == 0)
		{
			rl_scheduler::gNumWarpsExecutingMemInstr = new unsigned int[gNumSMs];
			for (unsigned int i = 0; i < gNumSMs; i++)
				rl_scheduler::gNumWarpsExecutingMemInstr[i] = 0;
		}
		cmdPipeAttr = true;
	}

	if (rl_attrs.find("NRMI") != std::string::npos)
	{
		addAttribute((char*)"NumOfReadyMemInstrs", NUM_OF_READY_MEM_INSTRS, 4, 0, 6); //0-5, 6-11, 12-17, 18+
		numReadyMemInstrs = 0;
		cmdPipeAttr = true;
	}

	if (rl_attrs.find("NRSFI") != std::string::npos)
	{
		addAttribute((char*)"NumOfReadySfuInstrs", NUM_OF_READY_SFU_INSTRS, 4, 0, 6); //0-5, 6-11, 12-17, 18+
		numReadySfuInstrs = 0;
		cmdPipeAttr = true;
	}

	if (rl_attrs.find("NRSPI") != std::string::npos)
	{
		addAttribute((char*)"NumOfReadySpInstrs", NUM_OF_READY_SP_INSTRS, 4, 0, 6); //0-5, 6-11, 12-17, 18+
		numReadySpInstrs = 0;
		cmdPipeAttr = true;
	}

	if (rl_attrs.find("NRI") != std::string::npos)
	{
		addAttribute((char*)"NumOfReadyInstrs", NUM_OF_READY_INSTRS, 4, 0, 6); //0-5, 6-11, 12-17, 18+
		numReadyInstrs = 0;
		cmdPipeAttr = true;
	}

	if (rl_attrs.find("RMISTB") != std::string::npos)
	{
		addAttribute((char*)"NumOfReadyMemInstrsWithSameTBasLastMemInstr", NUM_OF_READY_MEM_INSTRS_WITH_SAME_TB_AS_LAST_MEM_INSTR, 4, 0, 4); //1 - mem instr with same TB
		if (rl_scheduler::gNumReadyMemInstrsWithSameTB == 0)
		{
			rl_scheduler::gNumReadyMemInstrsWithSameTB = new unsigned int[gNumSMs];
			rl_scheduler::gLastMemInstrTB = new unsigned int[gNumSMs];
			for (unsigned int i = 0; i < gNumSMs; i++)
			{
				rl_scheduler::gNumReadyMemInstrsWithSameTB[i] = 0;
				rl_scheduler::gLastMemInstrTB[i] = 0;
			}
		}
		cmdPipeAttr = true;
	}

	if (rl_attrs.find("RMISPC") != std::string::npos)
	{
		addAttribute((char*)"NumOfReadyMemInstrsWithSamePCasLastMemInstr", NUM_OF_READY_MEM_INSTRS_WITH_SAME_PC_AS_LAST_MEM_INSTR, 4, 0, 6); //1 - mem instr with same PC
		if (rl_scheduler::gNumReadyMemInstrsWithSamePC == 0)
		{
			rl_scheduler::gNumReadyMemInstrsWithSamePC = new unsigned int[gNumSMs];
			rl_scheduler::gLastMemInstrPC = new unsigned int[gNumSMs];
			for (unsigned int i = 0; i < gNumSMs; i++)
			{
				rl_scheduler::gNumReadyMemInstrsWithSamePC[i] = 0;
				rl_scheduler::gLastMemInstrPC[i] = 0;
			}
		}
		cmdPipeAttr = true;
	}

	if (rl_attrs.find("NTBW") != std::string::npos)
	{
		addAttribute((char*)"NumOfTBsWaiting", NUM_OF_TBS_WAITING, 1, 0, gNumSMs);
		tbIdAttr = true;
	}

	if (rl_attrs.find("NWIB") != std::string::npos)
	{
		addAttribute((char*)"NumOfWarpsIssuedBarrier", NUM_OF_WARPS_ISSUED_BARRIER, 4, 0, gNumWarpsPerBlock / 4);
		if (rl_scheduler::gNumWarpsWaitingAtBarrier == 0)
		{
			rl_scheduler::gNumWarpsWaitingAtBarrier = new unsigned int[gNumSMs];
			for (unsigned int i = 0; i < gNumSMs; i++)
				rl_scheduler::gNumWarpsWaitingAtBarrier[i] = 0;
		}
		tbIdAttr = true;
	}

	if (rl_attrs.find("NWF") != std::string::npos)
	{
		addAttribute((char*)"NumOfWarpsFinished", NUM_OF_WARPS_FINISHED, 4, 0, gNumWarpsPerBlock / 4);
		if (rl_scheduler::gNumWarpsFinished == 0)
		{
			rl_scheduler::gNumWarpsFinished = new unsigned int[gNumSMs];
			for (unsigned int i = 0; i < gNumSMs; i++)
				rl_scheduler::gNumWarpsFinished[i] = 0;
		}
		tbIdAttr = true;
	}

	if (rl_attrs.find("NRRMI") != std::string::npos)
	{
		addAttribute((char*)"NumOfReadyReadMemInstrs", NUM_OF_READY_READ_MEM_INSTRS, 4, 0, 6); //0-5, 6-11, 12-17, 18+
		numReadyReadMemInstrs = 0;
		cmdPipeAttr = true;
	}

	if (rl_attrs.find("NRWMI") != std::string::npos)
	{
		addAttribute((char*)"NumOfReadyWriteMemInstrs", NUM_OF_READY_WRITE_MEM_INSTRS, 4, 0, 6); //0-5, 6-11, 12-17, 18+
		numReadyWriteMemInstrs = 0;
		cmdPipeAttr = true;
	}

	if (rl_attrs.find("NRGMI") != std::string::npos)
	{
		addAttribute((char*)"NumOfReadyGlobalMemInstrs", NUM_OF_READY_GLOBAL_MEM_INSTRS, 4, 0, 6); //0-5, 6-11, 12-17, 18+
		numReadyGlobalMemInstrs = 0;
		cmdPipeAttr = true;
	}

	if (rl_attrs.find("NRSMI") != std::string::npos)
	{
		addAttribute((char*)"NumOfReadySharedMemInstrs", NUM_OF_READY_SHARED_MEM_INSTRS, 4, 0, 6); //0-5, 6-11, 12-17, 18+
		numReadySharedMemInstrs = 0;
		cmdPipeAttr = true;
	}

	if (rl_attrs.find("NRCMI") != std::string::npos)
	{
		addAttribute((char*)"NumOfReadyConstantMemInstrs", NUM_OF_READY_CONSTANT_MEM_INSTRS, 4, 0, 6); //0-5, 6-11, 12-17, 18+
		numReadyConstantMemInstrs = 0;
		cmdPipeAttr = true;
	}

	if (rl_attrs.find("NRTMI") != std::string::npos)
	{
		addAttribute((char*)"NumOfReadyTextureMemInstrs", NUM_OF_READY_TEXTURE_MEM_INSTRS, 4, 0, 6); //0-5, 6-11, 12-17, 18+
		numReadyTextureMemInstrs = 0;
		cmdPipeAttr = true;
	}

	if (rl_attrs.find("L1MP") != std::string::npos)
	{
		addAttribute((char*)"L1MissPercent", L1_MISS_PERCENT, 4, 0, 25); // 0-24% -> 0, 25-49% -> 1, 50-74% -> 2, 75-100% -> 3
		cmdPipeAttr = true;
	}

	if (rl_attrs.find("L2MP") != std::string::npos)
	{
		addAttribute((char*)"L2MissPercent", L2_MISS_PERCENT, 4, 0, 25); // 0-24 %-> 0, 25-49% -> 1, 50-74% -> 2, 75-100% -> 3
		cmdPipeAttr = true;
	}

	if ((rl_attrs.find("WTB") != std::string::npos) ||
	    (rl_attrs.find("LTB") != std::string::npos) ||
		(rl_attrs.find("TBWB") != std::string::npos) ||
		(rl_attrs.find("TBWF") != std::string::npos) ||
		(rl_attrs.find("FTB") != std::string::npos) ||
		((rl_attrs.find("STB") != std::string::npos) && (rl_attrs.find("RMISTB") == std::string::npos)))
	{
		addAttribute((char*)"WhichThreadBlock", WHICH_THREAD_BLOCK, 8, 0, 1); //1-8
		tbIdAttr = true;

		if ((rl_attrs.find("TBWB") != std::string::npos) || (rl_attrs.find("WTB") != std::string::npos))
		{
			allocateTBWithWarpsAtBarrierArray(gNumSMs);
		}

		if ((rl_attrs.find("TBWF") != std::string::npos) || (rl_attrs.find("WTB") != std::string::npos))
		{
			allocateTBWithWarpsFinishedArray(gNumSMs);
		}

		if ((rl_attrs.find("FTB") != std::string::npos) ||
		    (rl_attrs.find("STB") != std::string::npos) || 
			(rl_attrs.find("WTB") != std::string::npos))
		{
			allocateTBProgressArray(gNumSMs);

			if (gTBNumSpInstrsArray == 0)
			{
				rl_scheduler::gTBNumSpInstrsArray = new unsigned int[gNumSMs * MAX_NUM_TB_PER_SM];
				rl_scheduler::gTBNumSfuInstrsArray = new unsigned int[gNumSMs * MAX_NUM_TB_PER_SM];
				rl_scheduler::gTBNumMemInstrsArray = new unsigned int[gNumSMs * MAX_NUM_TB_PER_SM];
				for (unsigned int i = 0; i < gNumSMs; i++)
				{
					for (unsigned int j = 0; j < MAX_NUM_TB_PER_SM; j++)
					{
						unsigned int index = i * MAX_NUM_TB_PER_SM + j;
						rl_scheduler::gTBNumSpInstrsArray[index] = 0;
						rl_scheduler::gTBNumSfuInstrsArray[index] = 0;
						rl_scheduler::gTBNumMemInstrsArray[index] = 0;
					}
				}
			}
		}
	}

	if ((rl_attrs.find("RSFI") != std::string::npos) && (rl_attrs.find("NRSFI") == std::string::npos))
	{
		addAttribute((char*)"ReadySfuInstrs", READY_SFU_INSTRS, 2, 0, 1); //>=1
		readySfuInstrs = 0;
		cmdPipeAttr = true;
	}

	if (rl_attrs.find("RGCTRMI") != std::string::npos)
	{
		addAttribute((char*)"ReadyGlobalConstTextureReadMemInstrs", READY_GLOBAL_CONST_TEXTURE_READ_MEM_INSTRS, 2, 0, 1); //>=1
		readyGlobalConstTexReadMemInstr = 0;
		cmdPipeAttr = true;
	}

	if (rl_attrs.find("RGRMI") != std::string::npos)
	{
		addAttribute((char*)"ReadyGlobalReadMemInstrs", READY_GLOBAL_READ_MEM_INSTRS, 2, 0, 1); //>=1
		readyGlobalReadMemInstr = 0;
		cmdPipeAttr = true;
	}

	if (rl_attrs.find("WW") != std::string::npos)
	{
		addAttribute((char*)"WhichWarp", WHICH_WARP, 32, 0, 1);

		allocateWarpProgressArray(gNumSMs);

		allocateTBWithWarpsFinishedArray(gNumSMs);

		allocateTBWithWarpsAtBarrierArray(gNumSMs);

		if (gSelectedTB == 0)
		{
			gSelectedTB = new unsigned int[gNumSMs];
			for (unsigned int i = 0; i < gNumSMs; i++)
				gSelectedTB[i] = 0xdeaddead;
		}
		if (gSelectedWarp == 0)
		{
			gSelectedWarp = new unsigned int[gNumSMs * NUM_SCHED_PER_SM];
			for (unsigned int i = 0; i < gNumSMs * NUM_SCHED_PER_SM; i++)
				gSelectedWarp[i] = 0xdeaddead;
		}
		warpIdAttr = true;
		tbIdAttr = true;
	}

	if (rl_attrs.find("WP") != std::string::npos)
	{
		addAttribute((char*)"WhichPipeline", WHICH_PIPELINE, 4, 0, 1);

		numReadyMemInstrs = 0;
		numReadySfuInstrs = 0;
		numReadySpInstrs = 0;

		rl_scheduler::gNumWarpsExecutingMemInstrGPU = 0;
		if (gGTCLongLatMemInstrCache == 0)
		{
			gGTCLongLatMemInstrCache = new unsigned int[gNumSMs * GTC_LONG_LAT_MEM_INSTR_CACHE_SIZE];
			for (unsigned int i = 0; i < gNumSMs * GTC_LONG_LAT_MEM_INSTR_CACHE_SIZE; i++)
				gGTCLongLatMemInstrCache[i] = 0;
		}
		if (gSFULongLatInstrCache == 0)
		{
			gSFULongLatInstrCache = new unsigned int[gNumSMs * SFU_LONG_LAT_INSTR_CACHE_SIZE];
			for (unsigned int i = 0; i < gNumSMs * SFU_LONG_LAT_INSTR_CACHE_SIZE; i++)
				gSFULongLatInstrCache[i] = 0;
		}
		if (rl_scheduler::gNumWarpsExecutingMemInstr == 0)
		{
			rl_scheduler::gNumWarpsExecutingMemInstr = new unsigned int[gNumSMs];
			for (unsigned int i = 0; i < gNumSMs; i++)
				rl_scheduler::gNumWarpsExecutingMemInstr[i] = 0;
		}
		cmdPipeAttr = true;
	}

	if (tbIdAttr)
	{
		if (cmdPipeAttr)
			rl_scheduler::gActionType = USE_TB_CMD_PIPE_AS_ACTION;
		else if (warpIdAttr)
			rl_scheduler::gActionType = USE_TB_WARP_ID_AS_ACTION;
		else
			rl_scheduler::gActionType = USE_TB_ID_AS_ACTION;
	}
	else if (cmdPipeAttr)
		rl_scheduler::gActionType = USE_CMD_PIPE_AS_ACTION;
	else if (warpIdAttr)
		rl_scheduler::gActionType = USE_WARP_ID_AS_ACTION;
	else
		assert(0);
}

FILE* qvaluesFile = 0;
void rl_scheduler::printQvalueUpdateCounts()
{
	unsigned int smId = m_shader->get_sid();
	printf("RL: QvalueUpdateCounts for SM %u Sched %u\n", smId, this->m_id);
	bool writeToqvaluesFile = false;
	if (qvaluesFile == 0)
	{
		qvaluesFile = fopen("qvalues.txt", "w");
		writeToqvaluesFile = true;
	}

	unsigned int totalQvalueUpdates = 0;
	for (unsigned int i = 0; i < QvalueArraySize; i++)
	{
		totalQvalueUpdates += QvalueUpdates[i];
		//if (QvalueUpdates[i] != 0)
			//printf("index %u has %u updates and final value is %f\n", i, QvalueUpdates[i], Qvalues[i]);
		if (writeToqvaluesFile)
			fprintf(qvaluesFile, "%f\n", Qvalues[i]);
	}

	float totalQvalue = 0.0;
	for (unsigned int i = 0; i < QvalueArraySize; i++)
	{
		totalQvalue += Qvalues[i];
		//printf("index %u-%u has %u(%.2f) updates and final value is %f\n", i >> numActionBits, i & ((1 << numActionBits) - 1), QvalueUpdates[i], (float)QvalueUpdates[i]/(float)totalQvalueUpdates, Qvalues[i]);
	}
	printf("RL: Total QvalueUpdates %u, total of all Qvalues = %.2f for SM %u Sched %u\n", totalQvalueUpdates, totalQvalue, smId, this->m_id);
}

void rl_scheduler::clear()
{
	//this function is called at the end of a kernel, so if there are multiple kernels
	//pass the existing Qvalues so that a new Qvalues table is not created and the existing
	//one is initialized to default values
	initQvalues();
	initQvalueUpdates();
	initCurrStateAndAction();

	if (rl_scheduler::gNumWarpsExecutingMemInstr)
	    rl_scheduler::gNumWarpsExecutingMemInstr[m_shader->get_sid()] = 0;
	if (rl_scheduler::gNumReadyMemInstrsWithSameTB)
	    rl_scheduler::gNumReadyMemInstrsWithSameTB[m_shader->get_sid()] = 0;
	if (rl_scheduler::gLastMemInstrTB)
	    rl_scheduler::gLastMemInstrTB[m_shader->get_sid()] = 0;
	if (rl_scheduler::gNumReadyMemInstrsWithSamePC)
	    rl_scheduler::gNumReadyMemInstrsWithSamePC[m_shader->get_sid()] = 0;
	if (rl_scheduler::gLastMemInstrPC)
	    rl_scheduler::gLastMemInstrPC[m_shader->get_sid()] = 0;
	if (rl_scheduler::gNumWarpsWaitingAtBarrier)
	    rl_scheduler::gNumWarpsWaitingAtBarrier[m_shader->get_sid()] = 0;
	if (rl_scheduler::gNumWarpsFinished)
	    rl_scheduler::gNumWarpsFinished[m_shader->get_sid()] = 0;

	firstTime = true;
	currPhaseCycles = 0;
	currPhase = NO_INST_ISSUED;
}

void rl_scheduler::initQvalues()
{
	FILE* fp = fopen("qvalues.txt", "r");
	if (rl_scheduler::gUsePrevQvalues && (fp != NULL))
	{
		unsigned int i = 0;
		while (!feof(fp))
		{
			char line[1024];
			char* ptr = fgets(line, 1024, fp);
			if (ptr == NULL)
				break;
			double qValue = atof(ptr);
			if (i >= QvalueArraySize)
			{
				printf("incompatible qvalues.txt file\n");
				for (unsigned int i = 0; i < QvalueArraySize; i++)
					Qvalues[i] = 1 / (1 - GAMMA);
				break;
			}
			Qvalues[i] = qValue;
			//if ((this->m_shader->get_sid() == 0) && (this->m_id == 0))
				//printf("initializing Qvalue[%u]=%f\n", i, qValue);
			i++;
		}
		while (i < QvalueArraySize)
		{
			Qvalues[i] = 1 / (1 - GAMMA);
			i++;
		}
	}
	else
	{
		for (unsigned int i = 0; i < QvalueArraySize; i++)
			Qvalues[i] = 1 / (1 - GAMMA);
	}
}

void rl_scheduler::initQvalueUpdates()
{
	for (unsigned int i = 0; i < QvalueArraySize; i++)
		QvalueUpdates[i] = 0;
}

float* rl_scheduler::allocateQvalues(float* QvalueTable)
{
	QvalueArraySize = getQvalueMatrixSize();

	if (QvalueTable == 0)
		Qvalues = new float[QvalueArraySize];
	else
		Qvalues = QvalueTable;

	return Qvalues;
}

unsigned int* rl_scheduler::allocateQvalueUpdates(unsigned int* QvalueUpdateTable)
{
	QvalueArraySize = getQvalueMatrixSize();

	if (QvalueUpdateTable == 0)
		QvalueUpdates = new unsigned int[QvalueArraySize];
	else
		QvalueUpdates = QvalueUpdateTable;

	return QvalueUpdates;
}

rl_scheduler::rl_scheduler ( shader_core_stats* stats, shader_core_ctx* shader,
                               Scoreboard* scoreboard, simt_stack** simt,
                               std::vector<shd_warp_t>* warp,
                               register_set* sp_out,
                               register_set* sfu_out,
                               register_set* mem_out,
                               int id)
    : scheduler_unit ( stats, shader, scoreboard, simt, warp, sp_out, sfu_out, mem_out, id )
{
	QvalueArraySize = 0;
	Qvalues = 0;
	QvalueUpdates = 0;
	numActionBits = 0;
	numAttrBits = 0;
	//reward = 0;
	reward = 0.0;
	firstTime = true;
	numInstrIssued = 0;
	currPhaseCycles = 0;
	currPhase = NO_INST_ISSUED;
	prevAction = 0;
	prevState = 0;
}

void rl_scheduler::initRLAttributeArrays(unsigned int smId)
{
	numReadyMemInstrs = 0;
	gGTCLongLatMemInstrReady = false;
	gSFULongLatInstrReady = false;
	numReadyGlobalMemInstrs = 0;
	numReadySharedMemInstrs = 0;
	numReadyConstantMemInstrs = 0;
	numReadyTextureMemInstrs = 0;
	numReadyReadMemInstrs = 0;
	numReadyWriteMemInstrs = 0;
	numReadySfuInstrs = 0;
	numReadySpInstrs = 0;
	numReadyInstrs = 0;

	if (rl_scheduler::gNumReadyMemInstrsWithSameTB)
		rl_scheduler::gNumReadyMemInstrsWithSameTB[smId] = 0;
	if (rl_scheduler::gNumReadyMemInstrsWithSamePC)
		rl_scheduler::gNumReadyMemInstrsWithSamePC[smId] = 0;
	if (rl_scheduler::gNumWarpsWaitingAtBarrier)
		rl_scheduler::gNumWarpsWaitingAtBarrier[smId] = 0;
	if (rl_scheduler::gNumWarpsFinished)
		rl_scheduler::gNumWarpsFinished[smId] = 0;

	readySfuInstrs = 0;
	readyGlobalConstTexReadMemInstr = 0;
	readyGlobalReadMemInstr = 0;
}

void rl_scheduler::order_warps()
{
    m_next_cycle_prioritized_warps.clear();

	unsigned int smId = m_shader->get_sid();
	assert(gNumSMs > smId);
	initRLAttributeArrays(smId);

	unsigned int tbWithWarpsAtBarrier = 0xdeaddead;
	bool prevTBWithWarpsAtBarrier = false;
	if (rl_scheduler::gTBWithWarpsAtBarrier)
	{
		tbWithWarpsAtBarrier = rl_scheduler::gTBWithWarpsAtBarrier[smId];
		prevTBWithWarpsAtBarrier = false;
		rl_scheduler::gTBWithWarpsAtBarrier[smId] = 0xdeaddead;
	}

	//unsigned int prevNumMemSchedQsLoaded = rl_scheduler::gNumMemSchedQsLoaded;
	rl_scheduler::gNumMemSchedQsLoaded = 0;
	rl_scheduler::gNumReqsInMemSchedQs = 0;
	if (gNumReqsInMemSchedArray)
	{
		for (unsigned int i = 0; i < 6; i++)
		{
			rl_scheduler::gNumReqsInMemSchedQs += gNumReqsInMemSchedArray[i];
			if (gNumReqsInMemSchedArray[i] > 12)
				rl_scheduler::gNumMemSchedQsLoaded++;
		}
		//if ((smId == 0) && (m_id == 0) /*&& (prevNumMemSchedQsLoaded != rl_scheduler::gNumMemSchedQsLoaded)*/)
			//printf("%llu: num mem sched queues loaded = %u, num mem reqs in sched queues = %u\n", gpu_sim_cycle, rl_scheduler::gNumMemSchedQsLoaded, rl_scheduler::gNumReqsInMemSchedQs);
	}

	gReadyTBIdSet.clear();
    for (std::vector<shd_warp_t*>::const_iterator iter = m_supervised_warps.begin(); 
	     iter != m_supervised_warps.end(); 
		 iter++) 
	{
       	// Don't consider warps that are not yet valid
       	if ((*iter) == NULL)
           	continue;
       	if ((*iter)->done_exit())
		{
           	if (rl_scheduler::gNumWarpsFinished)
				rl_scheduler::gNumWarpsFinished[smId]++;
			continue;
		}

       	unsigned warp_id = (*iter)->get_warp_id();
		unsigned int tbId = (*iter)->get_cta_id();

       	//if (!warp(warp_id).waiting() && !warp(warp_id).ibuffer_empty())
       	if (!warp(warp_id).ibuffer_empty())
		{
           	const warp_inst_t* pI = warp(warp_id).ibuffer_next_inst();

			if (m_shader->warp_waiting_at_barrier(warp_id))
			{
				if (tbWithWarpsAtBarrier == (*iter)->get_cta_id())
					prevTBWithWarpsAtBarrier = true;
				else if (rl_scheduler::gTBWithWarpsAtBarrier && (rl_scheduler::gTBWithWarpsAtBarrier[smId] == 0xdeaddead))
				{
					rl_scheduler::gTBWithWarpsAtBarrier[smId] = (*iter)->get_cta_id();
				}

				if (rl_scheduler::gNumWarpsWaitingAtBarrier)
					rl_scheduler::gNumWarpsWaitingAtBarrier[smId]++;
				
				continue;
			}
           	if(pI) 
			{
           		unsigned pc, rpc;

           		m_simt_stack[warp_id]->get_pdom_stack_top_info(&pc, &rpc);

               	if( pc == pI->pc ) 
				{
                   	if (!m_scoreboard->checkCollision(warp_id, pI)) 
					{
						gReadyTBIdSet.insert(tbId);
						numReadyInstrs++;
                        if ((pI->op == LOAD_OP) || (pI->op == STORE_OP) || (pI->op == MEMORY_BARRIER_OP)) 
						{
							numReadyMemInstrs++;

							if (gGTCLongLatMemInstrCache)
							{
								for (unsigned int i = 0; i < GTC_LONG_LAT_MEM_INSTR_CACHE_SIZE; i++)
								{
									if (pc == gGTCLongLatMemInstrCache[(smId * GTC_LONG_LAT_MEM_INSTR_CACHE_SIZE) + i])
									{
										gGTCLongLatMemInstrReady = true;
										//printf("%llu: gNumWarpsExecutingMemInstrGPU > HIGH_NUM_WARPS_EXECUTING_MEM_INSTR and long latency mem instr %u ready\n", gpu_sim_cycle, pc);
										break;
									}
								}
							}

							if (pI->space.get_type() == global_space)
								numReadyGlobalMemInstrs++;

							if (pI->space.get_type() == shared_space)
								numReadySharedMemInstrs++;

							if (pI->space.get_type() == const_space)
								numReadyConstantMemInstrs++;

							if (pI->space.get_type() == tex_space)
								numReadyTextureMemInstrs++;

							if (pI->op == LOAD_OP)
								numReadyReadMemInstrs++;

							if (pI->op == STORE_OP)
								numReadyWriteMemInstrs++;

							if (rl_scheduler::gLastMemInstrTB && rl_scheduler::gNumReadyMemInstrsWithSameTB)
								if ((*iter)->get_cta_id() == rl_scheduler::gLastMemInstrTB[smId])
									rl_scheduler::gNumReadyMemInstrsWithSameTB[smId]++;

							if (rl_scheduler::gLastMemInstrPC && rl_scheduler::gNumReadyMemInstrsWithSamePC)
								if (pI->pc == rl_scheduler::gLastMemInstrPC[smId])
									rl_scheduler::gNumReadyMemInstrsWithSamePC[smId]++;

							if ((pI->op == LOAD_OP) || (pI->op == MEMORY_BARRIER_OP))
							{
								if ((pI->space.get_type() == global_space) || 
								    (pI->space.get_type() == const_space) ||
									(pI->space.get_type() == tex_space))
								{
									readyGlobalConstTexReadMemInstr = 1;
								}
								if (pI->space.get_type() == global_space)
									readyGlobalReadMemInstr = 1;
							}
						}
						else if ((pI->op == SFU_OP) || (pI->op == ALU_SFU_OP))
						{
							numReadySfuInstrs++;
							readySfuInstrs = 1;
							if (gSFULongLatInstrCache)
							{
								for (unsigned int i = 0; i < SFU_LONG_LAT_INSTR_CACHE_SIZE; i++)
								{
									if (pc == gSFULongLatInstrCache[(smId * SFU_LONG_LAT_INSTR_CACHE_SIZE) + i])
									{
										gSFULongLatInstrReady = true;
										break;
									}
								}
							}
						}
						else
						{
							numReadySpInstrs++;
						}
					}
               	}
           	} 
       	}
   	}

	/*
	if ((smId == 0) && (this->m_id == 0) && (gReadyTBIdSet.size() > 0) && isRLSched())
	{
		printf("%llu: ready TBs = %u:", gpu_sim_cycle, gReadyTBIdSet.size());
		for (std::set<unsigned int>::iterator iter = gReadyTBIdSet.begin();
		     iter != gReadyTBIdSet.end();
			 iter++)
		{
			unsigned int cta_num = (*iter);
			unsigned index = smId * MAX_NUM_TB_PER_SM + cta_num;
			printf(" %u(%u)", cta_num, gTBProgressArray[index]);
		}
		printf("\n");
	}
	*/

	if (rl_scheduler::gTBWithWarpsAtBarrier && (prevTBWithWarpsAtBarrier == true))
		rl_scheduler::gTBWithWarpsAtBarrier[smId] = tbWithWarpsAtBarrier;

	setAttributeValues2();

	computeCurrState();

    std::vector<shd_warp_t*> warpVec = m_supervised_warps;
	if (warpVec.size() > 0)
	{
		long int randVal = random();
		float rVal = (randVal / (float) RAND_MAX) * 100;
		//bool exploration = (rVal < EXPLORATION_PERCENT) ? true : false;
		bool exploration = (rVal < rl_scheduler::gCurrExplorationPercent) ? true : false;
		if ((smId == 0) && (this->m_id == 0))
		{
			if ((rl_scheduler::gCurrExplorationPercent > 5) && ((gpu_sim_cycle % EXPLORATION_REDUCTION_CYCLES) == 0))
			{
				rl_scheduler::gCurrExplorationPercent--;
				//printf("gCurrExplorationPercent = %u\n", rl_scheduler::gCurrExplorationPercent);
			}
			if ((rl_scheduler::gCurrAlpha > 0.1) && ((gpu_sim_cycle % ALPHA_REDUCTION_CYCLES) == 0))
			{
				rl_scheduler::gCurrAlpha -= 0.01;
				//printf("gCurrAlpha = %f\n", rl_scheduler::gCurrAlpha);
			}
		}
		bool initPhase = (numInstrIssued < INIT_PHASE_NUM_INSTRS) ? 1 : 0;

		if (initPhase)
		{
			//in the init phase, use GTO sched to schedule instrs
			//this will bias the Qvalue table to 'learn' quickly

        	shd_warp_t* greedy_value = *m_last_supervised_issued;
        	m_next_cycle_prioritized_warps.push_back( greedy_value );

        	std::sort(warpVec.begin(), warpVec.end(), scheduler_unit::sort_warps_by_oldest_dynamic_id);
	
        	std::vector< shd_warp_t* >::iterator iter = warpVec.begin();
	
        	for (unsigned count = 0; count < warpVec.size(); ++count, ++iter)
			{
				if (greedy_value != (*iter))
           			m_next_cycle_prioritized_warps.push_back( *iter );
			}
		}
		else if ((firstTime || exploration) && (warpVec.size() > 0))
		{
			//if ((smId == 0) && (this->m_id == 0))
				//printf("cycle = %llu, Exploring ", gpu_sim_cycle);

			rl_scheduler::gExplorationCnt++;

			unsigned int idx = randVal % warpVec.size();
			shd_warp_t* warp = warpVec[idx];
			m_next_cycle_prioritized_warps.push_back(warp);
        	for (unsigned i = (idx + 1); i < warpVec.size(); ++i)
           		m_next_cycle_prioritized_warps.push_back(warpVec[i]);
        	for (unsigned i = 0; i < idx; ++i)
         		m_next_cycle_prioritized_warps.push_back(warpVec[i]);
		}
		else
		{
			//if ((smId == 0) && (this->m_id == 0))
				//printf("cycle = %llu, Exploiting ", gpu_sim_cycle);

			rl_scheduler::gQvalues = Qvalues;
			rl_scheduler::gStateVal = currState;
			rl_scheduler::gNumActionBits = numActionBits;
			rl_scheduler::gScoreboard = m_scoreboard;
			rl_scheduler::gSimtStack = m_simt_stack;
	
			if (rl_scheduler::gActionType == USE_CMD_PIPE_AS_ACTION)
			{
				std::vector<float> valVec;
				unsigned int index = (rl_scheduler::gStateVal << rl_scheduler::gNumActionBits) + UNKNOWN_OP;
				float nop_val =  rl_scheduler::gQvalues[index];
				valVec.push_back(nop_val);

				index = (rl_scheduler::gStateVal << rl_scheduler::gNumActionBits) + SP__OP;
				float sp_val =  rl_scheduler::gQvalues[index];
				valVec.push_back(sp_val);

				index = (rl_scheduler::gStateVal << rl_scheduler::gNumActionBits) + SFU__OP;
				float sfu_val =  rl_scheduler::gQvalues[index];
				valVec.push_back(sfu_val);

				index = (rl_scheduler::gStateVal << rl_scheduler::gNumActionBits) + MEM__OP;
				float mem_val =  rl_scheduler::gQvalues[index];
				valVec.push_back(mem_val);

				//if ((smId == 0) /*&& (this->m_id == 0)*/)
					//printf("nop_val = %f, sp_val = %f, sfu_val = %f, mem_val = %f, at %llu\n", nop_val, sp_val, sfu_val, mem_val, gpu_sim_cycle);

				float max_val = nop_val;
				rl_scheduler::gSelectedActionVal = UNKNOWN_OP;
				if (max_val <= sp_val)
				{
					rl_scheduler::gSelectedActionVal = SP__OP;
					max_val = sp_val;
				}
				if (max_val <= sfu_val)
				{
					rl_scheduler::gSelectedActionVal = SFU__OP;
					max_val = sfu_val;
				}
				if (max_val <= mem_val)
				{
					rl_scheduler::gSelectedActionVal = MEM__OP;
					max_val = mem_val;
				}
				//if ((smId == 0) && (this->m_id == 0))
					//printf("RL:Selected %u at %llu\n", rl_scheduler::gSelectedActionVal, gpu_sim_cycle);

				std::sort(valVec.begin(), valVec.end());

				/*
				if (smId == 0)
				{
					for (unsigned int i = 0; i < valVec.size(); i++)
						printf("valVec[%u] = %f ", i, valVec[i]);
					printf("\n");
				}
				*/

				std::set<unsigned int> addedWarps;
				//add warps in decreasing order of Q values
				for (int i = valVec.size() - 1 ; i >= 0; i--)
				{
        			std::vector< shd_warp_t* >::iterator iter = warpVec.begin();
        			for (unsigned count = 0; count < warpVec.size(); ++count, ++iter)
					{
						unsigned int warpId = (*iter)->get_warp_id();
						if (addedWarps.find(warpId) == addedWarps.end())
						{
							const warp_inst_t* pI;
							if ((*iter)->ibuffer_empty() == false)
							{
								pI = (*iter)->ibuffer_next_inst();
								unsigned int actionVal = getCmdPipeType(pI);
								unsigned int index = (rl_scheduler::gStateVal << rl_scheduler::gNumActionBits) + actionVal;
								if (valVec[i] == rl_scheduler::gQvalues[index])
								{
									addedWarps.insert(warpId);
            						m_next_cycle_prioritized_warps.push_back(*iter);
								}
							}
						}
					}
				}
			}
			else
			{
        		std::sort(warpVec.begin(), warpVec.end(), scheduler_unit::sort_warps_by_highest_q_value);
	
        		std::vector< shd_warp_t* >::iterator iter = warpVec.begin();
	
        		for (unsigned count = 0; count < warpVec.size(); ++count, ++iter)
            		m_next_cycle_prioritized_warps.push_back( *iter );
			}
	
			rl_scheduler::gQvalues = 0;
			rl_scheduler::gStateVal = 0;
			rl_scheduler::gNumActionBits = 0;
			rl_scheduler::gScoreboard = 0;
			rl_scheduler::gSimtStack = 0;
		}
	}
}

void rl_scheduler::computeNextQvalueAndUpdateOldQvalue(shd_warp_t* warp, operation_pipeline_t pipeUsed)
{
	//if ((m_shader->get_sid() == 0) && (this->m_id == 0))
		//printf("PipeUsed = %u\n", pipeUsed);

	//if ((m_shader->get_sid() == 0) /*&& (this->m_id == 0)*/ /*&& (rl_scheduler::gSelectedActionVal != pipeUsed)*/)
	//{
		//printf("RL:Executed %u at %llu\n", pipeUsed, gpu_sim_cycle);
		//printf("RL:Selected %u, Executed %u at %llu\n", rl_scheduler::gSelectedActionVal, pipeUsed, gpu_sim_cycle);
	//}
	if (firstTime == false)
	{
		unsigned int action = computeAction(warp, pipeUsed);

		float Qselected = getQvalue(action);

		updateQvalue(Qselected);
	}
	else
	{
		firstTime = false;
	}
}

void shader_core_ctx::read_operands()
{
}

address_type coalesced_segment(address_type addr, unsigned segment_size_lg2bytes)
{
   return  (addr >> segment_size_lg2bytes);
}

// Returns numbers of addresses in translated_addrs, each addr points to a 4B (32-bit) word
unsigned shader_core_ctx::translate_local_memaddr( address_type localaddr, unsigned tid, unsigned num_shader, unsigned datasize, new_addr_type* translated_addrs )
{
   // During functional execution, each thread sees its own memory space for local memory, but these
   // need to be mapped to a shared address space for timing simulation.  We do that mapping here.

   address_type thread_base = 0;
   unsigned max_concurrent_threads=0;
   if (m_config->gpgpu_local_mem_map) {
      // Dnew = D*N + T%nTpC + nTpC*C
      // N = nTpC*nCpS*nS (max concurent threads)
      // C = nS*K + S (hw cta number per gpu)
      // K = T/nTpC   (hw cta number per core)
      // D = data index
      // T = thread
      // nTpC = number of threads per CTA
      // nCpS = number of CTA per shader
      // 
      // for a given local memory address threads in a CTA map to contiguous addresses,
      // then distribute across memory space by CTAs from successive shader cores first, 
      // then by successive CTA in same shader core
      thread_base = 4*(kernel_padded_threads_per_cta * (m_sid + num_shader * (tid / kernel_padded_threads_per_cta))
                       + tid % kernel_padded_threads_per_cta); 
      max_concurrent_threads = kernel_padded_threads_per_cta * kernel_max_cta_per_shader * num_shader;
   } else {
      // legacy mapping that maps the same address in the local memory space of all threads 
      // to a single contiguous address region 
      thread_base = 4*(m_config->n_thread_per_shader * m_sid + tid);
      max_concurrent_threads = num_shader * m_config->n_thread_per_shader;
   }
   assert( thread_base < 4/*word size*/*max_concurrent_threads );

   // If requested datasize > 4B, split into multiple 4B accesses
   // otherwise do one sub-4 byte memory access
   unsigned num_accesses = 0;

   if(datasize >= 4) {
      // >4B access, split into 4B chunks
      assert(datasize%4 == 0);   // Must be a multiple of 4B
      num_accesses = datasize/4;
      assert(num_accesses <= MAX_ACCESSES_PER_INSN_PER_THREAD); // max 32B
      assert(localaddr%4 == 0); // Address must be 4B aligned - required if accessing 4B per request, otherwise access will overflow into next thread's space
      for(unsigned i=0; i<num_accesses; i++) {
          address_type local_word = localaddr/4 + i;
          address_type linear_address = local_word*max_concurrent_threads*4 + thread_base + LOCAL_GENERIC_START;
          translated_addrs[i] = linear_address;
      }
   } else {
      // Sub-4B access, do only one access
      assert(datasize > 0);
      num_accesses = 1;
      address_type local_word = localaddr/4;
      address_type local_word_offset = localaddr%4;
      assert( (localaddr+datasize-1)/4  == local_word ); // Make sure access doesn't overflow into next 4B chunk
      address_type linear_address = local_word*max_concurrent_threads*4 + local_word_offset + thread_base + LOCAL_GENERIC_START;
      translated_addrs[0] = linear_address;
   }
   return num_accesses;
}

/////////////////////////////////////////////////////////////////////////////////////////
int shader_core_ctx::test_res_bus(int latency){
	for(unsigned i=0; i<num_result_bus; i++){
		if(!m_result_bus[i]->test(latency)){return i;}
	}
	return -1;
}

void shader_core_ctx::execute()
{
	for(unsigned i=0; i<num_result_bus; i++){
		*(m_result_bus[i]) >>=1;
	}
    for( unsigned n=0; n < m_num_function_units; n++ ) {
        unsigned multiplier = m_fu[n]->clock_multiplier();
        for( unsigned c=0; c < multiplier; c++ ) 
            m_fu[n]->cycle();
        m_fu[n]->active_lanes_in_pipeline();
        enum pipeline_stage_name_t issue_port = m_issue_port[n];
        register_set& issue_inst = m_pipeline_reg[ issue_port ];
	warp_inst_t** ready_reg = issue_inst.get_ready();
        if( issue_inst.has_ready() && m_fu[n]->can_issue( **ready_reg ) ) {
            bool schedule_wb_now = !m_fu[n]->stallable();
            int resbus = -1;
            if( schedule_wb_now && (resbus=test_res_bus( (*ready_reg)->latency ))!=-1 ) {
                assert( (*ready_reg)->latency < MAX_ALU_LATENCY );
                m_result_bus[resbus]->set( (*ready_reg)->latency );
                m_fu[n]->issue( issue_inst );
            } else if( !schedule_wb_now ) {
                m_fu[n]->issue( issue_inst );
            } else {
                // stall issue (cannot reserve result bus)
            }
        }
    }
}

void ldst_unit::print_cache_stats( FILE *fp, unsigned& dl1_accesses, unsigned& dl1_misses ) {
   if( m_L1D ) {
       m_L1D->print( fp, dl1_accesses, dl1_misses );
   }
}

void ldst_unit::get_cache_stats(cache_stats &cs) {
    // Adds stats to 'cs' from each cache
    if(m_L1D)
        cs += m_L1D->get_stats();
    if(m_L1C)
        cs += m_L1C->get_stats();
    if(m_L1T)
        cs += m_L1T->get_stats();

}

void ldst_unit::get_L1D_sub_stats(struct cache_sub_stats &css) const{
    if(m_L1D)
        m_L1D->get_sub_stats(css);
}
void ldst_unit::get_L1C_sub_stats(struct cache_sub_stats &css) const{
    if(m_L1C)
        m_L1C->get_sub_stats(css);
}
void ldst_unit::get_L1T_sub_stats(struct cache_sub_stats &css) const{
    if(m_L1T)
        m_L1T->get_sub_stats(css);
}

std::map<unsigned int, unsigned long long> instrLatMap;
void shader_core_ctx::warp_inst_complete(const warp_inst_t &inst)
{
	//if (m_sid == 0)
		//printf("Inst %u for warp %u completed at %llu\n", inst.pc, inst.dynamic_warp_id(), gpu_sim_cycle);

	unsigned long long latCycles = gpu_sim_cycle - inst.get_issue_cycle();

	if (m_sid == 0)
	{
		char instrType[10];
  		if(inst.op_pipe==SP__OP)
  			strcpy(instrType, "SP");
		else if(inst.op_pipe==SFU__OP)
  			strcpy(instrType, "SFU");
 		else if(inst.op_pipe==MEM__OP)
  			strcpy(instrType, "MEM");
		else
  			strcpy(instrType, "");
		std::string instrStr = ptx_get_insn_str(inst.pc);
		if (instrLatMap.find(inst.pc) == instrLatMap.end())
			instrLatMap[inst.pc] = latCycles;
		//printf("%s instr pc = %u, (%s) latency = %llu at sim cycle %llu\n", instrType, inst.pc, instrStr.c_str(), latCycles, gpu_sim_cycle);
	}

    if ((inst.op == LOAD_OP) || (inst.op == STORE_OP) || (inst.op == MEMORY_BARRIER_OP)) 
	{
		if (rl_scheduler::gNumWarpsExecutingMemInstr)
		{
			uint smId = this->get_sid();
			assert(gNumSMs > smId);
			rl_scheduler::gNumWarpsExecutingMemInstr[smId]--;
		}

		rl_scheduler::gNumWarpsExecutingMemInstrGPU--;

		//if (m_sid == 0)
			//printf("%llu: numWarpsExecutingMemInstrGPU = %u\n", gpu_sim_cycle, rl_scheduler::gNumWarpsExecutingMemInstrGPU);

		if ((inst.space.get_type() == global_space) || 
			(inst.space.get_type() == const_space) || 
			(inst.space.get_type() == tex_space))
		{
			rl_scheduler::gNumGTCMemInstrFinished++;


			if (gGTCLongLatMemInstrCache)
			{
				gGTCTotalLatCycles += latCycles;
				//unsigned long long avgGTCLatCycles = gGTCTotalLatCycles / rl_scheduler::gNumGTCMemInstrFinished;

				char instType[10];
				if (inst.space.get_type() == global_space)
					strcpy(instType, "Global");
				else if (inst.space.get_type() == const_space)
					strcpy(instType, "Const");
				else if (inst.space.get_type() == tex_space)
					strcpy(instType, "Tex");
				else
					strcpy(instType, "OUCH");
	
				if (/*(latCycles >= avgGTCLatCycles) && */(latCycles >= LONG_LATENCY))
				{
					bool add = true;
					for (unsigned int i = 0; i < GTC_LONG_LAT_MEM_INSTR_CACHE_SIZE; i++)
					{
						if (inst.pc == gGTCLongLatMemInstrCache[(m_sid * GTC_LONG_LAT_MEM_INSTR_CACHE_SIZE) + i])
						{
							add = false;
							break;
						}
					}
					if (add == true)
					{
						//if (m_sid == 0)
							//printf("%llu: Adding inst %u(%s) as a GTC long latency instr with lat = %llu\n", gpu_sim_cycle, inst.pc, instType, latCycles);
						gGTCLongLatMemInstrCache[(m_sid * GTC_LONG_LAT_MEM_INSTR_CACHE_SIZE) + gGTCLongLatMemInstrCacheIndex] = inst.pc;
						gGTCLongLatMemInstrCacheIndex = (gGTCLongLatMemInstrCacheIndex + 1) % GTC_LONG_LAT_MEM_INSTR_CACHE_SIZE;
					}
				}
				else
				{
					//if this pc was a long lat instr earlier, then remove it from the long lat instr cache
					for (unsigned int i = 0; i < GTC_LONG_LAT_MEM_INSTR_CACHE_SIZE; i++)
					{
						if (inst.pc == gGTCLongLatMemInstrCache[(m_sid * GTC_LONG_LAT_MEM_INSTR_CACHE_SIZE) + i])
						{
							gGTCLongLatMemInstrCache[(m_sid * GTC_LONG_LAT_MEM_INSTR_CACHE_SIZE) + i] = 0xdeaddead;
							//if (m_sid == 0)
								//printf("%llu: Removing inst %u(%s) as a GTC long latency instr with lat = %llu\n", gpu_sim_cycle, inst.pc, instType, latCycles);
						}
					}
				}
			}
		}
		if (inst.space.get_type() == global_space)
		{
			uint smId = this->get_sid();
			std::map<unsigned int, unsigned int>& dynWarpIdToTBIdMap = gDynWarpIdToTBIdMapVec[smId];
			unsigned int tbId = dynWarpIdToTBIdMap[inst.dynamic_warp_id()];
			unsigned index = this->get_sid() * MAX_NUM_TB_PER_SM + tbId;
			if (gTBWarpsExecutingMemInstr)
				gTBWarpsExecutingMemInstr[index]--;
		}
	}
    else if (gSFULongLatInstrCache && ((inst.op == SFU_OP) || (inst.op == ALU_SFU_OP)))
	{
		gSFUTotalLatCycles += latCycles;

		gNumSFUInstrFinished++;

		//unsigned long long avgSFULatCycles = gSFUTotalLatCycles / gNumSFUInstrFinished;

		if (/*(latCycles >= avgSFULatCycles) && */(latCycles >= LONG_LATENCY))
		{
			bool add = true;
			for (unsigned int i = 0; i < SFU_LONG_LAT_INSTR_CACHE_SIZE; i++)
			{
				if (inst.pc == gSFULongLatInstrCache[(m_sid * SFU_LONG_LAT_INSTR_CACHE_SIZE) + i])
				{
					add = false;
					break;
				}
			}
			if (add == true)
			{
				//if (m_sid == 0)
					//printf("%llu: Adding inst %u as a SFU long latency instr with lat = %llu\n", gpu_sim_cycle, inst.pc, latCycles);
				gSFULongLatInstrCache[(m_sid * SFU_LONG_LAT_INSTR_CACHE_SIZE) + gSFULongLatInstrCacheIndex] = inst.pc;
				gSFULongLatInstrCacheIndex = (gSFULongLatInstrCacheIndex + 1) % SFU_LONG_LAT_INSTR_CACHE_SIZE;
			}
		}
		else
		{
			//if this pc was a long lat instr earlier then remove it from the long lat instr cache
			for (unsigned int i = 0; i < SFU_LONG_LAT_INSTR_CACHE_SIZE; i++)
			{
				if (inst.pc == gSFULongLatInstrCache[(m_sid * SFU_LONG_LAT_INSTR_CACHE_SIZE) + i])
				{
					gSFULongLatInstrCache[(m_sid * SFU_LONG_LAT_INSTR_CACHE_SIZE) + i] = 0xdeaddead;
					//if (m_sid == 0)
						//printf("%llu: Removing inst %u as a SFU long latency instr with lat = %llu\n", gpu_sim_cycle, inst.pc, latCycles);
				}
			}
		}
	}

   #if 0
      printf("[warp_inst_complete] uid=%u core=%u warp=%u pc=%#x @ time=%llu issued@%llu\n", 
             inst.get_uid(), m_sid, inst.warp_id(), inst.pc, gpu_tot_sim_cycle + gpu_sim_cycle, inst.get_issue_cycle()); 
   #endif
  if(inst.op_pipe==SP__OP)
	  m_stats->m_num_sp_committed[m_sid]++;
  else if(inst.op_pipe==SFU__OP)
	  m_stats->m_num_sfu_committed[m_sid]++;
  else if(inst.op_pipe==MEM__OP)
	  m_stats->m_num_mem_committed[m_sid]++;

  if(m_config->gpgpu_clock_gated_lanes==false)
	  m_stats->m_num_sim_insn[m_sid] += m_config->warp_size;
  else
	  m_stats->m_num_sim_insn[m_sid] += inst.active_count();

  m_stats->m_num_sim_winsn[m_sid]++;
  m_gpu->gpu_sim_insn += inst.active_count();
  inst.completed(gpu_tot_sim_cycle + gpu_sim_cycle);
}

void shader_core_ctx::writeback()
{

	unsigned max_committed_thread_instructions=m_config->warp_size * (m_config->pipe_widths[EX_WB]); //from the functional units
	m_stats->m_pipeline_duty_cycle[m_sid]=((float)(m_stats->m_num_sim_insn[m_sid]-m_stats->m_last_num_sim_insn[m_sid]))/max_committed_thread_instructions;

    m_stats->m_last_num_sim_insn[m_sid]=m_stats->m_num_sim_insn[m_sid];
    m_stats->m_last_num_sim_winsn[m_sid]=m_stats->m_num_sim_winsn[m_sid];

    warp_inst_t** preg = m_pipeline_reg[EX_WB].get_ready();
    warp_inst_t* pipe_reg = (preg==NULL)? NULL:*preg;
    while( preg and !pipe_reg->empty() ) {
    	/*
    	 * Right now, the writeback stage drains all waiting instructions
    	 * assuming there are enough ports in the register file or the
    	 * conflicts are resolved at issue.
    	 */
    	/*
    	 * The operand collector writeback can generally generate a stall
    	 * However, here, the pipelines should be un-stallable. This is
    	 * guaranteed because this is the first time the writeback function
    	 * is called after the operand collector's step function, which
    	 * resets the allocations. There is one case which could result in
    	 * the writeback function returning false (stall), which is when
    	 * an instruction tries to modify two registers (GPR and predicate)
    	 * To handle this case, we ignore the return value (thus allowing
    	 * no stalling).
    	 */
        m_operand_collector.writeback(*pipe_reg);
        unsigned warp_id = pipe_reg->warp_id();
        m_scoreboard->releaseRegisters( pipe_reg );
        m_warp[warp_id].dec_inst_in_pipeline();
        warp_inst_complete(*pipe_reg);
        m_gpu->gpu_sim_insn_last_update_sid = m_sid;
        m_gpu->gpu_sim_insn_last_update = gpu_sim_cycle;
        m_last_inst_gpu_sim_cycle = gpu_sim_cycle;
        m_last_inst_gpu_tot_sim_cycle = gpu_tot_sim_cycle;
        pipe_reg->clear();
        preg = m_pipeline_reg[EX_WB].get_ready();
        pipe_reg = (preg==NULL)? NULL:*preg;
    }
}

bool ldst_unit::shared_cycle( warp_inst_t &inst, mem_stage_stall_type &rc_fail, mem_stage_access_type &fail_type)
{
   if( inst.space.get_type() != shared_space )
       return true;

   if(inst.has_dispatch_delay()){
	   m_stats->gpgpu_n_shmem_bank_access[m_sid]++;
   }

   bool stall = inst.dispatch_delay();
   if( stall ) {
       fail_type = S_MEM;
       rc_fail = BK_CONF;
   } else 
       rc_fail = NO_RC_FAIL;
   return !stall; 
}

mem_stage_stall_type
ldst_unit::process_cache_access( cache_t* cache,
                                 new_addr_type address,
                                 warp_inst_t &inst,
                                 std::list<cache_event>& events,
                                 mem_fetch *mf,
                                 enum cache_request_status status )
{
	gDbgCnt2++;
	// if ((gDbgCnt2 % 100000) == 0)
	   	// printf("%llu: gDbgCnt2 = %u\n", gpu_sim_cycle, gDbgCnt2);
    mem_stage_stall_type result = NO_RC_FAIL;
    bool write_sent = was_write_sent(events);
    bool read_sent = was_read_sent(events);
    if( write_sent ) 
        m_core->inc_store_req( inst.warp_id() );
    if ( status == HIT ) {
		gDbgCnt2_1++;
		// if ((gDbgCnt2_1 % 100000) == 0)
	   		// printf("%llu: gDbgCnt2_1 = %u\n", gpu_sim_cycle, gDbgCnt2_1);

        assert( !read_sent );
        inst.accessq_pop_back();
        if ( inst.is_load() ) {
            for ( unsigned r=0; r < 4; r++)
                if (inst.out[r] > 0)
                    m_pending_writes[inst.warp_id()][inst.out[r]]--; 
        }
        if( !write_sent ) 
            delete mf;
    } else if ( status == RESERVATION_FAIL ) {
        result = COAL_STALL;
		gCoalStall2++;
		// if ((gCoalStall2 % 10000) == 0)
	    	// printf("%llu: gCoalStall2 = %u\n", gpu_sim_cycle, gCoalStall2);
        assert( !read_sent );
        assert( !write_sent );
        delete mf;
    } else {
		gDbgCnt2_2++;
		// if ((gDbgCnt2_2 % 100000) == 0)
	   		// printf("%llu: gDbgCnt2_2 = %u\n", gpu_sim_cycle, gDbgCnt2_2);

        assert( status == MISS || status == HIT_RESERVED );
        //inst.clear_active( access.get_warp_mask() ); // threads in mf writeback when mf returns
        inst.accessq_pop_back();
    }
    if( !inst.accessq_empty() )
        result = BK_CONF;
    return result;
}

mem_stage_stall_type ldst_unit::process_memory_access_queue( cache_t *cache, warp_inst_t &inst )
{
   	gDbgCnt1_6++;
	// if ((gDbgCnt1_6 % 100000) == 0)
		// printf("%llu: gDbgCnt1_6 = %u\n", gpu_sim_cycle, gDbgCnt1_6);

    mem_stage_stall_type result = NO_RC_FAIL;
    if( inst.accessq_empty() )
        return result;

   	gDbgCnt1_7++;
	// if ((gDbgCnt1_7 % 100000) == 0)
		// printf("%llu: gDbgCnt1_7 = %u\n", gpu_sim_cycle, gDbgCnt1_7);

    if( !cache->data_port_free() ) 
        return DATA_PORT_STALL; 

   	gDbgCnt1_8++;
	// if ((gDbgCnt1_8 % 100000) == 0)
		// printf("%llu: gDbgCnt1_8 = %u\n", gpu_sim_cycle, gDbgCnt1_8);

    //const mem_access_t &access = inst.accessq_back();
    mem_fetch *mf = m_mf_allocator->alloc(inst,inst.accessq_back());
    std::list<cache_event> events;
    enum cache_request_status status = cache->access(mf->get_addr(),mf,gpu_sim_cycle+gpu_tot_sim_cycle,events);
    return process_cache_access( cache, mf->get_addr(), inst, events, mf, status );
}

bool ldst_unit::constant_cycle( warp_inst_t &inst, mem_stage_stall_type &rc_fail, mem_stage_access_type &fail_type)
{
   if( inst.empty() || ((inst.space.get_type() != const_space) && (inst.space.get_type() != param_space_kernel)) )
       return true;
   if( inst.active_count() == 0 ) 
       return true;

   	gDbgCnt1_5++;
	// if ((gDbgCnt1_5 % 100000) == 0)
		// printf("%llu: gDbgCnt1_5 = %u\n", gpu_sim_cycle, gDbgCnt1_5);

   mem_stage_stall_type fail = process_memory_access_queue(m_L1C,inst);
   if (fail != NO_RC_FAIL){ 
      rc_fail = fail; //keep other fails if this didn't fail.
      fail_type = C_MEM;
      if (rc_fail == BK_CONF or rc_fail == COAL_STALL) {
         m_stats->gpgpu_n_cmem_portconflict++; //coal stalls aren't really a bank conflict, but this maintains previous behavior.
      }
   }
   return inst.accessq_empty(); //done if empty.
}

bool ldst_unit::texture_cycle( warp_inst_t &inst, mem_stage_stall_type &rc_fail, mem_stage_access_type &fail_type)
{
   if( inst.empty() || inst.space.get_type() != tex_space )
       return true;
   if( inst.active_count() == 0 ) 
       return true;

   	gDbgCnt1_4++;
	// if ((gDbgCnt1_4 % 100000) == 0)
		// printf("%llu: gDbgCnt1_4 = %u\n", gpu_sim_cycle, gDbgCnt1_4);

   mem_stage_stall_type fail = process_memory_access_queue(m_L1T,inst);
   if (fail != NO_RC_FAIL){ 
      rc_fail = fail; //keep other fails if this didn't fail.
      fail_type = T_MEM;
   }
   return inst.accessq_empty(); //done if empty.
}

bool ldst_unit::memory_cycle( warp_inst_t &inst, mem_stage_stall_type &stall_reason, mem_stage_access_type &access_type )
{
   	gDbgCnt1++;
	// if ((gDbgCnt1 % 100000) == 0)
	   	// printf("%llu: gDbgCnt1 = %u\n", gpu_sim_cycle, gDbgCnt1);

   if( inst.empty() || 
       ((inst.space.get_type() != global_space) &&
        (inst.space.get_type() != local_space) &&
        (inst.space.get_type() != param_space_local)) ) 
       return true;

   	gDbgCnt1_1++;
	// if ((gDbgCnt1_1 % 100000) == 0)
	   	// printf("%llu: gDbgCnt1_1 = %u\n", gpu_sim_cycle, gDbgCnt1_1);

   if( inst.active_count() == 0 ) 
       return true;

   	gDbgCnt1_2++;
	// if ((gDbgCnt1_2 % 100000) == 0)
	   	// printf("%llu: gDbgCnt1_2 = %u\n", gpu_sim_cycle, gDbgCnt1_2);

   assert( !inst.accessq_empty() );
   mem_stage_stall_type stall_cond = NO_RC_FAIL;
   const mem_access_t &access = inst.accessq_back();

   bool bypassL1D = false; 
   if ( CACHE_GLOBAL == inst.cache_op || (m_L1D == NULL) ) {
       bypassL1D = true; 
   } else if (inst.space.is_global()) { // global memory access 
       // skip L1 cache if the option is enabled
       if (m_core->get_config()->gmem_skip_L1D) 
           bypassL1D = true; 
   }

   if( bypassL1D ) {
       // bypass L1 cache
       unsigned control_size = inst.is_store() ? WRITE_PACKET_SIZE : READ_PACKET_SIZE;
       unsigned size = access.get_size() + control_size;
       if( m_icnt->full(size, inst.is_store() || inst.isatomic()) ) {
           stall_cond = ICNT_RC_FAIL;
       } else {
           mem_fetch *mf = m_mf_allocator->alloc(inst,access);
           m_icnt->push(mf);
           inst.accessq_pop_back();
           //inst.clear_active( access.get_warp_mask() );
           if( inst.is_load() ) { 
              for( unsigned r=0; r < 4; r++) 
                  if(inst.out[r] > 0) 
                      assert( m_pending_writes[inst.warp_id()][inst.out[r]] > 0 );
           } else if( inst.is_store() ) 
              m_core->inc_store_req( inst.warp_id() );
       }
   } else {
   		gDbgCnt1_3++;
		// if ((gDbgCnt1_3 % 100000) == 0)
	   		// printf("%llu: gDbgCnt1_3 = %u\n", gpu_sim_cycle, gDbgCnt1_3);

       assert( CACHE_UNDEFINED != inst.cache_op );
       stall_cond = process_memory_access_queue(m_L1D,inst);
   }
   if( !inst.accessq_empty() ) 
       stall_cond = COAL_STALL;
		gCoalStall1++;
		// if ((gCoalStall1 % 10000) == 0)
	    	// printf("%llu: gCoalStall1 = %u\n", gpu_sim_cycle, gCoalStall1);
   if (stall_cond != NO_RC_FAIL) {
      stall_reason = stall_cond;
      bool iswrite = inst.is_store();
      if (inst.space.is_local()) 
         access_type = (iswrite)?L_MEM_ST:L_MEM_LD;
      else 
         access_type = (iswrite)?G_MEM_ST:G_MEM_LD;
   }
   return inst.accessq_empty(); 
}


bool ldst_unit::response_buffer_full() const
{
    return m_response_fifo.size() >= m_config->ldst_unit_response_queue_size;
}

void ldst_unit::fill( mem_fetch *mf )
{
    mf->set_status(IN_SHADER_LDST_RESPONSE_FIFO,gpu_sim_cycle+gpu_tot_sim_cycle);
    m_response_fifo.push_back(mf);
}

void ldst_unit::flush(){
	// Flush L1D cache
	m_L1D->flush();
}

simd_function_unit::simd_function_unit( const shader_core_config *config )
{ 
    m_config=config;
    m_dispatch_reg = new warp_inst_t(config); 
}


sfu:: sfu(  register_set* result_port, const shader_core_config *config,shader_core_ctx *core  )
    : pipelined_simd_unit(result_port,config,config->max_sfu_latency,core)
{ 
    m_name = "SFU"; 
}

void sfu::issue( register_set& source_reg )
{
    warp_inst_t** ready_reg = source_reg.get_ready();
	//m_core->incexecstat((*ready_reg));

	(*ready_reg)->op_pipe=SFU__OP;
	m_core->incsfu_stat(m_core->get_config()->warp_size,(*ready_reg)->latency);
	pipelined_simd_unit::issue(source_reg);
}

void ldst_unit::active_lanes_in_pipeline(){
	unsigned active_count=pipelined_simd_unit::get_active_lanes_in_pipeline();
	assert(active_count<=m_core->get_config()->warp_size);
	m_core->incfumemactivelanes_stat(active_count);
}
void sp_unit::active_lanes_in_pipeline(){
	unsigned active_count=pipelined_simd_unit::get_active_lanes_in_pipeline();
	assert(active_count<=m_core->get_config()->warp_size);
	m_core->incspactivelanes_stat(active_count);
	m_core->incfuactivelanes_stat(active_count);
	m_core->incfumemactivelanes_stat(active_count);
}

void sfu::active_lanes_in_pipeline(){
	unsigned active_count=pipelined_simd_unit::get_active_lanes_in_pipeline();
	assert(active_count<=m_core->get_config()->warp_size);
	m_core->incsfuactivelanes_stat(active_count);
	m_core->incfuactivelanes_stat(active_count);
	m_core->incfumemactivelanes_stat(active_count);
}

sp_unit::sp_unit( register_set* result_port, const shader_core_config *config,shader_core_ctx *core)
    : pipelined_simd_unit(result_port,config,config->max_sp_latency,core)
{ 
    m_name = "SP "; 
}

void sp_unit :: issue(register_set& source_reg)
{
    warp_inst_t** ready_reg = source_reg.get_ready();
	//m_core->incexecstat((*ready_reg));
	(*ready_reg)->op_pipe=SP__OP;
	m_core->incsp_stat(m_core->get_config()->warp_size,(*ready_reg)->latency);
	pipelined_simd_unit::issue(source_reg);
}


pipelined_simd_unit::pipelined_simd_unit( register_set* result_port, const shader_core_config *config, unsigned max_latency,shader_core_ctx *core )
    : simd_function_unit(config) 
{
    m_result_port = result_port;
    m_pipeline_depth = max_latency;
    m_pipeline_reg = new warp_inst_t*[m_pipeline_depth];
    for( unsigned i=0; i < m_pipeline_depth; i++ ) 
	m_pipeline_reg[i] = new warp_inst_t( config );
    m_core=core;
}


void pipelined_simd_unit::issue( register_set& source_reg )
{
    //move_warp(m_dispatch_reg,source_reg);
    warp_inst_t** ready_reg = source_reg.get_ready();
	m_core->incexecstat((*ready_reg));
	//source_reg.move_out_to(m_dispatch_reg);
	simd_function_unit::issue(source_reg);
}

/*
    virtual void issue( register_set& source_reg )
    {
        //move_warp(m_dispatch_reg,source_reg);
        //source_reg.move_out_to(m_dispatch_reg);
        simd_function_unit::issue(source_reg);
    }
*/

void ldst_unit::init( mem_fetch_interface *icnt,
                      shader_core_mem_fetch_allocator *mf_allocator,
                      shader_core_ctx *core, 
                      opndcoll_rfu_t *operand_collector,
                      Scoreboard *scoreboard,
                      const shader_core_config *config,
                      const memory_config *mem_config,  
                      shader_core_stats *stats,
                      unsigned sid,
                      unsigned tpc )
{
    m_memory_config = mem_config;
    m_icnt = icnt;
    m_mf_allocator=mf_allocator;
    m_core = core;
    m_operand_collector = operand_collector;
    m_scoreboard = scoreboard;
    m_stats = stats;
    m_sid = sid;
    m_tpc = tpc;
    #define STRSIZE 1024
    char L1T_name[STRSIZE];
    char L1C_name[STRSIZE];
    snprintf(L1T_name, STRSIZE, "L1T_%03d", m_sid);
    snprintf(L1C_name, STRSIZE, "L1C_%03d", m_sid);
    m_L1T = new tex_cache(L1T_name,m_config->m_L1T_config,m_sid,get_shader_texture_cache_id(),icnt,IN_L1T_MISS_QUEUE,IN_SHADER_L1T_ROB);
    m_L1C = new read_only_cache(L1C_name,m_config->m_L1C_config,m_sid,get_shader_constant_cache_id(),icnt,IN_L1C_MISS_QUEUE);
    m_L1D = NULL;
    m_mem_rc = NO_RC_FAIL;
    m_num_writeback_clients=5; // = shared memory, global/local (uncached), L1D, L1T, L1C
    m_writeback_arb = 0;
    m_next_global=NULL;
    m_last_inst_gpu_sim_cycle=0;
    m_last_inst_gpu_tot_sim_cycle=0;
}


ldst_unit::ldst_unit( mem_fetch_interface *icnt,
                      shader_core_mem_fetch_allocator *mf_allocator,
                      shader_core_ctx *core, 
                      opndcoll_rfu_t *operand_collector,
                      Scoreboard *scoreboard,
                      const shader_core_config *config,
                      const memory_config *mem_config,  
                      shader_core_stats *stats,
                      unsigned sid,
                      unsigned tpc ) : pipelined_simd_unit(NULL,config,3,core), m_next_wb(config)
{
    init( icnt,
          mf_allocator,
          core, 
          operand_collector,
          scoreboard,
          config, 
          mem_config,  
          stats, 
          sid,
          tpc );
    if( !m_config->m_L1D_config.disabled() ) {
        char L1D_name[STRSIZE];
        snprintf(L1D_name, STRSIZE, "L1D_%03d", m_sid);
        m_L1D = new l1_cache( L1D_name,
                              m_config->m_L1D_config,
                              m_sid,
                              get_shader_normal_cache_id(),
                              m_icnt,
                              m_mf_allocator,
                              IN_L1D_MISS_QUEUE );
    }
}

ldst_unit::ldst_unit( mem_fetch_interface *icnt,
                      shader_core_mem_fetch_allocator *mf_allocator,
                      shader_core_ctx *core, 
                      opndcoll_rfu_t *operand_collector,
                      Scoreboard *scoreboard,
                      const shader_core_config *config,
                      const memory_config *mem_config,  
                      shader_core_stats *stats,
                      unsigned sid,
                      unsigned tpc,
                      l1_cache* new_l1d_cache )
    : pipelined_simd_unit(NULL,config,3,core), m_L1D(new_l1d_cache), m_next_wb(config)
{
    init( icnt,
          mf_allocator,
          core, 
          operand_collector,
          scoreboard,
          config, 
          mem_config,  
          stats, 
          sid,
          tpc );
}

void ldst_unit:: issue( register_set &reg_set )
{
	warp_inst_t* inst = *(reg_set.get_ready());

   // record how many pending register writes/memory accesses there are for this instruction
   assert(inst->empty() == false);
   if (inst->is_load() and inst->space.get_type() != shared_space) {
      unsigned warp_id = inst->warp_id();
      unsigned n_accesses = inst->accessq_count();
      for (unsigned r = 0; r < 4; r++) {
         unsigned reg_id = inst->out[r];
         if (reg_id > 0) {
            m_pending_writes[warp_id][reg_id] += n_accesses;
         }
      }
   }


	inst->op_pipe=MEM__OP;
	// stat collection
	m_core->mem_instruction_stats(*inst);
	m_core->incmem_stat(m_core->get_config()->warp_size,1);
	pipelined_simd_unit::issue(reg_set);
}

void ldst_unit::writeback()
{
    // process next instruction that is going to writeback
    if( !m_next_wb.empty() ) {
        if( m_operand_collector->writeback(m_next_wb) ) {
            bool insn_completed = false; 
            for( unsigned r=0; r < 4; r++ ) {
                if( m_next_wb.out[r] > 0 ) {
                    if( m_next_wb.space.get_type() != shared_space ) {
                        assert( m_pending_writes[m_next_wb.warp_id()][m_next_wb.out[r]] > 0 );
                        unsigned still_pending = --m_pending_writes[m_next_wb.warp_id()][m_next_wb.out[r]];
                        if( !still_pending ) {
                            m_pending_writes[m_next_wb.warp_id()].erase(m_next_wb.out[r]);
                            m_scoreboard->releaseRegister( m_next_wb.warp_id(), m_next_wb.out[r] );
                            insn_completed = true; 
                        }
                    } else { // shared 
                        m_scoreboard->releaseRegister( m_next_wb.warp_id(), m_next_wb.out[r] );
                        insn_completed = true; 
                    }
                }
            }
            if( insn_completed ) {
                m_core->warp_inst_complete(m_next_wb);
            }
            m_next_wb.clear();
            m_last_inst_gpu_sim_cycle = gpu_sim_cycle;
            m_last_inst_gpu_tot_sim_cycle = gpu_tot_sim_cycle;
        }
    }

    unsigned serviced_client = -1; 
    for( unsigned c = 0; m_next_wb.empty() && (c < m_num_writeback_clients); c++ ) {
        unsigned next_client = (c+m_writeback_arb)%m_num_writeback_clients;
        switch( next_client ) {
        case 0: // shared memory 
            if( !m_pipeline_reg[0]->empty() ) {
                m_next_wb = *m_pipeline_reg[0];
                if(m_next_wb.isatomic()) {
                    m_next_wb.do_atomic();
                    m_core->decrement_atomic_count(m_next_wb.warp_id(), m_next_wb.active_count());
                }
                m_core->dec_inst_in_pipeline(m_pipeline_reg[0]->warp_id());
                m_pipeline_reg[0]->clear();
                serviced_client = next_client; 
            }
            break;
        case 1: // texture response
            if( m_L1T->access_ready() ) {
                mem_fetch *mf = m_L1T->next_access();
                m_next_wb = mf->get_inst();
                delete mf;
                serviced_client = next_client; 
            }
            break;
        case 2: // const cache response
            if( m_L1C->access_ready() ) {
                mem_fetch *mf = m_L1C->next_access();
                m_next_wb = mf->get_inst();
                delete mf;
                serviced_client = next_client; 
            }
            break;
        case 3: // global/local
            if( m_next_global ) {
                m_next_wb = m_next_global->get_inst();
                if( m_next_global->isatomic() ) 
                    m_core->decrement_atomic_count(m_next_global->get_wid(),m_next_global->get_access_warp_mask().count());
                delete m_next_global;
                m_next_global = NULL;
                serviced_client = next_client; 
            }
            break;
        case 4: 
            if( m_L1D && m_L1D->access_ready() ) {
                mem_fetch *mf = m_L1D->next_access();
                m_next_wb = mf->get_inst();
                delete mf;
                serviced_client = next_client; 
            }
            break;
        default: abort();
        }
    }
    // update arbitration priority only if: 
    // 1. the writeback buffer was available 
    // 2. a client was serviced 
    if (serviced_client != (unsigned)-1) {
        m_writeback_arb = (serviced_client + 1) % m_num_writeback_clients; 
    }
}

unsigned ldst_unit::clock_multiplier() const
{ 
    return m_config->mem_warp_parts; 
}
/*
void ldst_unit::issue( register_set &reg_set )
{
	warp_inst_t* inst = *(reg_set.get_ready());
   // stat collection
   m_core->mem_instruction_stats(*inst); 

   // record how many pending register writes/memory accesses there are for this instruction 
   assert(inst->empty() == false); 
   if (inst->is_load() and inst->space.get_type() != shared_space) {
      unsigned warp_id = inst->warp_id(); 
      unsigned n_accesses = inst->accessq_count(); 
      for (unsigned r = 0; r < 4; r++) {
         unsigned reg_id = inst->out[r]; 
         if (reg_id > 0) {
            m_pending_writes[warp_id][reg_id] += n_accesses; 
         }
      }
   }

   pipelined_simd_unit::issue(reg_set);
}
*/
void ldst_unit::cycle()
{
   writeback();
   m_operand_collector->step();
   for( unsigned stage=0; (stage+1)<m_pipeline_depth; stage++ ) 
       if( m_pipeline_reg[stage]->empty() && !m_pipeline_reg[stage+1]->empty() )
            move_warp(m_pipeline_reg[stage], m_pipeline_reg[stage+1]);

   if( !m_response_fifo.empty() ) {
       mem_fetch *mf = m_response_fifo.front();
       if (mf->istexture()) {
           if (m_L1T->fill_port_free()) {
               m_L1T->fill(mf,gpu_sim_cycle+gpu_tot_sim_cycle);
               m_response_fifo.pop_front(); 
           }
       } else if (mf->isconst())  {
           if (m_L1C->fill_port_free()) {
               mf->set_status(IN_SHADER_FETCHED,gpu_sim_cycle+gpu_tot_sim_cycle);
               m_L1C->fill(mf,gpu_sim_cycle+gpu_tot_sim_cycle);
               m_response_fifo.pop_front(); 
           }
       } else {
    	   if( mf->get_type() == WRITE_ACK || ( m_config->gpgpu_perfect_mem && mf->get_is_write() )) {
               m_core->store_ack(mf);
               m_response_fifo.pop_front();
               delete mf;
           } else {
               assert( !mf->get_is_write() ); // L1 cache is write evict, allocate line on load miss only

               bool bypassL1D = false; 
               if ( CACHE_GLOBAL == mf->get_inst().cache_op || (m_L1D == NULL) ) {
                   bypassL1D = true; 
               } else if (mf->get_access_type() == GLOBAL_ACC_R || mf->get_access_type() == GLOBAL_ACC_W) { // global memory access 
                   if (m_core->get_config()->gmem_skip_L1D)
                       bypassL1D = true; 
               }
               if( bypassL1D ) {
                   if ( m_next_global == NULL ) {
                       mf->set_status(IN_SHADER_FETCHED,gpu_sim_cycle+gpu_tot_sim_cycle);
                       m_response_fifo.pop_front();
                       m_next_global = mf;
                   }
               } else {
                   if (m_L1D->fill_port_free()) {
                       m_L1D->fill(mf,gpu_sim_cycle+gpu_tot_sim_cycle);
                       m_response_fifo.pop_front();
                   }
               }
           }
       }
   }

   m_L1T->cycle();
   m_L1C->cycle();
   if( m_L1D ) m_L1D->cycle();

   warp_inst_t &pipe_reg = *m_dispatch_reg;
   enum mem_stage_stall_type rc_fail = NO_RC_FAIL;
   mem_stage_access_type type;
   bool done = true;
   done &= shared_cycle(pipe_reg, rc_fail, type);
   done &= constant_cycle(pipe_reg, rc_fail, type);
   done &= texture_cycle(pipe_reg, rc_fail, type);
   done &= memory_cycle(pipe_reg, rc_fail, type);
   m_mem_rc = rc_fail;

   if (!done) { // log stall types and return
      assert(rc_fail != NO_RC_FAIL);
      m_stats->gpgpu_n_stall_shd_mem++;
      m_stats->gpu_stall_shd_mem_breakdown[type][rc_fail]++;
      return;
   }

   if( !pipe_reg.empty() ) {
       unsigned warp_id = pipe_reg.warp_id();
       if( pipe_reg.is_load() ) {
           if( pipe_reg.space.get_type() == shared_space ) {
               if( m_pipeline_reg[2]->empty() ) {
                   // new shared memory request
                   move_warp(m_pipeline_reg[2],m_dispatch_reg);
                   m_dispatch_reg->clear();
               }
           } else {
               //if( pipe_reg.active_count() > 0 ) {
               //    if( !m_operand_collector->writeback(pipe_reg) ) 
               //        return;
               //} 

               bool pending_requests=false;
               for( unsigned r=0; r<4; r++ ) {
                   unsigned reg_id = pipe_reg.out[r];
                   if( reg_id > 0 ) {
                       if( m_pending_writes[warp_id].find(reg_id) != m_pending_writes[warp_id].end() ) {
                           if ( m_pending_writes[warp_id][reg_id] > 0 ) {
                               pending_requests=true;
                               break;
                           } else {
                               // this instruction is done already
                               m_pending_writes[warp_id].erase(reg_id); 
                           }
                       }
                   }
               }
               if( !pending_requests ) {
                   m_core->warp_inst_complete(*m_dispatch_reg);
                   m_scoreboard->releaseRegisters(m_dispatch_reg);
               }
               m_core->dec_inst_in_pipeline(warp_id);
               m_dispatch_reg->clear();
           }
       } else {
           // stores exit pipeline here
           m_core->dec_inst_in_pipeline(warp_id);
           m_core->warp_inst_complete(*m_dispatch_reg);
           m_dispatch_reg->clear();
       }
   }
}

unsigned long long gEarliestSMFinishTime = 0xFFFFFFFF;
unsigned long long gLatestSMFinishTime = 0;
extern unsigned int gHighPrioMemReq;
extern unsigned int gLowPrioMemReq;

void printLastMemInstrInfo(const char* kernelName)
{
	std::map<std::string, unsigned int> lastMemInstrCntMap;
	for (std::map<unsigned int, std::string>::iterator iter = gLastMemInstrMap.begin(); iter != gLastMemInstrMap.end(); iter++)
	{
		unsigned int index = iter->first;
		unsigned int smId = index & 0xF;
		unsigned int dynWarpId = index >> 4;
		std::string memInstrStr = iter->second;

		if (lastMemInstrCntMap.find(memInstrStr) == lastMemInstrCntMap.end())
		{
			lastMemInstrCntMap[memInstrStr] = 1;
			printf("kernel %s sm %u dyn_warp %u last mem instr %s\n", kernelName, smId, dynWarpId, memInstrStr.c_str());
		}
		else	
			lastMemInstrCntMap[memInstrStr]++;
	}
    for (std::map<std::string, unsigned int>::iterator iter = lastMemInstrCntMap.begin(); iter != lastMemInstrCntMap.end(); iter++)
		printf("last mem inst %s executed by %u warps\n", (iter->first).c_str(), iter->second);

	gLastMemInstrMap.clear();
}

void shader_core_ctx::register_cta_thread_exit( unsigned cta_num )
{
   assert( m_cta_status[cta_num] > 0 );
   m_cta_status[cta_num]--;
   if (!m_cta_status[cta_num]) {
      m_n_active_cta--;
      m_barriers.deallocate_barrier(cta_num);
      shader_CTA_count_unlog(m_sid, 1);

	if (rl_scheduler::gTBWithWarpsFinished && (rl_scheduler::gTBWithWarpsFinished[m_sid] == cta_num))
		rl_scheduler::gTBWithWarpsFinished[m_sid] = 0xdeaddead;

	if (rl_scheduler::gTBWithWarpsAtBarrier && (rl_scheduler::gTBWithWarpsAtBarrier[m_sid] == cta_num))
		rl_scheduler::gTBWithWarpsAtBarrier[m_sid] = 0xdeaddead;

	if (gNumWarpsAtFinishMapVec.size() != 0)
	{
		std::map<unsigned int, unsigned int>& numWarpsAtFinishMap = gNumWarpsAtFinishMapVec.at(m_sid);
		numWarpsAtFinishMap[cta_num] = 0;
	}

	if (gNumWarpsAtBarrierMapVec.size() != 0)
	{
		std::map<unsigned int, unsigned int>& numWarpsAtBarrierMap = gNumWarpsAtBarrierMapVec.at(m_sid);
		numWarpsAtBarrierMap[cta_num] = 0;
	}

	rl_scheduler::gNumFinishedTBs++;

	unsigned index = m_sid * MAX_NUM_TB_PER_SM + cta_num;
	unsigned int numInstrExeced = 0;

	if (gTBProgressArray)
	{
		bool printFlag = true;
		if (printFlag && (rl_scheduler::gNumFinishedTBs == 1))
		{
			unsigned int maxProgress = 0;
			for (unsigned int i = 0; i < MAX_NUM_TB_PER_SM; i++)
			{
				unsigned idx = m_sid * MAX_NUM_TB_PER_SM + i;
				unsigned int tbProgress = gTBProgressArray[idx];
				if ((tbProgress != 0) && (tbProgress != 0xdeaddead))
					if (maxProgress < tbProgress)
						maxProgress = tbProgress;
			}
			for (unsigned int i = 0; i < MAX_NUM_TB_PER_SM; i++)
			{
				unsigned idx = m_sid * MAX_NUM_TB_PER_SM + i;
				unsigned int tbProgress = gTBProgressArray[idx];
				if ((tbProgress != 0) && (tbProgress != 0xdeaddead))
					printf("%llu: tb %u progress %u(%f)\n", gpu_sim_cycle, i, tbProgress/32, (float)tbProgress/(float)maxProgress);
			}
		}

		rl_scheduler::gNumInstrsExecedByFinishedTBs += gTBProgressArray[index];
		numInstrExeced = gTBProgressArray[index];

		if (gTBProgressArray[index] > gMaxNumInstrsExecedByTB)
			gMaxNumInstrsExecedByTB = gTBProgressArray[index];

		if (gTBProgressArray[index] < gMinNumInstrsExecedByTB)
			gMinNumInstrsExecedByTB = gTBProgressArray[index];

		gTBProgressArray[index] = 0;
	}

	if (gTBPhaseArray)
	{
		bool printFlag = true;
		if (printFlag && (rl_scheduler::gNumFinishedTBs == 1))
		{
			for (unsigned int i = 0; i < MAX_NUM_TB_PER_SM; i++)
			{
				unsigned idx = m_sid * MAX_NUM_TB_PER_SM + i;
				unsigned int tbPhase = gTBPhaseArray[idx];
				if (tbPhase != 0xdeaddead)
					printf("%llu: tb %u phase %u\n", gpu_sim_cycle, i, tbPhase);
			}
		}

		gTBPhaseArray[index] = 0;
	}

	if (rl_scheduler::gTBNumSpInstrsArray)
		rl_scheduler::gTBNumSpInstrsArray[index] = 0;
	if (rl_scheduler::gTBNumSfuInstrsArray)
		rl_scheduler::gTBNumSfuInstrsArray[index] = 0;
	if (rl_scheduler::gTBNumMemInstrsArray)
		rl_scheduler::gTBNumMemInstrsArray[index] = 0;

	if (gSelectedTB && (gSelectedTB[m_sid] == cta_num))
		gSelectedTB[m_sid] = 0xdeaddead;

	  scheduler_unit* sched = schedulers[0];
	  assert(sched);
	  if (gFBISched)
	  	((fbi_scheduler*)sched)->mRemoveBlockFromFinishList(cta_num);

      printf("GPGPU-Sim uArch: Shader %d finished CTA #%d (%lld,%lld), executed %u instructions, %u CTAs running, total %u finished by all SMs\n", m_sid, cta_num, gpu_sim_cycle, gpu_tot_sim_cycle, numInstrExeced/32, m_n_active_cta, rl_scheduler::gNumFinishedTBs );
      if( m_n_active_cta == 0 ) {
          assert( m_kernel != NULL );
          m_kernel->dec_running();
          printf("GPGPU-Sim uArch: Shader %u empty (release kernel %u \'%s\').\n", m_sid, m_kernel->get_uid(),
                 m_kernel->name().c_str() );

	  	  if (gFBISched)
	  	  	 ((fbi_scheduler*)sched)->mClear();

		  if (gEarliestSMFinishTime > gpu_sim_cycle)
		  	gEarliestSMFinishTime = gpu_sim_cycle;
		  if (gLatestSMFinishTime < gpu_sim_cycle)
		  	gLatestSMFinishTime = gpu_sim_cycle;

          if( m_kernel->no_more_ctas_to_run() ) {
              if( !m_kernel->running() ) {
                  printf("GPGPU-Sim uArch: GPU detected kernel \'%s\' finished on shader %u.\n", m_kernel->name().c_str(), m_sid );
				  if (sched->isRR_GTOSched())
				  	printf("RR_GTO_SCHED: RR mode cnt = %u, GTO mode cnt = %u\n", gRRModeCnt, gGTOModeCnt);

				  if (rl_scheduler::gNumFinishedTBs != 0)
				  {
				  	unsigned int avgNumInstrs = rl_scheduler::gNumInstrsExecedByFinishedTBs/rl_scheduler::gNumFinishedTBs;
				  	printf("avg num instrs by a tb = %u, max = %u, min = %u, max/avg = %f, max/min = %f, avg/min = %f\n", 
				  	     	avgNumInstrs / 32, gMaxNumInstrsExecedByTB / 32, gMinNumInstrsExecedByTB / 32, 
						 	(float)gMaxNumInstrsExecedByTB / (float)avgNumInstrs,
						 	(float)gMaxNumInstrsExecedByTB / (float)gMinNumInstrsExecedByTB,
						 	(float)avgNumInstrs / (float)gMinNumInstrsExecedByTB);
				  }

		  		  printLastMemInstrInfo(m_kernel->name().c_str());
				  printf("gNumNonLastMemInstrIssuedAfterLastMemInstrIssuedByAnyWarp = %u\n", gNumNonLastMemInstrIssuedAfterLastMemInstrIssuedByAnyWarp);
				  printf("gEarliestSMFinishTime = %llu, gLatestSMFinishTime = %llu, %f\n", gEarliestSMFinishTime, gLatestSMFinishTime, ((float)gLatestSMFinishTime/(float)gEarliestSMFinishTime));
				  gEarliestSMFinishTime = 0xFFFFFFFF;
				  gLatestSMFinishTime = 0;
			 	  gPrintNoMoreCTAsMsg = true;
			 	  gLastMemInstrIssuedByAnyWarp = false;
			 	  gNumNonLastMemInstrIssuedAfterLastMemInstrIssuedByAnyWarp = 0;

				  printf("high prio mem req = %u, low prio mem req = %u\n", gHighPrioMemReq, gLowPrioMemReq);
				  gHighPrioMemReq = 0;
				  gLowPrioMemReq = 0;

				  if (gNumTBsGoingToBarrierList)
				  {
				  	for (unsigned int i = 1; i <= MAX_NUM_TB_PER_SM; i++)
				  	{
						if (gNumTBsGoingToBarrierList[i] > 0)
				  			printf("Num of TBs going from position %u to BARRIER list %u\n", i, gNumTBsGoingToBarrierList[i]);
						gNumTBsGoingToBarrierList[i] = 0;
				  	}
				  }

				  if (gNumTBsGoingToFinishList)
				  {
				  	for (unsigned int i = 1; i <= MAX_NUM_TB_PER_SM; i++)
				  	{
						if (gNumTBsGoingToFinishList[i] > 0)
				  			printf("Num of TBs going from position %u to FINISH list %u\n", i, gNumTBsGoingToFinishList[i]);
						gNumTBsGoingToFinishList[i] = 0;
				  	}
				  }
				  printf("Num of calls to sort inorder tbs %u\n", gNumCallsToSortInorderTBs);
				  printf("Num of calls to sort finish tbs %u\n", gNumCallsToSortFinishTBs);
				  printf("Num of calls to sort warps %u\n", gNumCallsToSortWarps);
				  printf("Num of calls to sort finish warps %u\n", gNumCallsToSortFinishWarps);
				  printf("Num of calls to sort barrier warps %u\n", gNumCallsToSortBarrierWarps);
				  printf("Num of calls to sort inorder warps %u\n", gNumCallsToSortInorderWarps);
				  gNumCallsToSortFinishTBs = 0;
				  gNumCallsToSortWarps = 0;
				  gNumCallsToSortFinishWarps = 0;
				  gNumCallsToSortBarrierWarps = 0;
				  gNumCallsToSortInorderWarps = 0;


                  m_gpu->set_kernel_done( m_kernel );
				  scheduler_unit* sched = schedulers[0];
				  sched->printQvalueUpdateCounts();
	  			  for (unsigned int j = 0; j < schedulers.size(); j++)
				  {
					  scheduler_unit* sched = schedulers[j];
					  sched->clear();
				  }
				  rl_scheduler::gNumFinishedTBs = 0;
				  rl_scheduler::gNumInstrsExecedByFinishedTBs = 0;
              }
          }
          m_kernel=NULL;
          fflush(stdout);
      }
   }
}

void gpgpu_sim::shader_print_runtime_stat( FILE *fout ) 
{
    /*
   fprintf(fout, "SHD_INSN: ");
   for (unsigned i=0;i<m_n_shader;i++) 
      fprintf(fout, "%u ",m_sc[i]->get_num_sim_insn());
   fprintf(fout, "\n");
   fprintf(fout, "SHD_THDS: ");
   for (unsigned i=0;i<m_n_shader;i++) 
      fprintf(fout, "%u ",m_sc[i]->get_not_completed());
   fprintf(fout, "\n");
   fprintf(fout, "SHD_DIVG: ");
   for (unsigned i=0;i<m_n_shader;i++) 
      fprintf(fout, "%u ",m_sc[i]->get_n_diverge());
   fprintf(fout, "\n");

   fprintf(fout, "THD_INSN: ");
   for (unsigned i=0; i<m_shader_config->n_thread_per_shader; i++) 
      fprintf(fout, "%d ", m_sc[0]->get_thread_n_insn(i) );
   fprintf(fout, "\n");
   */
}


void gpgpu_sim::shader_print_scheduler_stat( FILE* fout, bool print_dynamic_info ) const
{
    // Print out the stats from the sampling shader core
    const unsigned scheduler_sampling_core = m_shader_config->gpgpu_warp_issue_shader;
    #define STR_SIZE 55
    char name_buff[ STR_SIZE ];
    name_buff[ STR_SIZE - 1 ] = '\0';
    const std::vector< unsigned >& distro
        = print_dynamic_info ?
          m_shader_stats->get_dynamic_warp_issue()[ scheduler_sampling_core ] :
          m_shader_stats->get_warp_slot_issue()[ scheduler_sampling_core ];
    if ( print_dynamic_info ) {
        snprintf( name_buff, STR_SIZE - 1, "dynamic_warp_id" );
    } else {
        snprintf( name_buff, STR_SIZE - 1, "warp_id" );
    }
    fprintf( fout,
             "Shader %d %s issue ditsribution:\n",
             scheduler_sampling_core,
             name_buff );
    const unsigned num_warp_ids = distro.size();
    // First print out the warp ids
    fprintf( fout, "%s:\n", name_buff );
    for ( unsigned warp_id = 0;
          warp_id < num_warp_ids;
          ++warp_id  ) {
        fprintf( fout, "%d, ", warp_id );
    }

    fprintf( fout, "\ndistro:\n" );
    // Then print out the distribution of instuctions issued
    for ( std::vector< unsigned >::const_iterator iter = distro.begin();
          iter != distro.end();
          iter++ ) {
        fprintf( fout, "%d, ", *iter );
    }
    fprintf( fout, "\n" );
}

void gpgpu_sim::shader_print_cache_stats( FILE *fout ) const{

    // L1I
    struct cache_sub_stats total_css;
    struct cache_sub_stats css;

    if(!m_shader_config->m_L1I_config.disabled()){
        total_css.clear();
        css.clear();
        fprintf(fout, "\n========= Core cache stats =========\n");
        fprintf(fout, "L1I_cache:\n");
        for ( unsigned i = 0; i < m_shader_config->n_simt_clusters; ++i ) {
            m_cluster[i]->get_L1I_sub_stats(css);
            total_css += css;
        }
        fprintf(fout, "\tL1I_total_cache_accesses = %u\n", total_css.accesses);
        fprintf(fout, "\tL1I_total_cache_misses = %u\n", total_css.misses);
        fprintf(fout, "\tL1I_total_cache_cold_misses = %u\n", total_css.cold_misses);
        if(total_css.accesses > 0){
            fprintf(fout, "\tL1I_total_cache_miss_rate = %.4lf\n", (double)total_css.misses / (double)total_css.accesses);
            fprintf(fout, "\tL1I_total_cache_cold_miss_rate = %.4lf\n", (double)total_css.cold_misses / (double)total_css.misses);
        }
        fprintf(fout, "\tL1I_total_cache_pending_hits = %u\n", total_css.pending_hits);
        fprintf(fout, "\tL1I_total_cache_reservation_fails = %u\n", total_css.res_fails);
    }

    // L1D
    if(!m_shader_config->m_L1D_config.disabled()){
        total_css.clear();
        css.clear();
        fprintf(fout, "L1D_cache:\n");
        for (unsigned i=0;i<m_shader_config->n_simt_clusters;i++){
            m_cluster[i]->get_L1D_sub_stats(css);

            fprintf( stdout, "\tL1D_cache_core[%d]: Access = %d, Miss = %d, Miss_rate = %.3lf, Pending_hits = %u, Reservation_fails = %u\n",
                     i, css.accesses, css.misses, (double)css.misses / (double)css.accesses, css.pending_hits, css.res_fails);

            total_css += css;
        }
        fprintf(fout, "\tL1D_total_cache_accesses = %u\n", total_css.accesses);
        fprintf(fout, "\tL1D_total_cache_misses = %u\n", total_css.misses);
        fprintf(fout, "\tL1D_total_cache_cold_misses = %u\n", total_css.cold_misses);
        if(total_css.accesses > 0){
            fprintf(fout, "\tL1D_total_cache_miss_rate = %.4lf\n", (double)total_css.misses / (double)total_css.accesses);
            fprintf(fout, "\tL1D_total_cache_cold_miss_rate = %.4lf\n", (double)total_css.cold_misses / (double)total_css.misses);
        }
        fprintf(fout, "\tL1D_total_cache_pending_hits = %u\n", total_css.pending_hits);
        fprintf(fout, "\tL1D_total_cache_reservation_fails = %u\n", total_css.res_fails);
        total_css.print_port_stats(fout, "\tL1D_cache"); 
    }

    // L1C
    if(!m_shader_config->m_L1C_config.disabled()){
        total_css.clear();
        css.clear();
        fprintf(fout, "L1C_cache:\n");
        for ( unsigned i = 0; i < m_shader_config->n_simt_clusters; ++i ) {
            m_cluster[i]->get_L1C_sub_stats(css);
            total_css += css;
        }
        fprintf(fout, "\tL1C_total_cache_accesses = %u\n", total_css.accesses);
        fprintf(fout, "\tL1C_total_cache_misses = %u\n", total_css.misses);
        fprintf(fout, "\tL1C_total_cache_cold_misses = %u\n", total_css.cold_misses);
        if(total_css.accesses > 0){
            fprintf(fout, "\tL1C_total_cache_miss_rate = %.4lf\n", (double)total_css.misses / (double)total_css.accesses);
            fprintf(fout, "\tL1C_total_cache_cold_miss_rate = %.4lf\n", (double)total_css.cold_misses / (double)total_css.misses);
        }
        fprintf(fout, "\tL1C_total_cache_pending_hits = %u\n", total_css.pending_hits);
        fprintf(fout, "\tL1C_total_cache_reservation_fails = %u\n", total_css.res_fails);
    }

    // L1T
    if(!m_shader_config->m_L1T_config.disabled()){
        total_css.clear();
        css.clear();
        fprintf(fout, "L1T_cache:\n");
        for ( unsigned i = 0; i < m_shader_config->n_simt_clusters; ++i ) {
            m_cluster[i]->get_L1T_sub_stats(css);
            total_css += css;
        }
        fprintf(fout, "\tL1T_total_cache_accesses = %u\n", total_css.accesses);
        fprintf(fout, "\tL1T_total_cache_misses = %u\n", total_css.misses);
        fprintf(fout, "\tL1T_total_cache_cold_misses = %u\n", total_css.cold_misses);
        if(total_css.accesses > 0){
            fprintf(fout, "\tL1T_total_cache_miss_rate = %.4lf\n", (double)total_css.misses / (double)total_css.accesses);
            fprintf(fout, "\tL1T_total_cache_cold_miss_rate = %.4lf\n", (double)total_css.cold_misses / (double)total_css.misses);
        }
        fprintf(fout, "\tL1T_total_cache_pending_hits = %u\n", total_css.pending_hits);
        fprintf(fout, "\tL1T_total_cache_reservation_fails = %u\n", total_css.res_fails);
    }
}

void gpgpu_sim::shader_print_l1_miss_stat( FILE *fout ) const
{
   unsigned total_d1_misses = 0, total_d1_accesses = 0;
   for ( unsigned i = 0; i < m_shader_config->n_simt_clusters; ++i ) {
         unsigned custer_d1_misses = 0, cluster_d1_accesses = 0;
         m_cluster[ i ]->print_cache_stats( fout, cluster_d1_accesses, custer_d1_misses );
         total_d1_misses += custer_d1_misses;
         total_d1_accesses += cluster_d1_accesses;
   }
   fprintf( fout, "total_dl1_misses=%d\n", total_d1_misses );
   fprintf( fout, "total_dl1_accesses=%d\n", total_d1_accesses );
   fprintf( fout, "total_dl1_miss_rate= %f\n", (float)total_d1_misses / (float)total_d1_accesses );
   /*
   fprintf(fout, "THD_INSN_AC: ");
   for (unsigned i=0; i<m_shader_config->n_thread_per_shader; i++) 
      fprintf(fout, "%d ", m_sc[0]->get_thread_n_insn_ac(i));
   fprintf(fout, "\n");
   fprintf(fout, "T_L1_Mss: "); //l1 miss rate per thread
   for (unsigned i=0; i<m_shader_config->n_thread_per_shader; i++) 
      fprintf(fout, "%d ", m_sc[0]->get_thread_n_l1_mis_ac(i));
   fprintf(fout, "\n");
   fprintf(fout, "T_L1_Mgs: "); //l1 merged miss rate per thread
   for (unsigned i=0; i<m_shader_config->n_thread_per_shader; i++) 
      fprintf(fout, "%d ", m_sc[0]->get_thread_n_l1_mis_ac(i) - m_sc[0]->get_thread_n_l1_mrghit_ac(i));
   fprintf(fout, "\n");
   fprintf(fout, "T_L1_Acc: "); //l1 access per thread
   for (unsigned i=0; i<m_shader_config->n_thread_per_shader; i++) 
      fprintf(fout, "%d ", m_sc[0]->get_thread_n_l1_access_ac(i));
   fprintf(fout, "\n");

   //per warp
   int temp =0; 
   fprintf(fout, "W_L1_Mss: "); //l1 miss rate per warp
   for (unsigned i=0; i<m_shader_config->n_thread_per_shader; i++) {
      temp += m_sc[0]->get_thread_n_l1_mis_ac(i);
      if (i%m_shader_config->warp_size == (unsigned)(m_shader_config->warp_size-1)) {
         fprintf(fout, "%d ", temp);
         temp = 0;
      }
   }
   fprintf(fout, "\n");
   temp=0;
   fprintf(fout, "W_L1_Mgs: "); //l1 merged miss rate per warp
   for (unsigned i=0; i<m_shader_config->n_thread_per_shader; i++) {
      temp += (m_sc[0]->get_thread_n_l1_mis_ac(i) - m_sc[0]->get_thread_n_l1_mrghit_ac(i) );
      if (i%m_shader_config->warp_size == (unsigned)(m_shader_config->warp_size-1)) {
         fprintf(fout, "%d ", temp);
         temp = 0;
      }
   }
   fprintf(fout, "\n");
   temp =0;
   fprintf(fout, "W_L1_Acc: "); //l1 access per warp
   for (unsigned i=0; i<m_shader_config->n_thread_per_shader; i++) {
      temp += m_sc[0]->get_thread_n_l1_access_ac(i);
      if (i%m_shader_config->warp_size == (unsigned)(m_shader_config->warp_size-1)) {
         fprintf(fout, "%d ", temp);
         temp = 0;
      }
   }
   fprintf(fout, "\n");
   */
}

void warp_inst_t::print( FILE *fout ) const
{
    if (empty() ) {
        fprintf(fout,"bubble\n" );
        return;
    } else 
        fprintf(fout,"0x%04x ", pc );
    fprintf(fout, "w%02d[", m_warp_id);
    for (unsigned j=0; j<m_config->warp_size; j++)
        fprintf(fout, "%c", (active(j)?'1':'0') );
    fprintf(fout, "]: ");
    ptx_print_insn( pc, fout );
    fprintf(fout, "\n");
}
void shader_core_ctx::incexecstat(warp_inst_t *&inst)
{
	if(inst->mem_op==TEX)
		inctex_stat(inst->active_count(),1);

    // Latency numbers for next operations are used to scale the power values
    // for special operations, according observations from microbenchmarking
    // TODO: put these numbers in the xml configuration

	switch(inst->sp_op){
	case INT__OP:
		incialu_stat(inst->active_count(),25);
		break;
	case INT_MUL_OP:
		incimul_stat(inst->active_count(),7.2);
		break;
	case INT_MUL24_OP:
		incimul24_stat(inst->active_count(),4.2);
		break;
	case INT_MUL32_OP:
		incimul32_stat(inst->active_count(),4);
		break;
	case INT_DIV_OP:
		incidiv_stat(inst->active_count(),40);
		break;
	case FP__OP:
		incfpalu_stat(inst->active_count(),1);
		break;
	case FP_MUL_OP:
		incfpmul_stat(inst->active_count(),1.8);
		break;
	case FP_DIV_OP:
		incfpdiv_stat(inst->active_count(),48);
		break;
	case FP_SQRT_OP:
		inctrans_stat(inst->active_count(),25);
		break;
	case FP_LG_OP:
		inctrans_stat(inst->active_count(),35);
		break;
	case FP_SIN_OP:
		inctrans_stat(inst->active_count(),12);
		break;
	case FP_EXP_OP:
		inctrans_stat(inst->active_count(),35);
		break;
	default:
		break;
	}
}
void shader_core_ctx::print_stage(unsigned int stage, FILE *fout ) const
{
   m_pipeline_reg[stage].print(fout);
   //m_pipeline_reg[stage].print(fout);
}

void shader_core_ctx::display_simt_state(FILE *fout, int mask ) const
{
    if ( (mask & 4) && m_config->model == POST_DOMINATOR ) {
       fprintf(fout,"per warp SIMT control-flow state:\n");
       unsigned n = m_config->n_thread_per_shader / m_config->warp_size;
       for (unsigned i=0; i < n; i++) {
          unsigned nactive = 0;
          for (unsigned j=0; j<m_config->warp_size; j++ ) {
             unsigned tid = i*m_config->warp_size + j;
             int done = ptx_thread_done(tid);
             nactive += (ptx_thread_done(tid)?0:1);
             if ( done && (mask & 8) ) {
                unsigned done_cycle = m_thread[tid]->donecycle();
                if ( done_cycle ) {
                   printf("\n w%02u:t%03u: done @ cycle %u", i, tid, done_cycle );
                }
             }
          }
          if ( nactive == 0 ) {
             continue;
          }
          m_simt_stack[i]->print(fout);
       }
       fprintf(fout,"\n");
    }
}

void ldst_unit::print(FILE *fout) const
{
    fprintf(fout,"LD/ST unit  = ");
    m_dispatch_reg->print(fout);
    if ( m_mem_rc != NO_RC_FAIL ) {
        fprintf(fout,"              LD/ST stall condition: ");
        switch ( m_mem_rc ) {
        case BK_CONF:        fprintf(fout,"BK_CONF"); break;
        case MSHR_RC_FAIL:   fprintf(fout,"MSHR_RC_FAIL"); break;
        case ICNT_RC_FAIL:   fprintf(fout,"ICNT_RC_FAIL"); break;
        case COAL_STALL:     fprintf(fout,"COAL_STALL"); break;
        case WB_ICNT_RC_FAIL: fprintf(fout,"WB_ICNT_RC_FAIL"); break;
        case WB_CACHE_RSRV_FAIL: fprintf(fout,"WB_CACHE_RSRV_FAIL"); break;
        case N_MEM_STAGE_STALL_TYPE: fprintf(fout,"N_MEM_STAGE_STALL_TYPE"); break;
        default: abort();
        }
        fprintf(fout,"\n");
    }
    fprintf(fout,"LD/ST wb    = ");
    m_next_wb.print(fout);
    fprintf(fout, "Last LD/ST writeback @ %llu + %llu (gpu_sim_cycle+gpu_tot_sim_cycle)\n",
                  m_last_inst_gpu_sim_cycle, m_last_inst_gpu_tot_sim_cycle );
    fprintf(fout,"Pending register writes:\n");
    std::map<unsigned/*warp_id*/, std::map<unsigned/*regnum*/,unsigned/*count*/> >::const_iterator w;
    for( w=m_pending_writes.begin(); w!=m_pending_writes.end(); w++ ) {
        unsigned warp_id = w->first;
        const std::map<unsigned/*regnum*/,unsigned/*count*/> &warp_info = w->second;
        if( warp_info.empty() ) 
            continue;
        fprintf(fout,"  w%2u : ", warp_id );
        std::map<unsigned/*regnum*/,unsigned/*count*/>::const_iterator r;
        for( r=warp_info.begin(); r!=warp_info.end(); ++r ) {
            fprintf(fout,"  %u(%u)", r->first, r->second );
        }
        fprintf(fout,"\n");
    }
    m_L1C->display_state(fout);
    m_L1T->display_state(fout);
    if( !m_config->m_L1D_config.disabled() )
    	m_L1D->display_state(fout);
    fprintf(fout,"LD/ST response FIFO (occupancy = %zu):\n", m_response_fifo.size() );
    for( std::list<mem_fetch*>::const_iterator i=m_response_fifo.begin(); i != m_response_fifo.end(); i++ ) {
        const mem_fetch *mf = *i;
        mf->print(fout);
    }
}

void shader_core_ctx::display_pipeline(FILE *fout, int print_mem, int mask ) const
{
   fprintf(fout, "=================================================\n");
   fprintf(fout, "shader %u at cycle %Lu+%Lu (%u threads running)\n", m_sid, 
           gpu_tot_sim_cycle, gpu_sim_cycle, m_not_completed);
   fprintf(fout, "=================================================\n");

   dump_warp_state(fout);
   fprintf(fout,"\n");

   m_L1I->display_state(fout);

   fprintf(fout, "IF/ID       = ");
   if( !m_inst_fetch_buffer.m_valid )
       fprintf(fout,"bubble\n");
   else {
       fprintf(fout,"w%2u : pc = 0x%x, nbytes = %u\n", 
               m_inst_fetch_buffer.m_warp_id,
               m_inst_fetch_buffer.m_pc, 
               m_inst_fetch_buffer.m_nbytes );
   }
   fprintf(fout,"\nibuffer status:\n");
   for( unsigned i=0; i<m_config->max_warps_per_shader; i++) {
       if( !m_warp[i].ibuffer_empty() ) 
           m_warp[i].print_ibuffer(fout);
   }
   fprintf(fout,"\n");
   display_simt_state(fout,mask);
   fprintf(fout, "-------------------------- Scoreboard\n");
   m_scoreboard->printContents();
/*
   fprintf(fout,"ID/OC (SP)  = ");
   print_stage(ID_OC_SP, fout);
   fprintf(fout,"ID/OC (SFU) = ");
   print_stage(ID_OC_SFU, fout);
   fprintf(fout,"ID/OC (MEM) = ");
   print_stage(ID_OC_MEM, fout);
*/
   fprintf(fout, "-------------------------- OP COL\n");
   m_operand_collector.dump(fout);
/* fprintf(fout, "OC/EX (SP)  = ");
   print_stage(OC_EX_SP, fout);
   fprintf(fout, "OC/EX (SFU) = ");
   print_stage(OC_EX_SFU, fout);
   fprintf(fout, "OC/EX (MEM) = ");
   print_stage(OC_EX_MEM, fout);
*/
   fprintf(fout, "-------------------------- Pipe Regs\n");

   for (unsigned i = 0; i < N_PIPELINE_STAGES; i++) {
       fprintf(fout,"--- %s ---\n",pipeline_stage_name_decode[i]);
       print_stage(i,fout);fprintf(fout,"\n");
   }

   fprintf(fout, "-------------------------- Fu\n");
   for( unsigned n=0; n < m_num_function_units; n++ ){
       m_fu[n]->print(fout);
       fprintf(fout, "---------------\n");
   }
   fprintf(fout, "-------------------------- other:\n");

   for(unsigned i=0; i<num_result_bus; i++){
	   std::string bits = m_result_bus[i]->to_string();
	   fprintf(fout, "EX/WB sched[%d]= %s\n", i, bits.c_str() );
   }
   fprintf(fout, "EX/WB      = ");
   print_stage(EX_WB, fout);
   fprintf(fout, "\n");
   fprintf(fout, "Last EX/WB writeback @ %llu + %llu (gpu_sim_cycle+gpu_tot_sim_cycle)\n",
                 m_last_inst_gpu_sim_cycle, m_last_inst_gpu_tot_sim_cycle );

   if( m_active_threads.count() <= 2*m_config->warp_size ) {
       fprintf(fout,"Active Threads : ");
       unsigned last_warp_id = -1;
       for(unsigned tid=0; tid < m_active_threads.size(); tid++ ) {
           unsigned warp_id = tid/m_config->warp_size;
           if( m_active_threads.test(tid) ) {
               if( warp_id != last_warp_id ) {
                   fprintf(fout,"\n  warp %u : ", warp_id );
                   last_warp_id=warp_id;
               }
               fprintf(fout,"%u ", tid );
           }
       }
   }

}

unsigned int shader_core_config::max_cta( const kernel_info_t &k ) const
{
   unsigned threads_per_cta  = k.threads_per_cta();
   const class function_info *kernel = k.entry();
   unsigned int padded_cta_size = threads_per_cta;
   if (padded_cta_size%warp_size) 
      padded_cta_size = ((padded_cta_size/warp_size)+1)*(warp_size);

   //Limit by n_threads/shader
   unsigned int result_thread = n_thread_per_shader / padded_cta_size;

   const struct gpgpu_ptx_sim_kernel_info *kernel_info = ptx_sim_kernel_info(kernel);

   //Limit by shmem/shader
   unsigned int result_shmem = (unsigned)-1;
   if (kernel_info->smem > 0)
      result_shmem = gpgpu_shmem_size / kernel_info->smem;

   //Limit by register count, rounded up to multiple of 4.
   unsigned int result_regs = (unsigned)-1;
   if (kernel_info->regs > 0)
      result_regs = gpgpu_shader_registers / (padded_cta_size * ((kernel_info->regs+3)&~3));

   //Limit by CTA
   unsigned int result_cta = max_cta_per_core;

   unsigned result = result_thread;
   result = gs_min2(result, result_shmem);
   result = gs_min2(result, result_regs);
   result = gs_min2(result, result_cta);

   static const struct gpgpu_ptx_sim_kernel_info* last_kinfo = NULL;
   if (last_kinfo != kernel_info) {   //Only print out stats if kernel_info struct changes
      last_kinfo = kernel_info;
      printf ("GPGPU-Sim uArch: CTA/core = %u, limited by:", result);
      if (result == result_thread) printf (" threads");
      if (result == result_shmem) printf (" shmem");
      if (result == result_regs) printf (" regs");
      if (result == result_cta) printf (" cta_limit");
      printf ("\n");
   }

    //gpu_max_cta_per_shader is limited by number of CTAs if not enough to keep all cores busy    
    if( k.num_blocks() < result*num_shader() ) { 
       result = k.num_blocks() / num_shader();
       if (k.num_blocks() % num_shader())
          result++;
    }

    assert( result <= MAX_CTA_PER_SHADER );
    if (result < 1) {
       printf ("GPGPU-Sim uArch: ERROR ** Kernel requires more resources than shader has.\n");
       abort();
    }

    return result;
}

void shader_core_ctx::cycle()
{
	m_stats->shader_cycles[m_sid]++;
    writeback();
    execute();
    read_operands();
    issue();
    decode();
    fetch();
}

// Flushes all content of the cache to memory

void shader_core_ctx::cache_flush()
{
   m_ldst_unit->flush();
}

// modifiers
std::list<opndcoll_rfu_t::op_t> opndcoll_rfu_t::arbiter_t::allocate_reads() 
{
   std::list<op_t> result;  // a list of registers that (a) are in different register banks, (b) do not go to the same operand collector

   int input;
   int output;
   int _inputs = m_num_banks;
   int _outputs = m_num_collectors;
   int _square = ( _inputs > _outputs ) ? _inputs : _outputs;
   assert(_square > 0);
   int _pri = (int)m_last_cu;

   // Clear matching
   for ( int i = 0; i < _inputs; ++i ) 
      _inmatch[i] = -1;
   for ( int j = 0; j < _outputs; ++j ) 
      _outmatch[j] = -1;

   for( unsigned i=0; i<m_num_banks; i++) {
      for( unsigned j=0; j<m_num_collectors; j++) {
         assert( i < (unsigned)_inputs );
         assert( j < (unsigned)_outputs );
         _request[i][j] = 0;
      }
      if( !m_queue[i].empty() ) {
         const op_t &op = m_queue[i].front();
         int oc_id = op.get_oc_id();
         assert( i < (unsigned)_inputs );
         assert( oc_id < _outputs );
         _request[i][oc_id] = 1;
      }
      if( m_allocated_bank[i].is_write() ) {
         assert( i < (unsigned)_inputs );
         _inmatch[i] = 0; // write gets priority
      }
   }

   ///// wavefront allocator from booksim... --->
   
   // Loop through diagonals of request matrix

   for ( int p = 0; p < _square; ++p ) {
      output = ( _pri + p ) % _square;

      // Step through the current diagonal
      for ( input = 0; input < _inputs; ++input ) {
          assert( input < _inputs );
          assert( output < _outputs );
         if ( ( output < _outputs ) && 
              ( _inmatch[input] == -1 ) && 
              ( _outmatch[output] == -1 ) &&
              ( _request[input][output]/*.label != -1*/ ) ) {
            // Grant!
            _inmatch[input] = output;
            _outmatch[output] = input;
         }

         output = ( output + 1 ) % _square;
      }
   }

   // Round-robin the priority diagonal
   _pri = ( _pri + 1 ) % _square;

   /// <--- end code from booksim

   m_last_cu = _pri;
   for( unsigned i=0; i < m_num_banks; i++ ) {
      if( _inmatch[i] != -1 ) {
         if( !m_allocated_bank[i].is_write() ) {
            unsigned bank = (unsigned)i;
            op_t &op = m_queue[bank].front();
            result.push_back(op);
            m_queue[bank].pop_front();
         }
      }
   }

   return result;
}

barrier_set_t::barrier_set_t( unsigned max_warps_per_core, unsigned max_cta_per_core )
{
   m_max_warps_per_core = max_warps_per_core;
   m_max_cta_per_core = max_cta_per_core;
   if( max_warps_per_core > WARP_PER_CTA_MAX ) {
      printf("ERROR ** increase WARP_PER_CTA_MAX in shader.h from %u to >= %u or warps per cta in gpgpusim.config\n",
             WARP_PER_CTA_MAX, max_warps_per_core );
      exit(1);
   }
   m_warp_active.reset();
   m_warp_at_barrier.reset();
}

// during cta allocation
void barrier_set_t::allocate_barrier( unsigned cta_id, warp_set_t warps )
{
   assert( cta_id < m_max_cta_per_core );
   cta_to_warp_t::iterator w=m_cta_to_warps.find(cta_id);
   assert( w == m_cta_to_warps.end() ); // cta should not already be active or allocated barrier resources
   m_cta_to_warps[cta_id] = warps;
   assert( m_cta_to_warps.size() <= m_max_cta_per_core ); // catch cta's that were not properly deallocated
  
   m_warp_active |= warps;
   m_warp_at_barrier &= ~warps;
}

// during cta deallocation
void barrier_set_t::deallocate_barrier( unsigned cta_id )
{
   cta_to_warp_t::iterator w=m_cta_to_warps.find(cta_id);
   if( w == m_cta_to_warps.end() )
      return;
   warp_set_t warps = w->second;
   warp_set_t at_barrier = warps & m_warp_at_barrier;
   assert( at_barrier.any() == false ); // no warps stuck at barrier
   warp_set_t active = warps & m_warp_active;
   assert( active.any() == false ); // no warps in CTA still running
   m_warp_active &= ~warps;
   m_warp_at_barrier &= ~warps;
   m_cta_to_warps.erase(w);
}

// individual warp hits barrier
void barrier_set_t::warp_reaches_barrier( unsigned cta_id, unsigned warp_id )
{
   cta_to_warp_t::iterator w=m_cta_to_warps.find(cta_id);

   if( w == m_cta_to_warps.end() ) { // cta is active
      printf("ERROR ** cta_id %u not found in barrier set on cycle %llu+%llu...\n", cta_id, gpu_tot_sim_cycle, gpu_sim_cycle );
      dump();
      abort();
   }
   assert( w->second.test(warp_id) == true ); // warp is in cta

   m_warp_at_barrier.set(warp_id);

   warp_set_t warps_in_cta = w->second;
   warp_set_t at_barrier = warps_in_cta & m_warp_at_barrier;
   warp_set_t active = warps_in_cta & m_warp_active;

   if( at_barrier == active ) {
      // all warps have reached barrier, so release waiting warps...
      m_warp_at_barrier &= ~at_barrier;
   }
}

// fetching a warp
bool barrier_set_t::available_for_fetch( unsigned warp_id ) const
{
   return m_warp_active.test(warp_id) && m_warp_at_barrier.test(warp_id);
}

// warp reaches exit 
void barrier_set_t::warp_exit( unsigned warp_id )
{
   // caller needs to verify all threads in warp are done, e.g., by checking PDOM stack to 
   // see it has only one entry during exit_impl()
   m_warp_active.reset(warp_id);

   // test for barrier release 
   cta_to_warp_t::iterator w=m_cta_to_warps.begin(); 
   for (; w != m_cta_to_warps.end(); ++w) {
      if (w->second.test(warp_id) == true) break; 
   }
   warp_set_t warps_in_cta = w->second;
   warp_set_t at_barrier = warps_in_cta & m_warp_at_barrier;
   warp_set_t active = warps_in_cta & m_warp_active;

   if( at_barrier == active ) {
      // all warps have reached barrier, so release waiting warps...
      m_warp_at_barrier &= ~at_barrier;
   }
}

// assertions
bool barrier_set_t::warp_waiting_at_barrier( unsigned warp_id ) const
{ 
   return m_warp_at_barrier.test(warp_id);
}

void barrier_set_t::dump() const
{
   printf( "barrier set information\n");
   printf( "  m_max_cta_per_core = %u\n",  m_max_cta_per_core );
   printf( "  m_max_warps_per_core = %u\n", m_max_warps_per_core );
   printf( "  cta_to_warps:\n");
   
   cta_to_warp_t::const_iterator i;
   for( i=m_cta_to_warps.begin(); i!=m_cta_to_warps.end(); i++ ) {
      unsigned cta_id = i->first;
      warp_set_t warps = i->second;
      printf("    cta_id %u : %s\n", cta_id, warps.to_string().c_str() );
   }
   printf("  warp_active: %s\n", m_warp_active.to_string().c_str() );
   printf("  warp_at_barrier: %s\n", m_warp_at_barrier.to_string().c_str() );
   fflush(stdout); 
}

void shader_core_ctx::warp_exit( unsigned warp_id )
{
	bool done = true;
	for (	unsigned i = warp_id*get_config()->warp_size;
			i < (warp_id+1)*get_config()->warp_size;
			i++ ) {

//		if(this->m_thread[i]->m_functional_model_thread_state && this->m_thread[i].m_functional_model_thread_state->donecycle()==0) {
//			done = false;
//		}


		if (m_thread[i] && !m_thread[i]->is_done()) done = false;
	}
	//if (m_warp[warp_id].get_n_completed() == get_config()->warp_size)
	//if (this->m_simt_stack[warp_id]->get_num_entries() == 0)
	if (done)
		m_barriers.warp_exit( warp_id );
}

bool shader_core_ctx::warp_waiting_at_barrier( unsigned warp_id ) const
{
   return m_barriers.warp_waiting_at_barrier(warp_id);
}

bool shader_core_ctx::warp_waiting_at_mem_barrier( unsigned warp_id ) 
{
   if( !m_warp[warp_id].get_membar() ) 
      return false;
   if( !m_scoreboard->pendingWrites(warp_id) ) {
      m_warp[warp_id].clear_membar();
      return false;
   }
   return true;
}

void shader_core_ctx::set_max_cta( const kernel_info_t &kernel ) 
{
    // calculate the max cta count and cta size for local memory address mapping
    kernel_max_cta_per_shader = m_config->max_cta(kernel);
    unsigned int gpu_cta_size = kernel.threads_per_cta();
    kernel_padded_threads_per_cta = (gpu_cta_size%m_config->warp_size) ? 
        m_config->warp_size*((gpu_cta_size/m_config->warp_size)+1) : 
        gpu_cta_size;
}

void shader_core_ctx::decrement_atomic_count( unsigned wid, unsigned n )
{
   assert( m_warp[wid].get_n_atomic() >= n );
   m_warp[wid].dec_n_atomic(n);
}


bool shader_core_ctx::fetch_unit_response_buffer_full() const
{
    return false;
}

void shader_core_ctx::accept_fetch_response( mem_fetch *mf )
{
    mf->set_status(IN_SHADER_FETCHED,gpu_sim_cycle+gpu_tot_sim_cycle);
    m_L1I->fill(mf,gpu_sim_cycle+gpu_tot_sim_cycle);
}

bool shader_core_ctx::ldst_unit_response_buffer_full() const
{
    return m_ldst_unit->response_buffer_full();
}

void shader_core_ctx::accept_ldst_unit_response(mem_fetch * mf) 
{
   m_ldst_unit->fill(mf);
}

void shader_core_ctx::store_ack( class mem_fetch *mf )
{
	assert( mf->get_type() == WRITE_ACK  || ( m_config->gpgpu_perfect_mem && mf->get_is_write() ) );
    unsigned warp_id = mf->get_wid();
    m_warp[warp_id].dec_store_req();
}

void shader_core_ctx::print_cache_stats( FILE *fp, unsigned& dl1_accesses, unsigned& dl1_misses ) {
   m_ldst_unit->print_cache_stats( fp, dl1_accesses, dl1_misses );
}

void shader_core_ctx::get_cache_stats(cache_stats &cs){
    // Adds stats from each cache to 'cs'
    cs += m_L1I->get_stats(); // Get L1I stats
    m_ldst_unit->get_cache_stats(cs); // Get L1D, L1C, L1T stats
}

void shader_core_ctx::get_L1I_sub_stats(struct cache_sub_stats &css) const{
    if(m_L1I)
        m_L1I->get_sub_stats(css);
}
void shader_core_ctx::get_L1D_sub_stats(struct cache_sub_stats &css) const{
    m_ldst_unit->get_L1D_sub_stats(css);
}
void shader_core_ctx::get_L1C_sub_stats(struct cache_sub_stats &css) const{
    m_ldst_unit->get_L1C_sub_stats(css);
}
void shader_core_ctx::get_L1T_sub_stats(struct cache_sub_stats &css) const{
    m_ldst_unit->get_L1T_sub_stats(css);
}

void shader_core_ctx::get_icnt_power_stats(long &n_simt_to_mem, long &n_mem_to_simt) const{
	n_simt_to_mem += m_stats->n_simt_to_mem[m_sid];
	n_mem_to_simt += m_stats->n_mem_to_simt[m_sid];
}

bool shd_warp_t::functional_done() const
{
    return get_n_completed() == m_warp_size;
}

bool shd_warp_t::hardware_done() const
{
    return functional_done() && stores_done() && !inst_in_pipeline(); 
}

bool shd_warp_t::waiting() 
{
    if ( functional_done() ) {
        // waiting to be initialized with a kernel
        return true;
    } else if ( m_shader->warp_waiting_at_barrier(m_warp_id) ) {
        // waiting for other warps in CTA to reach barrier
        return true;
    } else if ( m_shader->warp_waiting_at_mem_barrier(m_warp_id) ) {
        // waiting for memory barrier
        return true;
    } else if ( m_n_atomic >0 ) {
        // waiting for atomic operation to complete at memory:
        // this stall is not required for accurate timing model, but rather we
        // stall here since if a call/return instruction occurs in the meantime
        // the functional execution of the atomic when it hits DRAM can cause
        // the wrong register to be read.
        return true;
    }
    return false;
}

void shd_warp_t::print( FILE *fout ) const
{
    if( !done_exit() ) {
        fprintf( fout, "w%02u npc: 0x%04x, done:%c%c%c%c:%2u i:%u s:%u a:%u (done: ", 
                m_warp_id,
                m_next_pc,
                (functional_done()?'f':' '),
                (stores_done()?'s':' '),
                (inst_in_pipeline()?' ':'i'),
                (done_exit()?'e':' '),
                n_completed,
                m_inst_in_pipeline, 
                m_stores_outstanding,
                m_n_atomic );
        for (unsigned i = m_warp_id*m_warp_size; i < (m_warp_id+1)*m_warp_size; i++ ) {
          if ( m_shader->ptx_thread_done(i) ) fprintf(fout,"1");
          else fprintf(fout,"0");
          if ( (((i+1)%4) == 0) && (i+1) < (m_warp_id+1)*m_warp_size ) 
             fprintf(fout,",");
        }
        fprintf(fout,") ");
        fprintf(fout," active=%s", m_active_threads.to_string().c_str() );
        fprintf(fout," last fetched @ %5llu", m_last_fetch);
        if( m_imiss_pending ) 
            fprintf(fout," i-miss pending");
        fprintf(fout,"\n");
    }
}

void shd_warp_t::print_ibuffer( FILE *fout ) const
{
    fprintf(fout,"  ibuffer[%2u] : ", m_warp_id );
    for( unsigned i=0; i < IBUFFER_SIZE; i++) {
        const inst_t *inst = m_ibuffer[i].m_inst;
        if( inst ) inst->print_insn(fout);
        else if( m_ibuffer[i].m_valid ) 
           fprintf(fout," <invalid instruction> ");
        else fprintf(fout," <empty> ");
    }
    fprintf(fout,"\n");
}

void opndcoll_rfu_t::add_cu_set(unsigned set_id, unsigned num_cu, unsigned num_dispatch){
    m_cus[set_id].reserve(num_cu); //this is necessary to stop pointers in m_cu from being invalid do to a resize;
    for (unsigned i = 0; i < num_cu; i++) {
        m_cus[set_id].push_back(collector_unit_t());
        m_cu.push_back(&m_cus[set_id].back());
    }
    // for now each collector set gets dedicated dispatch units.
    for (unsigned i = 0; i < num_dispatch; i++) {
        m_dispatch_units.push_back(dispatch_unit_t(&m_cus[set_id]));
    }
}


void opndcoll_rfu_t::add_port(port_vector_t & input, port_vector_t & output, uint_vector_t cu_sets)
{
    //m_num_ports++;
    //m_num_collectors += num_collector_units;
    //m_input.resize(m_num_ports);
    //m_output.resize(m_num_ports);
    //m_num_collector_units.resize(m_num_ports);
    //m_input[m_num_ports-1]=input_port;
    //m_output[m_num_ports-1]=output_port;
    //m_num_collector_units[m_num_ports-1]=num_collector_units;
    m_in_ports.push_back(input_port_t(input,output,cu_sets));
}

void opndcoll_rfu_t::init( unsigned num_banks, shader_core_ctx *shader )
{
   m_shader=shader;
   m_arbiter.init(m_cu.size(),num_banks);
   //for( unsigned n=0; n<m_num_ports;n++ ) 
   //    m_dispatch_units[m_output[n]].init( m_num_collector_units[n] );
   m_num_banks = num_banks;
   m_bank_warp_shift = 0; 
   m_warp_size = shader->get_config()->warp_size;
   m_bank_warp_shift = (unsigned)(int) (log(m_warp_size+0.5) / log(2.0));
   assert( (m_bank_warp_shift == 5) || (m_warp_size != 32) );

   for( unsigned j=0; j<m_cu.size(); j++) {
       m_cu[j]->init(j,num_banks,m_bank_warp_shift,shader->get_config(),this);
   }
   m_initialized=true;
}

int register_bank(int regnum, int wid, unsigned num_banks, unsigned bank_warp_shift)
{
   int bank = regnum;
   if (bank_warp_shift)
      bank += wid;
   return bank % num_banks;
}

bool opndcoll_rfu_t::writeback( const warp_inst_t &inst )
{
   assert( !inst.empty() );
   std::list<unsigned> regs = m_shader->get_regs_written(inst);
   std::list<unsigned>::iterator r;
   unsigned n=0;
   for( r=regs.begin(); r!=regs.end();r++,n++ ) {
      unsigned reg = *r;
      unsigned bank = register_bank(reg,inst.warp_id(),m_num_banks,m_bank_warp_shift);
      if( m_arbiter.bank_idle(bank) ) {
          m_arbiter.allocate_bank_for_write(bank,op_t(&inst,reg,m_num_banks,m_bank_warp_shift));
      } else {
          return false;
      }
   }
   for(unsigned i=0;i<(unsigned)regs.size();i++){
	      if(m_shader->get_config()->gpgpu_clock_gated_reg_file){
	    	  unsigned active_count=0;
	    	  for(unsigned i=0;i<m_shader->get_config()->warp_size;i=i+m_shader->get_config()->n_regfile_gating_group){
	    		  for(unsigned j=0;j<m_shader->get_config()->n_regfile_gating_group;j++){
	    			  if(inst.get_active_mask().test(i+j)){
	    				  active_count+=m_shader->get_config()->n_regfile_gating_group;
	    				  break;
	    			  }
	    		  }
	    	  }
	    	  m_shader->incregfile_writes(active_count);
	      }else{
	    	  m_shader->incregfile_writes(m_shader->get_config()->warp_size);//inst.active_count());
	      }
   }
   return true;
}

void opndcoll_rfu_t::dispatch_ready_cu()
{
   for( unsigned p=0; p < m_dispatch_units.size(); ++p ) {
      dispatch_unit_t &du = m_dispatch_units[p];
      collector_unit_t *cu = du.find_ready();
      if( cu ) {
    	 for(unsigned i=0;i<(cu->get_num_operands()-cu->get_num_regs());i++){
   	      if(m_shader->get_config()->gpgpu_clock_gated_reg_file){
   	    	  unsigned active_count=0;
   	    	  for(unsigned i=0;i<m_shader->get_config()->warp_size;i=i+m_shader->get_config()->n_regfile_gating_group){
   	    		  for(unsigned j=0;j<m_shader->get_config()->n_regfile_gating_group;j++){
   	    			  if(cu->get_active_mask().test(i+j)){
   	    				  active_count+=m_shader->get_config()->n_regfile_gating_group;
   	    				  break;
   	    			  }
   	    		  }
   	    	  }
   	    	  m_shader->incnon_rf_operands(active_count);
   	      }else{
    		 m_shader->incnon_rf_operands(m_shader->get_config()->warp_size);//cu->get_active_count());
   	      }
    	}
         cu->dispatch();
      }
   }
}

void opndcoll_rfu_t::allocate_cu( unsigned port_num )
{
   input_port_t& inp = m_in_ports[port_num];
   for (unsigned i = 0; i < inp.m_in.size(); i++) {
       if( (*inp.m_in[i]).has_ready() ) {
          //find a free cu 
          for (unsigned j = 0; j < inp.m_cu_sets.size(); j++) {
              std::vector<collector_unit_t> & cu_set = m_cus[inp.m_cu_sets[j]];
	      bool allocated = false;
              for (unsigned k = 0; k < cu_set.size(); k++) {
                  if(cu_set[k].is_free()) {
                     collector_unit_t *cu = &cu_set[k];
                     allocated = cu->allocate(inp.m_in[i],inp.m_out[i]);
                     m_arbiter.add_read_requests(cu);
                     break;
                  }
              }
              if (allocated) break; //cu has been allocated, no need to search more.
          }
          break; // can only service a single input, if it failed it will fail for others.
       }
   }
}

void opndcoll_rfu_t::allocate_reads()
{
   // process read requests that do not have conflicts
   std::list<op_t> allocated = m_arbiter.allocate_reads();
   std::map<unsigned,op_t> read_ops;
   for( std::list<op_t>::iterator r=allocated.begin(); r!=allocated.end(); r++ ) {
      const op_t &rr = *r;
      unsigned reg = rr.get_reg();
      unsigned wid = rr.get_wid();
      unsigned bank = register_bank(reg,wid,m_num_banks,m_bank_warp_shift);
      m_arbiter.allocate_for_read(bank,rr);
      read_ops[bank] = rr;
   }
   std::map<unsigned,op_t>::iterator r;
   for(r=read_ops.begin();r!=read_ops.end();++r ) {
      op_t &op = r->second;
      unsigned cu = op.get_oc_id();
      unsigned operand = op.get_operand();
      m_cu[cu]->collect_operand(operand);
      if(m_shader->get_config()->gpgpu_clock_gated_reg_file){
    	  unsigned active_count=0;
    	  for(unsigned i=0;i<m_shader->get_config()->warp_size;i=i+m_shader->get_config()->n_regfile_gating_group){
    		  for(unsigned j=0;j<m_shader->get_config()->n_regfile_gating_group;j++){
    			  if(op.get_active_mask().test(i+j)){
    				  active_count+=m_shader->get_config()->n_regfile_gating_group;
    				  break;
    			  }
    		  }
    	  }
    	  m_shader->incregfile_reads(active_count);
      }else{
    	  m_shader->incregfile_reads(m_shader->get_config()->warp_size);//op.get_active_count());
      }
  }
} 

bool opndcoll_rfu_t::collector_unit_t::ready() const 
{ 
   return (!m_free) && m_not_ready.none() && (*m_output_register).has_free(); 
}

void opndcoll_rfu_t::collector_unit_t::dump(FILE *fp, const shader_core_ctx *shader ) const
{
   if( m_free ) {
      fprintf(fp,"    <free>\n");
   } else {
      m_warp->print(fp);
      for( unsigned i=0; i < MAX_REG_OPERANDS*2; i++ ) {
         if( m_not_ready.test(i) ) {
            std::string r = m_src_op[i].get_reg_string();
            fprintf(fp,"    '%s' not ready\n", r.c_str() );
         }
      }
   }
}

void opndcoll_rfu_t::collector_unit_t::init( unsigned n, 
                                             unsigned num_banks, 
                                             unsigned log2_warp_size,
                                             const core_config *config,
                                             opndcoll_rfu_t *rfu ) 
{ 
   m_rfu=rfu;
   m_cuid=n; 
   m_num_banks=num_banks;
   assert(m_warp==NULL); 
   m_warp = new warp_inst_t(config);
   m_bank_warp_shift=log2_warp_size;
}

bool opndcoll_rfu_t::collector_unit_t::allocate( register_set* pipeline_reg_set, register_set* output_reg_set ) 
{
   assert(m_free);
   assert(m_not_ready.none());
   m_free = false;
   m_output_register = output_reg_set;
   warp_inst_t **pipeline_reg = pipeline_reg_set->get_ready();
   if( (pipeline_reg) and !((*pipeline_reg)->empty()) ) {
      m_warp_id = (*pipeline_reg)->warp_id();
      for( unsigned op=0; op < MAX_REG_OPERANDS; op++ ) {
         int reg_num = (*pipeline_reg)->arch_reg.src[op]; // this math needs to match that used in function_info::ptx_decode_inst
         if( reg_num >= 0 ) { // valid register
            m_src_op[op] = op_t( this, op, reg_num, m_num_banks, m_bank_warp_shift );
            m_not_ready.set(op);
         } else 
            m_src_op[op] = op_t();
      }
      //move_warp(m_warp,*pipeline_reg);
      pipeline_reg_set->move_out_to(m_warp);
      return true;
   }
   return false;
}

void opndcoll_rfu_t::collector_unit_t::dispatch()
{
   assert( m_not_ready.none() );
   //move_warp(*m_output_register,m_warp);
   m_output_register->move_in(m_warp);
   m_free=true;
   m_output_register = NULL;
   for( unsigned i=0; i<MAX_REG_OPERANDS*2;i++)
      m_src_op[i].reset();
}

simt_core_cluster::simt_core_cluster( class gpgpu_sim *gpu, 
                                      unsigned cluster_id, 
                                      const struct shader_core_config *config, 
                                      const struct memory_config *mem_config,
                                      shader_core_stats *stats, 
                                      class memory_stats_t *mstats )
{
    m_config = config;
    m_cta_issue_next_core=m_config->n_simt_cores_per_cluster-1; // this causes first launch to use hw cta 0
    m_cluster_id=cluster_id;
    m_gpu = gpu;
    m_stats = stats;
    m_memory_stats = mstats;
    m_core = new shader_core_ctx*[ config->n_simt_cores_per_cluster ];
    for( unsigned i=0; i < config->n_simt_cores_per_cluster; i++ ) {
        unsigned sid = m_config->cid_to_sid(i,m_cluster_id);
        m_core[i] = new shader_core_ctx(gpu,this,sid,m_cluster_id,config,mem_config,stats);
        m_core_sim_order.push_back(i); 
    }
}

void simt_core_cluster::core_cycle()
{
    for( std::list<unsigned>::iterator it = m_core_sim_order.begin(); it != m_core_sim_order.end(); ++it ) {
        m_core[*it]->cycle();
    }

    if (m_config->simt_core_sim_order == 1) {
        m_core_sim_order.splice(m_core_sim_order.end(), m_core_sim_order, m_core_sim_order.begin()); 
    }
}

void simt_core_cluster::reinit()
{
    for( unsigned i=0; i < m_config->n_simt_cores_per_cluster; i++ ) 
        m_core[i]->reinit(0,m_config->n_thread_per_shader,true);
}

unsigned simt_core_cluster::max_cta( const kernel_info_t &kernel )
{
    return m_config->n_simt_cores_per_cluster * m_config->max_cta(kernel);
}

unsigned simt_core_cluster::get_not_completed() const
{
    unsigned not_completed=0;
    for( unsigned i=0; i < m_config->n_simt_cores_per_cluster; i++ ) 
        not_completed += m_core[i]->get_not_completed();
    return not_completed;
}

void simt_core_cluster::print_not_completed( FILE *fp ) const
{
    for( unsigned i=0; i < m_config->n_simt_cores_per_cluster; i++ ) {
        unsigned not_completed=m_core[i]->get_not_completed();
        unsigned sid=m_config->cid_to_sid(i,m_cluster_id);
        fprintf(fp,"%u(%u) ", sid, not_completed );
    }
}

unsigned simt_core_cluster::get_n_active_cta() const
{
    unsigned n=0;
    for( unsigned i=0; i < m_config->n_simt_cores_per_cluster; i++ ) 
        n += m_core[i]->get_n_active_cta();
    return n;
}

unsigned simt_core_cluster::get_n_active_sms() const
{
    unsigned n=0;
    for( unsigned i=0; i < m_config->n_simt_cores_per_cluster; i++ )
        n += m_core[i]->isactive();
    return n;
}

unsigned simt_core_cluster::issue_block2core()
{
    unsigned num_blocks_issued=0;
    for( unsigned i=0; i < m_config->n_simt_cores_per_cluster; i++ ) {
        unsigned core = (i+m_cta_issue_next_core+1)%m_config->n_simt_cores_per_cluster;
        if( m_core[core]->get_not_completed() == 0 ) {
            if( m_core[core]->get_kernel() == NULL ) {
                kernel_info_t *k = m_gpu->select_kernel();
                if( k ) 
                    m_core[core]->set_kernel(k);
            }
        }
        kernel_info_t *kernel = m_core[core]->get_kernel();
        if( kernel && !kernel->no_more_ctas_to_run() && (m_core[core]->get_n_active_cta() < m_config->max_cta(*kernel)) ) {
            m_core[core]->issue_block2core(*kernel);
            num_blocks_issued++;
            m_cta_issue_next_core=core; 
            break;
        }
    }
    return num_blocks_issued;
}

void simt_core_cluster::cache_flush()
{
    for( unsigned i=0; i < m_config->n_simt_cores_per_cluster; i++ ) 
        m_core[i]->cache_flush();
}

bool simt_core_cluster::icnt_injection_buffer_full(unsigned size, bool write)
{
    unsigned request_size = size;
    if (!write) 
        request_size = READ_PACKET_SIZE;
    return ! ::icnt_has_buffer(m_cluster_id, request_size);
}

void simt_core_cluster::icnt_inject_request_packet(class mem_fetch *mf)
{
    // stats
    if (mf->get_is_write()) m_stats->made_write_mfs++;
    else m_stats->made_read_mfs++;
    switch (mf->get_access_type()) {
    case CONST_ACC_R: m_stats->gpgpu_n_mem_const++; break;
    case TEXTURE_ACC_R: m_stats->gpgpu_n_mem_texture++; break;
    case GLOBAL_ACC_R: m_stats->gpgpu_n_mem_read_global++; break;
    case GLOBAL_ACC_W: m_stats->gpgpu_n_mem_write_global++; break;
    case LOCAL_ACC_R: m_stats->gpgpu_n_mem_read_local++; break;
    case LOCAL_ACC_W: m_stats->gpgpu_n_mem_write_local++; break;
    case INST_ACC_R: m_stats->gpgpu_n_mem_read_inst++; break;
    case L1_WRBK_ACC: m_stats->gpgpu_n_mem_write_global++; break;
    case L2_WRBK_ACC: m_stats->gpgpu_n_mem_l2_writeback++; break;
    case L1_WR_ALLOC_R: m_stats->gpgpu_n_mem_l1_write_allocate++; break;
    case L2_WR_ALLOC_R: m_stats->gpgpu_n_mem_l2_write_allocate++; break;
    default: assert(0);
    }

   // The packet size varies depending on the type of request: 
   // - For write request and atomic request, the packet contains the data 
   // - For read request (i.e. not write nor atomic), the packet only has control metadata
   unsigned int packet_size = mf->size(); 
   if (!mf->get_is_write() && !mf->isatomic()) {
      packet_size = mf->get_ctrl_size(); 
   }
   m_stats->m_outgoing_traffic_stats->record_traffic(mf, packet_size); 
   unsigned destination = mf->get_sub_partition_id();
   mf->set_status(IN_ICNT_TO_MEM,gpu_sim_cycle+gpu_tot_sim_cycle);
   if (!mf->get_is_write() && !mf->isatomic())
      ::icnt_push(m_cluster_id, m_config->mem2device(destination), (void*)mf, mf->get_ctrl_size() );
   else 
      ::icnt_push(m_cluster_id, m_config->mem2device(destination), (void*)mf, mf->size());
}

void simt_core_cluster::icnt_cycle()
{
    if( !m_response_fifo.empty() ) {
        mem_fetch *mf = m_response_fifo.front();
        unsigned cid = m_config->sid_to_cid(mf->get_sid());
        if( mf->get_access_type() == INST_ACC_R ) {
            // instruction fetch response
            if( !m_core[cid]->fetch_unit_response_buffer_full() ) {
                m_response_fifo.pop_front();
                m_core[cid]->accept_fetch_response(mf);
            }
        } else {
            // data response
            if( !m_core[cid]->ldst_unit_response_buffer_full() ) {
                m_response_fifo.pop_front();
                m_memory_stats->memlatstat_read_done(mf);
                m_core[cid]->accept_ldst_unit_response(mf);
            }
        }
    }
    if( m_response_fifo.size() < m_config->n_simt_ejection_buffer_size ) {
        mem_fetch *mf = (mem_fetch*) ::icnt_pop(m_cluster_id);
        if (!mf) 
            return;
        assert(mf->get_tpc() == m_cluster_id);
        assert(mf->get_type() == READ_REPLY || mf->get_type() == WRITE_ACK );

        // The packet size varies depending on the type of request: 
        // - For read request and atomic request, the packet contains the data 
        // - For write-ack, the packet only has control metadata
        unsigned int packet_size = (mf->get_is_write())? mf->get_ctrl_size() : mf->size(); 
        m_stats->m_incoming_traffic_stats->record_traffic(mf, packet_size); 
        mf->set_status(IN_CLUSTER_TO_SHADER_QUEUE,gpu_sim_cycle+gpu_tot_sim_cycle);
        //m_memory_stats->memlatstat_read_done(mf,m_shader_config->max_warps_per_shader);
        m_response_fifo.push_back(mf);
        m_stats->n_mem_to_simt[m_cluster_id] += mf->get_num_flits(false);
    }
}

void simt_core_cluster::get_pdom_stack_top_info( unsigned sid, unsigned tid, unsigned *pc, unsigned *rpc ) const
{
    unsigned cid = m_config->sid_to_cid(sid);
    m_core[cid]->get_pdom_stack_top_info(tid,pc,rpc);
}

void simt_core_cluster::display_pipeline( unsigned sid, FILE *fout, int print_mem, int mask )
{
    m_core[m_config->sid_to_cid(sid)]->display_pipeline(fout,print_mem,mask);

    fprintf(fout,"\n");
    fprintf(fout,"Cluster %u pipeline state\n", m_cluster_id );
    fprintf(fout,"Response FIFO (occupancy = %zu):\n", m_response_fifo.size() );
    for( std::list<mem_fetch*>::const_iterator i=m_response_fifo.begin(); i != m_response_fifo.end(); i++ ) {
        const mem_fetch *mf = *i;
        mf->print(fout);
    }
}

void simt_core_cluster::print_cache_stats( FILE *fp, unsigned& dl1_accesses, unsigned& dl1_misses ) const {
   for ( unsigned i = 0; i < m_config->n_simt_cores_per_cluster; ++i ) {
      m_core[ i ]->print_cache_stats( fp, dl1_accesses, dl1_misses );
   }
}

void simt_core_cluster::get_icnt_stats(long &n_simt_to_mem, long &n_mem_to_simt) const {
	long simt_to_mem=0;
	long mem_to_simt=0;
	for ( unsigned i = 0; i < m_config->n_simt_cores_per_cluster; ++i ) {
		m_core[i]->get_icnt_power_stats(simt_to_mem, mem_to_simt);
	}
	n_simt_to_mem = simt_to_mem;
	n_mem_to_simt = mem_to_simt;
}

void simt_core_cluster::get_cache_stats(cache_stats &cs) const{
    for ( unsigned i = 0; i < m_config->n_simt_cores_per_cluster; ++i ) {
        m_core[i]->get_cache_stats(cs);
    }
}

void simt_core_cluster::get_L1I_sub_stats(struct cache_sub_stats &css) const{
    struct cache_sub_stats temp_css;
    struct cache_sub_stats total_css;
    temp_css.clear();
    total_css.clear();
    for ( unsigned i = 0; i < m_config->n_simt_cores_per_cluster; ++i ) {
        m_core[i]->get_L1I_sub_stats(temp_css);
        total_css += temp_css;
    }
    css = total_css;
}
void simt_core_cluster::get_L1D_sub_stats(struct cache_sub_stats &css) const{
    struct cache_sub_stats temp_css;
    struct cache_sub_stats total_css;
    temp_css.clear();
    total_css.clear();
    for ( unsigned i = 0; i < m_config->n_simt_cores_per_cluster; ++i ) {
        m_core[i]->get_L1D_sub_stats(temp_css);
        total_css += temp_css;
    }
    css = total_css;
}
void simt_core_cluster::get_L1C_sub_stats(struct cache_sub_stats &css) const{
    struct cache_sub_stats temp_css;
    struct cache_sub_stats total_css;
    temp_css.clear();
    total_css.clear();
    for ( unsigned i = 0; i < m_config->n_simt_cores_per_cluster; ++i ) {
        m_core[i]->get_L1C_sub_stats(temp_css);
        total_css += temp_css;
    }
    css = total_css;
}
void simt_core_cluster::get_L1T_sub_stats(struct cache_sub_stats &css) const{
    struct cache_sub_stats temp_css;
    struct cache_sub_stats total_css;
    temp_css.clear();
    total_css.clear();
    for ( unsigned i = 0; i < m_config->n_simt_cores_per_cluster; ++i ) {
        m_core[i]->get_L1T_sub_stats(temp_css);
        total_css += temp_css;
    }
    css = total_css;
}

void shader_core_ctx::checkExecutionStatusAndUpdate(warp_inst_t &inst, unsigned t, unsigned tid)
{
    if( inst.has_callback(t) ) 
           m_warp[inst.warp_id()].inc_n_atomic();
        if (inst.space.is_local() && (inst.is_load() || inst.is_store())) {
            new_addr_type localaddrs[MAX_ACCESSES_PER_INSN_PER_THREAD];
            unsigned num_addrs;
            num_addrs = translate_local_memaddr(inst.get_addr(t), tid, m_config->n_simt_clusters*m_config->n_simt_cores_per_cluster,
                   inst.data_size, (new_addr_type*) localaddrs );
            inst.set_addr(t, (new_addr_type*) localaddrs, num_addrs);
        }
        if ( ptx_thread_done(tid) ) {
            m_warp[inst.warp_id()].set_completed(t);
            m_warp[inst.warp_id()].ibuffer_flush();
        }

    // PC-Histogram Update 
    unsigned warp_id = inst.warp_id(); 
    unsigned pc = inst.pc; 
    for (unsigned t = 0; t < m_config->warp_size; t++) {
        if (inst.active(t)) {
            int tid = warp_id * m_config->warp_size + t; 
            cflog_update_thread_pc(m_sid, tid, pc);  
        }
    }
}

